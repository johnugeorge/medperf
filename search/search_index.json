{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction Medical Artificial Intelligence (AI) has the potential to revolutionize healthcare by advancing evidence-based medicine, personalizing patient care, and reducing costs. Unlocking this potential requires reliable methods for evaluating the efficacy of medical machine learning (ML) models on large-scale heterogeneous data while maintaining patient privacy. What is MedPerf? MedPerf is an open-source framework for benchmarking ML models to deliver clinical efficacy while prioritizing patient privacy and mitigating legal and regulatory risks. It enables federated evaluation in which models are securely distributed to different facilities for evaluation. The goal of federated evaluation is to make it simple and reliable to share models with many Data Providers, evaluate those models against their data in controlled settings, then aggregate and analyze the findings. The MedPerf approach empowers healthcare organizations to assess and verify the performance of ML models in an efficient and human-supervised process without sharing any patient data across facilities during the process. Why MedPerf? MedPerf reduces the risks and costs associated with data sharing, maximizing medical and patient outcomes. The platform leads to an effective, broader, cost-effective adoption of medical ML and improves patient outcomes. Anyone who joins our platform can get several benefits, regardless of the role they will assume. Benefits if you are a Data Providers : Evaluate how well machine learning models perform on your population\u2019s data; Connect to Model Owners to help them improve medical ML in a specific domain Help define impactful medical ML benchmarks; Benefits if you are a Model Owner : Measure model performance on private datasets that you would never have access to; Connect to specific Data Providers that can help you increase the performance of your model; Benefits if you own a benchmark ( Benchmark Committee ): Hold a leading role in the MedPerf ecosystem by defining specifications of a benchmark for a particular medical ML task; Get help to create a strong community around a specific area; Rule point on creating the guidelines to generate impactful ML for a specific area; Help improve best practices in your area of interest; Ensure the choice of the metrics as well as the proper reference implementations Benefits to the Broad Community Provide consistent and rigorous approaches for evaluating the accuracy of ML models for real-world use in a standardized manner; Enable model usability measurement across institutions while maintaining data privacy and model reliability; Connect with a community of expert groups to employ scientific evaluation methodologies and technical approaches to operate benchmarks that not only have well-defined clinical aspects, such as clinical impact, clinical workflow integration and patient outcome, but also support robust technical aspects, including proper metrics, data preprocessing and reference implementation. What is a benchmark in the MedPerf perspective? A benchmark is a collection of assets used by the platform to test the performance of ML models for a specific clinical problem. The primary components of a benchmark are: Specifications : precise definition of the clinical setting (e.g., problem or task and specific patient population) on which trained ML models are to be evaluated. It also includes the labeling (annotation) methodology as well as the choice of evaluation metrics. Dataset Preparation : a process that prepares datasets for use in evaluation, and can also test the prepared datasets for quality and compatibility. This is implemented as an MLCube (see Data Preparator MLCube ). Registered Datasets : a list of registered datasets prepared according to the benchmark criteria and approved for evaluation use by their owners (e.g. patient data from multiple facilities representing (as a whole) a diverse patient population). Evaluation : a consistent implementation of the testing pipelines and evaluation metrics. Reference Implementation : a detailed example of a benchmark submission consisting of example model code, the evaluation component, and de-identified or synthetic publicly available sample data. Registered Models : a list of registered models to run in this benchmark. Documentation : documents for understanding and using the benchmark.","title":"Introduction"},{"location":"#introduction","text":"Medical Artificial Intelligence (AI) has the potential to revolutionize healthcare by advancing evidence-based medicine, personalizing patient care, and reducing costs. Unlocking this potential requires reliable methods for evaluating the efficacy of medical machine learning (ML) models on large-scale heterogeneous data while maintaining patient privacy.","title":"Introduction"},{"location":"#what-is-medperf","text":"MedPerf is an open-source framework for benchmarking ML models to deliver clinical efficacy while prioritizing patient privacy and mitigating legal and regulatory risks. It enables federated evaluation in which models are securely distributed to different facilities for evaluation. The goal of federated evaluation is to make it simple and reliable to share models with many Data Providers, evaluate those models against their data in controlled settings, then aggregate and analyze the findings. The MedPerf approach empowers healthcare organizations to assess and verify the performance of ML models in an efficient and human-supervised process without sharing any patient data across facilities during the process.","title":"What is MedPerf?"},{"location":"#why-medperf","text":"MedPerf reduces the risks and costs associated with data sharing, maximizing medical and patient outcomes. The platform leads to an effective, broader, cost-effective adoption of medical ML and improves patient outcomes. Anyone who joins our platform can get several benefits, regardless of the role they will assume. Benefits if you are a Data Providers : Evaluate how well machine learning models perform on your population\u2019s data; Connect to Model Owners to help them improve medical ML in a specific domain Help define impactful medical ML benchmarks; Benefits if you are a Model Owner : Measure model performance on private datasets that you would never have access to; Connect to specific Data Providers that can help you increase the performance of your model; Benefits if you own a benchmark ( Benchmark Committee ): Hold a leading role in the MedPerf ecosystem by defining specifications of a benchmark for a particular medical ML task; Get help to create a strong community around a specific area; Rule point on creating the guidelines to generate impactful ML for a specific area; Help improve best practices in your area of interest; Ensure the choice of the metrics as well as the proper reference implementations Benefits to the Broad Community Provide consistent and rigorous approaches for evaluating the accuracy of ML models for real-world use in a standardized manner; Enable model usability measurement across institutions while maintaining data privacy and model reliability; Connect with a community of expert groups to employ scientific evaluation methodologies and technical approaches to operate benchmarks that not only have well-defined clinical aspects, such as clinical impact, clinical workflow integration and patient outcome, but also support robust technical aspects, including proper metrics, data preprocessing and reference implementation.","title":"Why MedPerf?"},{"location":"#what-is-a-benchmark-in-the-medperf-perspective","text":"A benchmark is a collection of assets used by the platform to test the performance of ML models for a specific clinical problem. The primary components of a benchmark are: Specifications : precise definition of the clinical setting (e.g., problem or task and specific patient population) on which trained ML models are to be evaluated. It also includes the labeling (annotation) methodology as well as the choice of evaluation metrics. Dataset Preparation : a process that prepares datasets for use in evaluation, and can also test the prepared datasets for quality and compatibility. This is implemented as an MLCube (see Data Preparator MLCube ). Registered Datasets : a list of registered datasets prepared according to the benchmark criteria and approved for evaluation use by their owners (e.g. patient data from multiple facilities representing (as a whole) a diverse patient population). Evaluation : a consistent implementation of the testing pipelines and evaluation metrics. Reference Implementation : a detailed example of a benchmark submission consisting of example model code, the evaluation component, and de-identified or synthetic publicly available sample data. Registered Models : a list of registered models to run in this benchmark. Documentation : documents for understanding and using the benchmark.","title":"What is a benchmark in the MedPerf perspective?"},{"location":"general_instructions/","text":"General Instructions Installing MedPerf Dependencies MedPerf has some dependencies that must be installed by the user before being able to run it, including MLCube and the required runners (right now there are Docker and Singularity runners). Use the following command to install the dependencies: pip install mlcube mlcube-docker mlcube-singularity Depending on the runner you are going to use, you also need to download the runner engine. Hosting the Server To host the server, please follow the instructions inside the server/README.md file. Installing the CLI To install the CLI, please follow the instructions inside the cli/README.md file. Creating a User After installing everything you need to run MedPerf, the first step to joining the platform is to create a user, which is done by the MedPerf administrator. If you haven\u2019t received your credentials to access MedPerf, get in touch with the team. After you get your credentials and server URL, you can log in to MedPerf using your credentials with the following command: medperf login You can also modify MedPerf\u2019s cli/medperf/config.py file so it points to the development server ( server variable) and you don\u2019t need to provide additional arguments on runtime. For example, if you are running a server instance somewhere different from what the CLI expects, it might be better to modify the configuration file so that you don't need to write --host=&lt;SERVER_URL> with every command. However, if you don\u2019t want to change the configuration file, you can run every MedPerf command with medperf --host =<SERVER_URL> .","title":"General Instructions"},{"location":"general_instructions/#general-instructions","text":"","title":"General Instructions"},{"location":"general_instructions/#installing-medperf-dependencies","text":"MedPerf has some dependencies that must be installed by the user before being able to run it, including MLCube and the required runners (right now there are Docker and Singularity runners). Use the following command to install the dependencies: pip install mlcube mlcube-docker mlcube-singularity Depending on the runner you are going to use, you also need to download the runner engine.","title":"Installing MedPerf Dependencies"},{"location":"general_instructions/#hosting-the-server","text":"To host the server, please follow the instructions inside the server/README.md file.","title":"Hosting the Server"},{"location":"general_instructions/#installing-the-cli","text":"To install the CLI, please follow the instructions inside the cli/README.md file.","title":"Installing the CLI"},{"location":"general_instructions/#creating-a-user","text":"After installing everything you need to run MedPerf, the first step to joining the platform is to create a user, which is done by the MedPerf administrator. If you haven\u2019t received your credentials to access MedPerf, get in touch with the team. After you get your credentials and server URL, you can log in to MedPerf using your credentials with the following command: medperf login You can also modify MedPerf\u2019s cli/medperf/config.py file so it points to the development server ( server variable) and you don\u2019t need to provide additional arguments on runtime. For example, if you are running a server instance somewhere different from what the CLI expects, it might be better to modify the configuration file so that you don't need to write --host=&lt;SERVER_URL> with every command. However, if you don\u2019t want to change the configuration file, you can run every MedPerf command with medperf --host =<SERVER_URL> .","title":"Creating a User"},{"location":"instructions_benchmark_committee/","text":"Instructions for Benchmark Committee Implementing ML Cubes In order to create a benchmark, Benchmark Committee need to develop the three MLCubes. You can get details on how to implement the Model MLCube here , the Data Preparator MLCube here , and finally the Metrics MLCube here . Once all MLCubes are developed, they must be hosted somewhere that MedPerf can download for later use. For each implemented MLCube, we expect: At the minimum, to find the mlcube manifest ( mlcube.yaml ) and the parameters file ( parameters.yaml ) as part of a public github repository. The mlcube image must be publicly available through docker hub. Additional files (those contained inside the mlcube/workspace/additional_files directory) must be compressed as a .tar.gz file and publicly hosted somewhere on the internet. Develop a Demo Dataset In order to test the implementation of your MLCubes, as well as potentially allow other users to test their implementations for your benchmark, a public demonstration dataset should be hosted on the web. This demo dataset can be either a subset of a publicly accessible dataset, which you have clearance to provide or a synthetic dataset. It\u2019s up to you what the contents of this dataset are. Note: The demo dataset will be used as input to the data preparation cube as part of the compatibility testing. Demo datasets must be compressed as a .tar.gz file, and at the root there should be a paths.yaml file with the following structure: data_path: <DATA_PATH> labels_path: <LABELS_PATH> where: data_path: path relative to the location of the paths.yaml file that should be used as data_path input for the Data Preparator MLCube labels_path: path relative to the location of the paths.yaml file that should be used as labels_path input for the Data Preparator MLCube If your some reason you can\u2019t host your demo dataset , you can bypass the demo dataset retrieval by following the next steps: Get the hash of your demo dataset tarball file with the following command shasum demo_dataset.tar.gz Create a folder with that hash under Medperf\u2019s demo dataset filesystem Move the tarball inside that folder and name it tmp.tar.gz mv demo_dataset.tar.gz ~/.medperf/demo/<hash>/tmp.tar.gz Keep in mind that your benchmark will only work for those that have done the same thing on their machines. Note: These steps should be followed if you wish to test your implementation before submitting anything to the platform. Test Your Implementation You can test your benchmark workflow before submitting anything to the platform. To do this, run the following command: medperf test -p path/to/data_preparator/mlcube -m path/to/model/mlcube -e path/to/evaluator/mlcube -d <demo_dataset hash> <demo_datashet hash> Once your tests are complete, you can continue with submitting your MLCubes and your benchmark. Submitting Your MLCubes To submit your MLCube, you should have the following information at hand: mlcube.yaml raw github url with commit hash parameters.yaml raw github url with commit hash URL for the additional files tarball (optional) You can submit your MLCube using the following command: medperf mlcube submit --name <mlcube_name> --mlcube-file Then CLI will return a response as follows: https://raw.githubusercontent.com/aristizabal95/medical/02e142f9a83250a0108e73f955bf4cb6c72f5a0f/cubes/xrv_chex_densenet/mlcube/mlcube.yaml --parameters-file https://raw.githubusercontent.com/aristizabal95/medperf-server/1a0a8c21f92c3d9a162ce5e61732eed2d0eb95cc/app/database/cubes/xrv_chex_densenet/parameters.yaml --additional-file https://storage.googleapis.com/medperf-storage/xrv_chex_densenet.tar.gz --additional-hash c5c408b5f9ef8b1da748e3b1f2d58b8b3eebf96e MedPerf 0.0.0 Additional file hash generated \u2705 Done! Submitting Your Benchmark Once all your cubes are submitted to the platform, you can create your benchmark. For this, you need to keep at hand the following information: Data preparator UID (obtained through medperf mlcube ls ) Reference model UID (obtained through medperf mlcube ls ) Evaluator UID (obtained through medperf mlcube ls ) Demo Dataset URL (if not hosted publicly, can be blank) Demo Dataset Hash (Only required if not hosted publicly) You can create your benchmark using the following command: medperf benchmark submit --name <benchmark_name> --description <benchmark_description> --demo-url <demo_url> --data-preparation-mlcube <data_preparator_MLCube_uid> --reference-model-mlcube <model_MLCube_uid> --evaluator-mlcube <evaluator_MLCube_uid> If all the compatibility tests run successfully, the CLI will provide a response as follows: MedPerf 0.0.0 Running compatibility test Benchmark Data Preparation: tmp_1_2_3 > Preparation cube download complete > MLCommons TorchXRayVision Preprocessor MD5 hash check complete > Cube execution complete > Sanity checks complete > Statistics complete Benchmark Execution: tmp_1_2_3 > Evaluator cube download complete > MLCommons Metrics MD5 hash check complete > Model cube download complete > MLCommons TorchXRayVision CheXpert DenseNet model MD5 hash check complete > Model execution complete > Completed benchmark registration information Submitting Benchmark to MedPerf Uploaded \u2705 Done! In order to check the full list of arguments that you can provide, use the help command: medperf benchmark submit --help The CLI will return you the following response: MedPerf 0.0.0 Usage: medperf [OPTIONS] COMMAND [ARGS]... Options: --log TEXT [default: INFO] --log-file TEXT --comms TEXT [default: REST] --ui TEXT [default: CLI] --host TEXT [default: https://medperf.org] --storage TEXT [default: /Users/aristizabal- factored/.medperf] --certificate TEXT --local / --no-local Run the CLI with local server configuration [default: False] --install-completion [bash|zsh|fish|powershell|pwsh] Install completion for the specified shell. --show-completion [bash|zsh|fish|powershell|pwsh] Show completion for the specified shell, to copy it or customize the installation. --help Show this message and exit. Commands: dataset Manage datasets login Login to the medperf server. mlcube Manage mlcubes passwd Set a new password. result Manage results run Runs the benchmark execution step for a given benchmark, prepared... In addition, you can also see information about your benchmarks with: medperf benchmark ls The CLI will provide a list of your benchmarks and MedPerf 0.0.0 UID Name Description State Approval Status ----- ------ ------------- --------- ----------------- 7 dfci submit OPERATION APPROVED Approving Additional Associations You can retrieve all existing associations with: medperf association ls The CLI will return the existing associations: MedPerf 0.0.0 Dataset UID MLCube UID Benchmark UID Initiated by Status ------------- ------------ --------------- -------------- -------- 32 7 13 APPROVED 33 7 13 APPROVED 35 7 13 APPROVED 38 7 13 APPROVED 37 7 13 PENDING In addition, you can filter associations to your benchmark by their approval status: medperf association ls [pending | approved | rejected] You can approve additional model associations with the following command: medperf association approve -b <benchmark_uid> [-m <mlcube_uid>] A similar approach is used for approving dataset associations: medperf association approve -b <benchmark_uid> [-d <dataset_uid>] If you can to reject an association, use the following command for rejecting a model medperf association reject -b <benchmark_uid> [-m <model_uid>] and the following one to reject a dataset association: medperf association reject -b <benchmark_uid> [-m <dataset_uid>] Then, the CLI will provide a response for approving or rejecting a benchmark as follows: MedPerf 0.0.0 \u2705 Done!","title":"Instructions for Bechmark Committee"},{"location":"instructions_benchmark_committee/#instructions-for-benchmark-committee","text":"","title":"Instructions for Benchmark Committee"},{"location":"instructions_benchmark_committee/#implementing-ml-cubes","text":"In order to create a benchmark, Benchmark Committee need to develop the three MLCubes. You can get details on how to implement the Model MLCube here , the Data Preparator MLCube here , and finally the Metrics MLCube here . Once all MLCubes are developed, they must be hosted somewhere that MedPerf can download for later use. For each implemented MLCube, we expect: At the minimum, to find the mlcube manifest ( mlcube.yaml ) and the parameters file ( parameters.yaml ) as part of a public github repository. The mlcube image must be publicly available through docker hub. Additional files (those contained inside the mlcube/workspace/additional_files directory) must be compressed as a .tar.gz file and publicly hosted somewhere on the internet.","title":"Implementing ML Cubes"},{"location":"instructions_benchmark_committee/#develop-a-demo-dataset","text":"In order to test the implementation of your MLCubes, as well as potentially allow other users to test their implementations for your benchmark, a public demonstration dataset should be hosted on the web. This demo dataset can be either a subset of a publicly accessible dataset, which you have clearance to provide or a synthetic dataset. It\u2019s up to you what the contents of this dataset are. Note: The demo dataset will be used as input to the data preparation cube as part of the compatibility testing. Demo datasets must be compressed as a .tar.gz file, and at the root there should be a paths.yaml file with the following structure: data_path: <DATA_PATH> labels_path: <LABELS_PATH> where: data_path: path relative to the location of the paths.yaml file that should be used as data_path input for the Data Preparator MLCube labels_path: path relative to the location of the paths.yaml file that should be used as labels_path input for the Data Preparator MLCube If your some reason you can\u2019t host your demo dataset , you can bypass the demo dataset retrieval by following the next steps: Get the hash of your demo dataset tarball file with the following command shasum demo_dataset.tar.gz Create a folder with that hash under Medperf\u2019s demo dataset filesystem Move the tarball inside that folder and name it tmp.tar.gz mv demo_dataset.tar.gz ~/.medperf/demo/<hash>/tmp.tar.gz Keep in mind that your benchmark will only work for those that have done the same thing on their machines. Note: These steps should be followed if you wish to test your implementation before submitting anything to the platform.","title":"Develop a Demo Dataset"},{"location":"instructions_benchmark_committee/#test-your-implementation","text":"You can test your benchmark workflow before submitting anything to the platform. To do this, run the following command: medperf test -p path/to/data_preparator/mlcube -m path/to/model/mlcube -e path/to/evaluator/mlcube -d <demo_dataset hash> <demo_datashet hash> Once your tests are complete, you can continue with submitting your MLCubes and your benchmark.","title":"Test Your Implementation"},{"location":"instructions_benchmark_committee/#submitting-your-mlcubes","text":"To submit your MLCube, you should have the following information at hand: mlcube.yaml raw github url with commit hash parameters.yaml raw github url with commit hash URL for the additional files tarball (optional) You can submit your MLCube using the following command: medperf mlcube submit --name <mlcube_name> --mlcube-file Then CLI will return a response as follows: https://raw.githubusercontent.com/aristizabal95/medical/02e142f9a83250a0108e73f955bf4cb6c72f5a0f/cubes/xrv_chex_densenet/mlcube/mlcube.yaml --parameters-file https://raw.githubusercontent.com/aristizabal95/medperf-server/1a0a8c21f92c3d9a162ce5e61732eed2d0eb95cc/app/database/cubes/xrv_chex_densenet/parameters.yaml --additional-file https://storage.googleapis.com/medperf-storage/xrv_chex_densenet.tar.gz --additional-hash c5c408b5f9ef8b1da748e3b1f2d58b8b3eebf96e MedPerf 0.0.0 Additional file hash generated \u2705 Done!","title":"Submitting Your MLCubes"},{"location":"instructions_benchmark_committee/#submitting-your-benchmark","text":"Once all your cubes are submitted to the platform, you can create your benchmark. For this, you need to keep at hand the following information: Data preparator UID (obtained through medperf mlcube ls ) Reference model UID (obtained through medperf mlcube ls ) Evaluator UID (obtained through medperf mlcube ls ) Demo Dataset URL (if not hosted publicly, can be blank) Demo Dataset Hash (Only required if not hosted publicly) You can create your benchmark using the following command: medperf benchmark submit --name <benchmark_name> --description <benchmark_description> --demo-url <demo_url> --data-preparation-mlcube <data_preparator_MLCube_uid> --reference-model-mlcube <model_MLCube_uid> --evaluator-mlcube <evaluator_MLCube_uid> If all the compatibility tests run successfully, the CLI will provide a response as follows: MedPerf 0.0.0 Running compatibility test Benchmark Data Preparation: tmp_1_2_3 > Preparation cube download complete > MLCommons TorchXRayVision Preprocessor MD5 hash check complete > Cube execution complete > Sanity checks complete > Statistics complete Benchmark Execution: tmp_1_2_3 > Evaluator cube download complete > MLCommons Metrics MD5 hash check complete > Model cube download complete > MLCommons TorchXRayVision CheXpert DenseNet model MD5 hash check complete > Model execution complete > Completed benchmark registration information Submitting Benchmark to MedPerf Uploaded \u2705 Done! In order to check the full list of arguments that you can provide, use the help command: medperf benchmark submit --help The CLI will return you the following response: MedPerf 0.0.0 Usage: medperf [OPTIONS] COMMAND [ARGS]... Options: --log TEXT [default: INFO] --log-file TEXT --comms TEXT [default: REST] --ui TEXT [default: CLI] --host TEXT [default: https://medperf.org] --storage TEXT [default: /Users/aristizabal- factored/.medperf] --certificate TEXT --local / --no-local Run the CLI with local server configuration [default: False] --install-completion [bash|zsh|fish|powershell|pwsh] Install completion for the specified shell. --show-completion [bash|zsh|fish|powershell|pwsh] Show completion for the specified shell, to copy it or customize the installation. --help Show this message and exit. Commands: dataset Manage datasets login Login to the medperf server. mlcube Manage mlcubes passwd Set a new password. result Manage results run Runs the benchmark execution step for a given benchmark, prepared... In addition, you can also see information about your benchmarks with: medperf benchmark ls The CLI will provide a list of your benchmarks and MedPerf 0.0.0 UID Name Description State Approval Status ----- ------ ------------- --------- ----------------- 7 dfci submit OPERATION APPROVED","title":"Submitting Your Benchmark"},{"location":"instructions_benchmark_committee/#approving-additional-associations","text":"You can retrieve all existing associations with: medperf association ls The CLI will return the existing associations: MedPerf 0.0.0 Dataset UID MLCube UID Benchmark UID Initiated by Status ------------- ------------ --------------- -------------- -------- 32 7 13 APPROVED 33 7 13 APPROVED 35 7 13 APPROVED 38 7 13 APPROVED 37 7 13 PENDING In addition, you can filter associations to your benchmark by their approval status: medperf association ls [pending | approved | rejected] You can approve additional model associations with the following command: medperf association approve -b <benchmark_uid> [-m <mlcube_uid>] A similar approach is used for approving dataset associations: medperf association approve -b <benchmark_uid> [-d <dataset_uid>] If you can to reject an association, use the following command for rejecting a model medperf association reject -b <benchmark_uid> [-m <model_uid>] and the following one to reject a dataset association: medperf association reject -b <benchmark_uid> [-m <dataset_uid>] Then, the CLI will provide a response for approving or rejecting a benchmark as follows: MedPerf 0.0.0 \u2705 Done!","title":"Approving Additional Associations"},{"location":"instructions_data_owners/","text":"Instructions for Data Providers Preparing a Dataset Once your registration is approved, you can prepare a raw dataset to be used in a benchmark following the steps below: Use the following command to start the data preparation process, where path/ to /labels is the specific path to the dataset labels, benchmark_uid is the unique identifier of the benchmark you want to associate to, dataset_name is the name you want to give to the dataset, dataset_description can be used to describe it, and dataset_location is origin of the dataset (e.g. organization name). medperf dataset create -b <benchmark_uid> -d <path/to/data> -l <path/to/labels> --name <dataset_name> --description <dataset_description> --location <dataset_location> If the request is successful, the CLI will return a response as follows: MedPerf 0.0.0 Benchmark Data Preparation: xrv > Preparation cube download complete > MLCommons TorchXRayVision Preprocessor MD5 hash check complete > Cube execution complete > Sanity checks complete > Statistics complete \u2705 Done! Next step: register the dataset with 'medperf dataset submit -d <generated_uid>' The next step is to submit the dataset with the following command: medperf dataset submit -d <generated_uid> The CLI will return a response as follows: MedPerf 0.0.0 ==================== data_preparation_mlcube: '1' description: name generated_uid: 63a58fa902e131c2f0c3d8e8ca6498dc5a706504 input_data_hash: b7248606e46fd2ab31d69bbac9d65ffedbeb0b1b location: test metadata: column statistics: Atelectasis: '0.0': 0.625 '1.0': 0.375 Cardiomegaly: '0.0': 0.67 '1.0': 0.33 Consolidation: '0.0': 0.84 '1.0': 0.16 Edema: '0.0': 0.79 '1.0': 0.21 images statistics: channels: 1 height: max: 224.0 mean: 224.0 min: 224.0 std: 0.0 pixels_max: max: 255.0 mean: 255.0 min: 255.0 std: 0.0 pixels_mean: max: 149.2486049107 mean: 137.7983645568 min: 121.2493223852 std: 5.8131783153 pixels_min: max: 6.0 mean: 0.07 min: 0.0 std: 0.5063416925 pixels_std: max: 79.0369075709 mean: 71.5894559861 min: 64.0834766712 std: 2.8957115261 width: max: 224.0 mean: 224.0 min: 224.0 std: 0.0 labels: - Atelectasis - Cardiomegaly - Consolidation - Edema size: 200 name: test separate_labels: false split_seed: 0 state: OPERATION status: PENDING ==================== Above are the information and statistics that will be registered to the database Do you approve the registration of the presented data to the MLCommons comms? [Y/n] Y Uploading... \u2705 Done! Next step: associate the dataset with 'medperf dataset associate -b <BENCHMARK_UID> -d <generated_uid>' After running the submit command successfully, note that the CLI will ask you if you want to confirm the dataset registration to the MLCommons database. Send Y to confirm or N to halt the process. Note: The submission of a dataset is not the same as its upload. The data stays in the user\u2019s machine and we only upload metadata regarding the dataset, which must be approved beforehand by the user. Finally, you need to associate the dataset to the benchmark using the following command: medperf dataset associate -b <BENCHMARK_UID> -d <generated_uid> The CLI will return a response as follows: MedPerf 0.0.0 Benchmark Execution: tmp_1_2_3 > Metrics cube download complete > xrv_metrics MD5 hash check complete > Model cube download complete > xrv_chex_densenet MD5 hash check complete > Model execution complete These are the results generated by the compatibility test. This will be sent along the association request. They will not be part of the benchmark. ==================== approval_status: PENDING benchmark: tmp_1_2_3 dataset: 1 metadata: {} model: 2 name: tmp_1_2_3_2_1 results: AUC: Atelectasis: 0.8024533333333334 Cardiomegaly: 0.7913839891451833 Consolidation: 0.8694196428571428 Edema: 0.8300180831826401 F1: Atelectasis: 0.6379310344827586 Cardiomegaly: 0.6243386243386244 Consolidation: 0.378698224852071 Edema: 0.422680412371134 ==================== Please confirm that you would like to associate the dataset test with the benchmark xrv. [Y/n] Y Generating dataset benchmark association \u2705 Done! Next step: Once approved, run the benchmark with 'medperf run -b <benchmark_uid> -d <generated_uid> Confirm whether you would like to associate the dataset with the benchmark. After the submission and association of the dataset, you need to wait for the Benchmark Committee to approve your request. Obtaining information from a dataset The following command will give you more information about the dataset, such as registration status and, most importantly, the unique identifier (UID) of the dataset, which will then be used as <dataset_uid> : medperf dataset ls A response examples from the CLI for that command is shown as follows: MedPerf 0.0.0 UID Server UID Name Data Preparation Cube UID Registered Local \u2014----- \u2014--------- \u2014--- \u2014------------------------ \u2014--------- \u2014----- <uid> test 1 False True Running an Experiment After the Benchmark Committee approves your dataset submission and association, you can start experimenting. There are basically two methods for generating and submitting results: Creating an experiment and then submitting the results separately; Using the run command to create and submit results with just a command . Note: Both the dataset and the model must have been approved by the benchmark owner. Creating an Experiment You can obtain metrics for a given benchmark, dataset and model with the following command: medperf result create -b <benchmark_uid> -d <dataset_uid> -m <model_uid> Then CLI will provide a response as follows: MedPerf 0.0.0 Benchmark Execution: xrv > Metrics cube download complete > MLCommons Metrics MD5 hash check complete > Model cube download complete > MLCommons TorchXRayVision CheXpert DenseNet model MD5 hash check complete > Model execution complete \u2705 Done! Submitting results This command will submit the results under the user\u2019s approval. If submissions are canceled by the user, it can always be done again with: medperf result submit -b <benchmark_uid> -d <dataset_uid> -m <model_uid> Then CLI will provide a response as follows: MedPerf 0.0.0 ==================== approval_status: PENDING benchmark: 1 dataset: 1 metadata: {} model: 2 name: '1_2_1' results: AUC: Atelectasis: 0.8024533333333334 Cardiomegaly: 0.7913839891451833 Consolidation: 0.8694196428571428 Edema: 0.8300180831826401 F1: Atelectasis: 0.6379310344827586 Cardiomegaly: 0.6243386243386244 Consolidation: 0.378698224852071 Edema: 0.422680412371134 ==================== Above are the results generated by the model Do you approve uploading the presented results to the MLCommons comms? [Y/n] Y \u2705 Done! After that, you need to whether approve (Y) or not (n) the submission of the results. Creating an experiment and submitting results with a Single Command You can create results and submit them with the following command: medperf run -b <benchmark_uid> -d <generated_uid> -m <model_uid> Then CLI will provide a response as follows: MedPerf 0.0.0 Benchmark Execution: xrv > Metrics cube download complete > MLCommons Metrics MD5 hash check complete > Model cube download complete > MLCommons TorchXRayVision CheXpert DenseNet model MD5 hash check complete > Model execution complete ==================== approval_status: PENDING benchmark: 1 dataset: 1 metadata: {} model: 2 name: '1_2_1' results: AUC: Atelectasis: 0.8024533333333334 Cardiomegaly: 0.7913839891451833 Consolidation: 0.8694196428571428 Edema: 0.8300180831826401 F1: Atelectasis: 0.6379310344827586 Cardiomegaly: 0.6243386243386244 Consolidation: 0.378698224852071 Edema: 0.422680412371134 ==================== Above are the results generated by the model Do you approve uploading the presented results to the MLCommons comms? [Y/n] Y \u2705 Done! After that, you need to whether approve (Y) or not (n) the submission of the results. Getting information about your results In addition, you can get information about your results with: medperf result ls The CLI will return a response such as: MedPerf 0.0.0 Benchmark UID Model UID Data UID Submitted Local --------------- ----------- ---------- ----------- ------- 1 2 1 True True tmp_1_2_3 2 1 False True","title":"Instructions for Data Owners"},{"location":"instructions_data_owners/#instructions-for-data-providers","text":"","title":"Instructions for Data Providers"},{"location":"instructions_data_owners/#preparing-a-dataset","text":"Once your registration is approved, you can prepare a raw dataset to be used in a benchmark following the steps below: Use the following command to start the data preparation process, where path/ to /labels is the specific path to the dataset labels, benchmark_uid is the unique identifier of the benchmark you want to associate to, dataset_name is the name you want to give to the dataset, dataset_description can be used to describe it, and dataset_location is origin of the dataset (e.g. organization name). medperf dataset create -b <benchmark_uid> -d <path/to/data> -l <path/to/labels> --name <dataset_name> --description <dataset_description> --location <dataset_location> If the request is successful, the CLI will return a response as follows: MedPerf 0.0.0 Benchmark Data Preparation: xrv > Preparation cube download complete > MLCommons TorchXRayVision Preprocessor MD5 hash check complete > Cube execution complete > Sanity checks complete > Statistics complete \u2705 Done! Next step: register the dataset with 'medperf dataset submit -d <generated_uid>' The next step is to submit the dataset with the following command: medperf dataset submit -d <generated_uid> The CLI will return a response as follows: MedPerf 0.0.0 ==================== data_preparation_mlcube: '1' description: name generated_uid: 63a58fa902e131c2f0c3d8e8ca6498dc5a706504 input_data_hash: b7248606e46fd2ab31d69bbac9d65ffedbeb0b1b location: test metadata: column statistics: Atelectasis: '0.0': 0.625 '1.0': 0.375 Cardiomegaly: '0.0': 0.67 '1.0': 0.33 Consolidation: '0.0': 0.84 '1.0': 0.16 Edema: '0.0': 0.79 '1.0': 0.21 images statistics: channels: 1 height: max: 224.0 mean: 224.0 min: 224.0 std: 0.0 pixels_max: max: 255.0 mean: 255.0 min: 255.0 std: 0.0 pixels_mean: max: 149.2486049107 mean: 137.7983645568 min: 121.2493223852 std: 5.8131783153 pixels_min: max: 6.0 mean: 0.07 min: 0.0 std: 0.5063416925 pixels_std: max: 79.0369075709 mean: 71.5894559861 min: 64.0834766712 std: 2.8957115261 width: max: 224.0 mean: 224.0 min: 224.0 std: 0.0 labels: - Atelectasis - Cardiomegaly - Consolidation - Edema size: 200 name: test separate_labels: false split_seed: 0 state: OPERATION status: PENDING ==================== Above are the information and statistics that will be registered to the database Do you approve the registration of the presented data to the MLCommons comms? [Y/n] Y Uploading... \u2705 Done! Next step: associate the dataset with 'medperf dataset associate -b <BENCHMARK_UID> -d <generated_uid>' After running the submit command successfully, note that the CLI will ask you if you want to confirm the dataset registration to the MLCommons database. Send Y to confirm or N to halt the process. Note: The submission of a dataset is not the same as its upload. The data stays in the user\u2019s machine and we only upload metadata regarding the dataset, which must be approved beforehand by the user. Finally, you need to associate the dataset to the benchmark using the following command: medperf dataset associate -b <BENCHMARK_UID> -d <generated_uid> The CLI will return a response as follows: MedPerf 0.0.0 Benchmark Execution: tmp_1_2_3 > Metrics cube download complete > xrv_metrics MD5 hash check complete > Model cube download complete > xrv_chex_densenet MD5 hash check complete > Model execution complete These are the results generated by the compatibility test. This will be sent along the association request. They will not be part of the benchmark. ==================== approval_status: PENDING benchmark: tmp_1_2_3 dataset: 1 metadata: {} model: 2 name: tmp_1_2_3_2_1 results: AUC: Atelectasis: 0.8024533333333334 Cardiomegaly: 0.7913839891451833 Consolidation: 0.8694196428571428 Edema: 0.8300180831826401 F1: Atelectasis: 0.6379310344827586 Cardiomegaly: 0.6243386243386244 Consolidation: 0.378698224852071 Edema: 0.422680412371134 ==================== Please confirm that you would like to associate the dataset test with the benchmark xrv. [Y/n] Y Generating dataset benchmark association \u2705 Done! Next step: Once approved, run the benchmark with 'medperf run -b <benchmark_uid> -d <generated_uid> Confirm whether you would like to associate the dataset with the benchmark. After the submission and association of the dataset, you need to wait for the Benchmark Committee to approve your request.","title":"Preparing a Dataset"},{"location":"instructions_data_owners/#obtaining-information-from-a-dataset","text":"The following command will give you more information about the dataset, such as registration status and, most importantly, the unique identifier (UID) of the dataset, which will then be used as <dataset_uid> : medperf dataset ls A response examples from the CLI for that command is shown as follows: MedPerf 0.0.0 UID Server UID Name Data Preparation Cube UID Registered Local \u2014----- \u2014--------- \u2014--- \u2014------------------------ \u2014--------- \u2014----- <uid> test 1 False True","title":"Obtaining information from a dataset"},{"location":"instructions_data_owners/#running-an-experiment","text":"After the Benchmark Committee approves your dataset submission and association, you can start experimenting. There are basically two methods for generating and submitting results: Creating an experiment and then submitting the results separately; Using the run command to create and submit results with just a command . Note: Both the dataset and the model must have been approved by the benchmark owner.","title":"Running an Experiment"},{"location":"instructions_data_owners/#creating-an-experiment","text":"You can obtain metrics for a given benchmark, dataset and model with the following command: medperf result create -b <benchmark_uid> -d <dataset_uid> -m <model_uid> Then CLI will provide a response as follows: MedPerf 0.0.0 Benchmark Execution: xrv > Metrics cube download complete > MLCommons Metrics MD5 hash check complete > Model cube download complete > MLCommons TorchXRayVision CheXpert DenseNet model MD5 hash check complete > Model execution complete \u2705 Done!","title":"Creating an Experiment"},{"location":"instructions_data_owners/#submitting-results","text":"This command will submit the results under the user\u2019s approval. If submissions are canceled by the user, it can always be done again with: medperf result submit -b <benchmark_uid> -d <dataset_uid> -m <model_uid> Then CLI will provide a response as follows: MedPerf 0.0.0 ==================== approval_status: PENDING benchmark: 1 dataset: 1 metadata: {} model: 2 name: '1_2_1' results: AUC: Atelectasis: 0.8024533333333334 Cardiomegaly: 0.7913839891451833 Consolidation: 0.8694196428571428 Edema: 0.8300180831826401 F1: Atelectasis: 0.6379310344827586 Cardiomegaly: 0.6243386243386244 Consolidation: 0.378698224852071 Edema: 0.422680412371134 ==================== Above are the results generated by the model Do you approve uploading the presented results to the MLCommons comms? [Y/n] Y \u2705 Done! After that, you need to whether approve (Y) or not (n) the submission of the results.","title":"Submitting results"},{"location":"instructions_data_owners/#creating-an-experiment-and-submitting-results-with-a-single-command","text":"You can create results and submit them with the following command: medperf run -b <benchmark_uid> -d <generated_uid> -m <model_uid> Then CLI will provide a response as follows: MedPerf 0.0.0 Benchmark Execution: xrv > Metrics cube download complete > MLCommons Metrics MD5 hash check complete > Model cube download complete > MLCommons TorchXRayVision CheXpert DenseNet model MD5 hash check complete > Model execution complete ==================== approval_status: PENDING benchmark: 1 dataset: 1 metadata: {} model: 2 name: '1_2_1' results: AUC: Atelectasis: 0.8024533333333334 Cardiomegaly: 0.7913839891451833 Consolidation: 0.8694196428571428 Edema: 0.8300180831826401 F1: Atelectasis: 0.6379310344827586 Cardiomegaly: 0.6243386243386244 Consolidation: 0.378698224852071 Edema: 0.422680412371134 ==================== Above are the results generated by the model Do you approve uploading the presented results to the MLCommons comms? [Y/n] Y \u2705 Done! After that, you need to whether approve (Y) or not (n) the submission of the results.","title":"Creating an experiment and submitting results with a Single Command"},{"location":"instructions_data_owners/#getting-information-about-your-results","text":"In addition, you can get information about your results with: medperf result ls The CLI will return a response such as: MedPerf 0.0.0 Benchmark UID Model UID Data UID Submitted Local --------------- ----------- ---------- ----------- ------- 1 2 1 True True tmp_1_2_3 2 1 False True","title":"Getting information about your results"},{"location":"instructions_model_owners/","text":"Instructions for Model Owners Implementing the Model MLCube Model Owners that want to submit and associate models to a benchmark need to implement a new Model MLCube following the structure and conventions MedPerf uses to successfully run models on the platform. You can get details on how to implement the Model MLCube here . Testing Your MLCube After developing your MLCube, you can test if your implementation follows the expected I/O of the benchmark. For such, use the following command: medperf test -b <benchmark_uid> -m <path/to/model/mlcube> Then CLI will provide a response as follows: MedPerf 0.0.0 Benchmark Data Preparation: tmp_1_test_1655394059_3 > Preparation cube download complete > MLCommons TorchXRayVision Preprocessor MD5 hash check complete > Cube execution complete > Sanity checks complete > Statistics complete Benchmark Execution: tmp_1_test_1655394059_3 > Metrics cube download complete > MLCommons Metrics MD5 hash check complete > Model cube download complete > MLCommons TorchXRayVision CheXpert DenseNet model MD5 hash check complete > Model execution complete \u2705 Done! Submitting Your MLCube To submit your MLCube, you should have the following information at hand: mlcube.yaml raw github url with commit hash parameters.yaml raw github url with commit hash URL for the additional files tarball (optional) You can submit your MLCube using the following command: medperf mlcube submit --name <mlcube_name> --mlcube-file Then CLI will return a response as follows: https://raw.githubusercontent.com/aristizabal95/medical/02e142f9a83250a0108e73f955bf4cb6c72f5a0f/cubes/xrv_chex_densenet/mlcube/mlcube.yaml --parameters-file https://raw.githubusercontent.com/aristizabal95/medperf-server/1a0a8c21f92c3d9a162ce5e61732eed2d0eb95cc/app/database/cubes/xrv_chex_densenet/parameters.yaml --additional-file https://storage.googleapis.com/medperf-storage/xrv_chex_densenet.tar.gz --additional-hash c5c408b5f9ef8b1da748e3b1f2d58b8b3eebf96e MedPerf 0.0.0 Additional file hash generated \u2705 Done! Note: Currently, it is not possible to edit your submission, so ensure the information is correct. This will change in the near future, and instructions might need to be updated. Getting information about your MLCubes You can get information on your submitted MLCubes through: medperf mlcube ls Then CLI will provide a response as follows: MedPerf 0.0.0 MLCube UID Name State ------------ --------- --------- 5 densenet2 OPERATION Requesting Association to a Benchmark Once that cube passes the test and is submitted to the platform, you can create an association to the desired benchmark with: medperf mlcube associate -m <mlcube_uid> -b <benchmark_uid> The CLI will return a simple response to confirm the association request: MedPerf 0.0.0 \u2705 Done! Once an association is created, the Benchmark Committee must approve it. If the association is for a benchmark that you own, then it will be automatically approved and you can start using the model with your benchmark. Retrieving an association Existing associations can be retrieved with the following command: medperf association ls And the CLI will provide a list of associations: MedPerf 0.0.0 Dataset UID MLCube UID Benchmark UID Initiated by Status ------------- ------------ --------------- -------------- -------- 32 7 13 APPROVED 33 7 13 APPROVED 35 7 13 APPROVED 38 7 13 APPROVED 37 7 13 PENDING","title":"Instructions for Model Owners"},{"location":"instructions_model_owners/#instructions-for-model-owners","text":"","title":"Instructions for Model Owners"},{"location":"instructions_model_owners/#implementing-the-model-mlcube","text":"Model Owners that want to submit and associate models to a benchmark need to implement a new Model MLCube following the structure and conventions MedPerf uses to successfully run models on the platform. You can get details on how to implement the Model MLCube here .","title":"Implementing the Model MLCube"},{"location":"instructions_model_owners/#testing-your-mlcube","text":"After developing your MLCube, you can test if your implementation follows the expected I/O of the benchmark. For such, use the following command: medperf test -b <benchmark_uid> -m <path/to/model/mlcube> Then CLI will provide a response as follows: MedPerf 0.0.0 Benchmark Data Preparation: tmp_1_test_1655394059_3 > Preparation cube download complete > MLCommons TorchXRayVision Preprocessor MD5 hash check complete > Cube execution complete > Sanity checks complete > Statistics complete Benchmark Execution: tmp_1_test_1655394059_3 > Metrics cube download complete > MLCommons Metrics MD5 hash check complete > Model cube download complete > MLCommons TorchXRayVision CheXpert DenseNet model MD5 hash check complete > Model execution complete \u2705 Done!","title":"Testing Your MLCube"},{"location":"instructions_model_owners/#submitting-your-mlcube","text":"To submit your MLCube, you should have the following information at hand: mlcube.yaml raw github url with commit hash parameters.yaml raw github url with commit hash URL for the additional files tarball (optional) You can submit your MLCube using the following command: medperf mlcube submit --name <mlcube_name> --mlcube-file Then CLI will return a response as follows: https://raw.githubusercontent.com/aristizabal95/medical/02e142f9a83250a0108e73f955bf4cb6c72f5a0f/cubes/xrv_chex_densenet/mlcube/mlcube.yaml --parameters-file https://raw.githubusercontent.com/aristizabal95/medperf-server/1a0a8c21f92c3d9a162ce5e61732eed2d0eb95cc/app/database/cubes/xrv_chex_densenet/parameters.yaml --additional-file https://storage.googleapis.com/medperf-storage/xrv_chex_densenet.tar.gz --additional-hash c5c408b5f9ef8b1da748e3b1f2d58b8b3eebf96e MedPerf 0.0.0 Additional file hash generated \u2705 Done! Note: Currently, it is not possible to edit your submission, so ensure the information is correct. This will change in the near future, and instructions might need to be updated.","title":"Submitting Your MLCube"},{"location":"instructions_model_owners/#getting-information-about-your-mlcubes","text":"You can get information on your submitted MLCubes through: medperf mlcube ls Then CLI will provide a response as follows: MedPerf 0.0.0 MLCube UID Name State ------------ --------- --------- 5 densenet2 OPERATION","title":"Getting information about your MLCubes"},{"location":"instructions_model_owners/#requesting-association-to-a-benchmark","text":"Once that cube passes the test and is submitted to the platform, you can create an association to the desired benchmark with: medperf mlcube associate -m <mlcube_uid> -b <benchmark_uid> The CLI will return a simple response to confirm the association request: MedPerf 0.0.0 \u2705 Done! Once an association is created, the Benchmark Committee must approve it. If the association is for a benchmark that you own, then it will be automatically approved and you can start using the model with your benchmark.","title":"Requesting Association to a Benchmark"},{"location":"instructions_model_owners/#retrieving-an-association","text":"Existing associations can be retrieved with the following command: medperf association ls And the CLI will provide a list of associations: MedPerf 0.0.0 Dataset UID MLCube UID Benchmark UID Initiated by Status ------------- ------------ --------------- -------------- -------- 32 7 13 APPROVED 33 7 13 APPROVED 35 7 13 APPROVED 38 7 13 APPROVED 37 7 13 PENDING","title":"Retrieving an association"},{"location":"medperf_components/","text":"MedPerf Main Components MedPerf is currently composed of two main components: MedPerf Server The server contains all the metadata necessary to coordinate and execute experiments. No code assets or datasets are stored on the server. The backend server is implemented in Django, and it can be found in the server folder in the MedPerf Github repository. MedPerf Command-Line Interface (CLI) A command-line interface (CLI) is a text-based user interface (UI) used to run programs, manage files and interact with the computer. The MedPerf CLI contains all the necessary tools to interact with the server, preparing datasets for benchmarks and running experiments on the local machine. It can be found in the cli folder in the MedPerf Github repository. The CLI communicates to the server through the API to, for example, authenticate a user, retrieve benchmarks/MLcubes and send results.","title":"MedPerf Components"},{"location":"medperf_components/#medperf-main-components","text":"MedPerf is currently composed of two main components:","title":"MedPerf Main Components"},{"location":"medperf_components/#medperf-server","text":"The server contains all the metadata necessary to coordinate and execute experiments. No code assets or datasets are stored on the server. The backend server is implemented in Django, and it can be found in the server folder in the MedPerf Github repository.","title":"MedPerf Server"},{"location":"medperf_components/#medperf-command-line-interface-cli","text":"A command-line interface (CLI) is a text-based user interface (UI) used to run programs, manage files and interact with the computer. The MedPerf CLI contains all the necessary tools to interact with the server, preparing datasets for benchmarks and running experiments on the local machine. It can be found in the cli folder in the MedPerf Github repository. The CLI communicates to the server through the API to, for example, authenticate a user, retrieve benchmarks/MLcubes and send results.","title":"MedPerf Command-Line Interface (CLI)"},{"location":"mlcubes/","text":"MedPerf MLCubes MLCube is a set of common conventions for creating ML software that can \"plug-and-play\" on many different systems. It is basically a container image with a simple interface and the correct metadata that allows anyone who receives the ML model to run it on their local machine, Google Cloud Platform (GCP), etc. Note: A Docker container image is a lightweight, executable package of software that includes everything needed to run an application, and this container needs to conform to one of the MedPerf interfaces for data preparation, ML models, or evaluation metrics. You can get started with MLCube here . In MedPerf, MLCubes are required for creating benchmarks. A Benchmark Committee will require three cubes to be submitted to the platform before creating a benchmark: Data Preparator MLCube, Reference Model MLCube and Metrics MLCube. The steps to build each of these cubes can be found here . Data Preparator MLCube The Data Preparator MLCube is used to prepare the data for executing the benchmark. Ideally, it can receive different data standards for the task at hand, transforming them into a single, unified standard. Additionally, it ensures the quality and compatibility of the data and computes statistics and metadata for registration purposes. This MLCube provides the following tasks: Prepare: Transforms the input data into the expected output data standard. It receives as input the location of the original data, as well as the location of the labels, and outputs the prepared dataset and accompanying labels. Sanity check: Ensures data integrity of the prepared data. It ideally checks for anomalies and data corruption (e.g. blank images, empty test cases). It constitutes a set of conditions the prepared data should comply with. Statistics: Computes statistics on the prepared data. These statistics are displayed to the user and, if given consent, uploaded to the server for dataset registration. An example is shown as follows: generated_metadata: column statistics: Atelectasis: '0.0': 0.625 '1.0': 0.375 Cardiomegaly: '0.0': 0.67 '1.0': 0.33 Consolidation: '0.0': 0.84 '1.0': 0.16 Edema: '0.0': 0.79 '1.0': 0.21 images statistics: channels: 1 height: max: 224.0 mean: 224.0 min: 224.0 std: 0.0 pixels_max: max: 255.0 mean: 255.0 min: 255.0 std: 0.0 pixels_mean: max: 149.2486049107 mean: 137.7983645568 min: 121.2493223852 std: 5.8131783153 pixels_min: max: 6.0 mean: 0.07 min: 0.0 std: 0.5063416925 pixels_std: max: 79.0369075709 mean: 71.5894559861 min: 64.0834766712 std: 2.8957115261 width: max: 224.0 mean: 224.0 min: 224.0 std: 0.0 labels: - Atelectasis - Cardiomegaly - Consolidation - Edema size: 200 Model MLCube The model MLCube contains a pre-trained machine learning model that is going to be evaluated by the benchmark. It provides the following task: Infer: Obtains predictions on the prepared data. It receives as input the location of the prepared data and outputs predictions. Metrics/Evaluator MLCube The Metrics MLCube is used for computing metrics on the model predictions by comparing it against the provided labels. It provides the following task: Evaluate: Computes the metrics. It receives as input the location of the predictions and the location of the prepared data (which contains the labels) and generates a yaml file with the metrics. Both the Model and Metrics MLCubes can be executed independently if given the correct inputs. Medperf provides a single command for both obtaining predictions and computing metrics on an already prepared dataset.","title":"MLCubes"},{"location":"mlcubes/#medperf-mlcubes","text":"MLCube is a set of common conventions for creating ML software that can \"plug-and-play\" on many different systems. It is basically a container image with a simple interface and the correct metadata that allows anyone who receives the ML model to run it on their local machine, Google Cloud Platform (GCP), etc. Note: A Docker container image is a lightweight, executable package of software that includes everything needed to run an application, and this container needs to conform to one of the MedPerf interfaces for data preparation, ML models, or evaluation metrics. You can get started with MLCube here . In MedPerf, MLCubes are required for creating benchmarks. A Benchmark Committee will require three cubes to be submitted to the platform before creating a benchmark: Data Preparator MLCube, Reference Model MLCube and Metrics MLCube. The steps to build each of these cubes can be found here .","title":"MedPerf MLCubes"},{"location":"mlcubes/#data-preparator-mlcube","text":"The Data Preparator MLCube is used to prepare the data for executing the benchmark. Ideally, it can receive different data standards for the task at hand, transforming them into a single, unified standard. Additionally, it ensures the quality and compatibility of the data and computes statistics and metadata for registration purposes. This MLCube provides the following tasks: Prepare: Transforms the input data into the expected output data standard. It receives as input the location of the original data, as well as the location of the labels, and outputs the prepared dataset and accompanying labels. Sanity check: Ensures data integrity of the prepared data. It ideally checks for anomalies and data corruption (e.g. blank images, empty test cases). It constitutes a set of conditions the prepared data should comply with. Statistics: Computes statistics on the prepared data. These statistics are displayed to the user and, if given consent, uploaded to the server for dataset registration. An example is shown as follows: generated_metadata: column statistics: Atelectasis: '0.0': 0.625 '1.0': 0.375 Cardiomegaly: '0.0': 0.67 '1.0': 0.33 Consolidation: '0.0': 0.84 '1.0': 0.16 Edema: '0.0': 0.79 '1.0': 0.21 images statistics: channels: 1 height: max: 224.0 mean: 224.0 min: 224.0 std: 0.0 pixels_max: max: 255.0 mean: 255.0 min: 255.0 std: 0.0 pixels_mean: max: 149.2486049107 mean: 137.7983645568 min: 121.2493223852 std: 5.8131783153 pixels_min: max: 6.0 mean: 0.07 min: 0.0 std: 0.5063416925 pixels_std: max: 79.0369075709 mean: 71.5894559861 min: 64.0834766712 std: 2.8957115261 width: max: 224.0 mean: 224.0 min: 224.0 std: 0.0 labels: - Atelectasis - Cardiomegaly - Consolidation - Edema size: 200","title":"Data Preparator MLCube"},{"location":"mlcubes/#model-mlcube","text":"The model MLCube contains a pre-trained machine learning model that is going to be evaluated by the benchmark. It provides the following task: Infer: Obtains predictions on the prepared data. It receives as input the location of the prepared data and outputs predictions.","title":"Model MLCube"},{"location":"mlcubes/#metricsevaluator-mlcube","text":"The Metrics MLCube is used for computing metrics on the model predictions by comparing it against the provided labels. It provides the following task: Evaluate: Computes the metrics. It receives as input the location of the predictions and the location of the prepared data (which contains the labels) and generates a yaml file with the metrics. Both the Model and Metrics MLCubes can be executed independently if given the correct inputs. Medperf provides a single command for both obtaining predictions and computing metrics on an already prepared dataset.","title":"Metrics/Evaluator MLCube"},{"location":"roles/","text":"Benchmarking User Roles and Responsibilities There are different types of users/roles that can be created in MedPerf: Benchmark Committee Include regulatory bodies, groups of experts (e.g., clinicians, patient representative groups), and data or Model Owners wishing to drive the evaluation of their model or data. Benchmark Committee do not have admin privileges on the system, but they have elevated permissions regarding the benchmark they define, such as deciding which model and Data Providers will participate. Data Providers Include hospitals, medical practices, research organizations, and healthcare insurance providers that own medical data, register medical data, and execute benchmark requests. Model Owners Include ML researchers and software vendors that own a trained medical ML model and want to evaluate its performance. Platform Provider Organizations like MLCommons that operate a platform that enables a benchmark committee to run benchmarks by connecting Data Providers with Model Owners.","title":"Benchmark Roles"},{"location":"roles/#benchmarking-user-roles-and-responsibilities","text":"There are different types of users/roles that can be created in MedPerf:","title":"Benchmarking User Roles and Responsibilities"},{"location":"roles/#benchmark-committee","text":"Include regulatory bodies, groups of experts (e.g., clinicians, patient representative groups), and data or Model Owners wishing to drive the evaluation of their model or data. Benchmark Committee do not have admin privileges on the system, but they have elevated permissions regarding the benchmark they define, such as deciding which model and Data Providers will participate.","title":"Benchmark Committee"},{"location":"roles/#data-providers","text":"Include hospitals, medical practices, research organizations, and healthcare insurance providers that own medical data, register medical data, and execute benchmark requests.","title":"Data Providers"},{"location":"roles/#model-owners","text":"Include ML researchers and software vendors that own a trained medical ML model and want to evaluate its performance.","title":"Model Owners"},{"location":"roles/#platform-provider","text":"Organizations like MLCommons that operate a platform that enables a benchmark committee to run benchmarks by connecting Data Providers with Model Owners.","title":"Platform Provider"},{"location":"workflow/","text":"MedPerf Benchmarking Workflow Creating a User Currently, the MedPerf administration is the only one able to create users, controlling access to the system and permissions to own a benchmark. For example, if a hospital (Data Provider) wants to have access to MedPerf, they need to contact the MedPerf administrator to add a user. You check more details on how to create a user, host the server, and install the CLI/MedPerf Dependencies here . Establishing a Benchmark Committee The benchmarking process starts with establishing a benchmark committee of healthcare stakeholders (experts, committee), which will identify a clinical problem where an effective ML-based solution can have a significant clinical impact. Recruiting Data and Model Owners The benchmark committee recruits Data Providers and Model Owners either by inviting trusted parties or by making an open call for participation. A higher number of dataset providers recruited can maximize diversity on a global scale. MLCubes Submission MLCubes are the building blocks of an experiment. They are required in order to create a benchmark, thus the three MLCubes (Data Preparator MLCube, Model MLCube, and Metrics MLCube) need to be submitted. Check this section to understand how to submit your MLCubes. The submission process is described as follows: Wrap your logic in an MLCube (with the appropriate MLCube interface); Host the following files on file server: the image file (can be a tarball on a file server, or hosted in an image repository, e.g. dockerhub). The image file is only required if not using an image repository that MLCube knows how to handle; the mlcube.yaml file that describes the image (currently only GitHub hosting is supported); the parameters.yaml file (also GitHub hosted-only) that defines the specific configuration (e.g. hyper-parameters if the MLCube supports multiple options); the additional files tarball, if used (e.g. model weights) Provide the following information to the server (via the MedPerf client command \"mlcube submit\", IIRC): \"name\": \"string\", \"git_mlcube_url\": \"string\", \"git_parameters_url\": \"string\", \"image_tarball_url\": \"string\", \"additional_files_tarball_url\": \"string\", \"user_metadata\": { }, Note: At present, all MLCubes require a GitHub repository to host their mlcube.yaml and parameters.yaml files. The image file is only required if not using an image repository that MLCube knows how to handle and hashes can be passed manually or computed at submission-time by the CLI (preferred). Note: The URLs of the files hosted on github must be UserContent github URLs (domain raw.githubusercontent.com). Below are the steps to get this URL for a specific file: Open the github repository and ensure you are in the correct branch Click on \u201cCommits\u201d at the right top corner of the repository explorer. Locate the latest commit, it is the top most commit. If you are targeting previous versions of your file, make sure to consider the right commit. Click on this button \u201c<>\u201d corresponding to the commit (Browse the repository at this point in the history). Navigate to the file of interest. Click on \u201cRaw\u201d. Copy the url from your browser. Benchmark Submission After submitting the Data Preparator MLCube, Model MLCube, and Metrics MLCube, the Benchmark Committee is now capable of creating a benchmark. Once a benchmark is submitted, the Medperf admin must approve it before it can be seen by other users. Check this section to get the details on how to submit a benchmark. Submitting and Associating Additional Models Once a reference benchmark is submitted by the Benchmark Committee, any user can submit their own Model MLCubes and request an association with the benchmark (check step 4 in the section \u201c Preparing a Dataset \u201d if you are a Data Provider, or check this section if you are a Model Owner). This association request executes the benchmark locally with the given model to ensure compatibility. If the model successfully passes the compatibility test, and its association is approved by the Benchmark Committee, then it becomes part of the benchmark. Dataset Preparation and Association Data Providers that want to be part of the benchmark can prepare their own datasets, register them and associate them with the benchmark. After downloading MLCube in their cloud or on-prem and verifying it is the right piece of software, Data Providers will run the Data Preparator MLCube so that they can extract, preprocess, label, and review the datasets for legal/ethical compliance. If the execution is successful and the dataset has successfully passed the compatibility test, and its association is approved by the Benchmark Committee, then it\u2019s registered with the benchmarking platform and associated with that specific benchmark. Executing the Benchmark Once the benchmark, dataset, and models are registered to the benchmarking platform, the Benchmark Committee notifies Data Providers that models are available for benchmarking, thus they can generate results by executing a model with their local data (check the \u201c Running an Experiment \u201d section to understand how to run an experiment). This procedure retrieves the specified Model MLCube and runs it with the indicated prepared dataset to generate predictions. The Model MLCube will execute the machine learning inference task to generate predictions based on the prepared data. Finally, the Metrics MLCube is retrieved to compute metrics on the predictions. Once results are generated, the user can submit them to the platform. Release Results to Participants The benchmarking platform aggregates the results of running the models against the datasets and shares them according to the Benchmark Committee's policy. The sharing policy controls how much of the data is shared, ranging from a single aggregated metric to a more detailed model-data cross product. A public leaderboard is available to Model Owners who produce the best performances.","title":"Workflow"},{"location":"workflow/#medperf-benchmarking-workflow","text":"","title":"MedPerf Benchmarking Workflow"},{"location":"workflow/#creating-a-user","text":"Currently, the MedPerf administration is the only one able to create users, controlling access to the system and permissions to own a benchmark. For example, if a hospital (Data Provider) wants to have access to MedPerf, they need to contact the MedPerf administrator to add a user. You check more details on how to create a user, host the server, and install the CLI/MedPerf Dependencies here .","title":"Creating a User"},{"location":"workflow/#establishing-a-benchmark-committee","text":"The benchmarking process starts with establishing a benchmark committee of healthcare stakeholders (experts, committee), which will identify a clinical problem where an effective ML-based solution can have a significant clinical impact.","title":"Establishing a Benchmark Committee"},{"location":"workflow/#recruiting-data-and-model-owners","text":"The benchmark committee recruits Data Providers and Model Owners either by inviting trusted parties or by making an open call for participation. A higher number of dataset providers recruited can maximize diversity on a global scale.","title":"Recruiting Data and Model Owners"},{"location":"workflow/#mlcubes-submission","text":"MLCubes are the building blocks of an experiment. They are required in order to create a benchmark, thus the three MLCubes (Data Preparator MLCube, Model MLCube, and Metrics MLCube) need to be submitted. Check this section to understand how to submit your MLCubes. The submission process is described as follows: Wrap your logic in an MLCube (with the appropriate MLCube interface); Host the following files on file server: the image file (can be a tarball on a file server, or hosted in an image repository, e.g. dockerhub). The image file is only required if not using an image repository that MLCube knows how to handle; the mlcube.yaml file that describes the image (currently only GitHub hosting is supported); the parameters.yaml file (also GitHub hosted-only) that defines the specific configuration (e.g. hyper-parameters if the MLCube supports multiple options); the additional files tarball, if used (e.g. model weights) Provide the following information to the server (via the MedPerf client command \"mlcube submit\", IIRC): \"name\": \"string\", \"git_mlcube_url\": \"string\", \"git_parameters_url\": \"string\", \"image_tarball_url\": \"string\", \"additional_files_tarball_url\": \"string\", \"user_metadata\": { }, Note: At present, all MLCubes require a GitHub repository to host their mlcube.yaml and parameters.yaml files. The image file is only required if not using an image repository that MLCube knows how to handle and hashes can be passed manually or computed at submission-time by the CLI (preferred). Note: The URLs of the files hosted on github must be UserContent github URLs (domain raw.githubusercontent.com). Below are the steps to get this URL for a specific file: Open the github repository and ensure you are in the correct branch Click on \u201cCommits\u201d at the right top corner of the repository explorer. Locate the latest commit, it is the top most commit. If you are targeting previous versions of your file, make sure to consider the right commit. Click on this button \u201c<>\u201d corresponding to the commit (Browse the repository at this point in the history). Navigate to the file of interest. Click on \u201cRaw\u201d. Copy the url from your browser.","title":"MLCubes Submission"},{"location":"workflow/#benchmark-submission","text":"After submitting the Data Preparator MLCube, Model MLCube, and Metrics MLCube, the Benchmark Committee is now capable of creating a benchmark. Once a benchmark is submitted, the Medperf admin must approve it before it can be seen by other users. Check this section to get the details on how to submit a benchmark.","title":"Benchmark Submission"},{"location":"workflow/#submitting-and-associating-additional-models","text":"Once a reference benchmark is submitted by the Benchmark Committee, any user can submit their own Model MLCubes and request an association with the benchmark (check step 4 in the section \u201c Preparing a Dataset \u201d if you are a Data Provider, or check this section if you are a Model Owner). This association request executes the benchmark locally with the given model to ensure compatibility. If the model successfully passes the compatibility test, and its association is approved by the Benchmark Committee, then it becomes part of the benchmark.","title":"Submitting and Associating Additional Models"},{"location":"workflow/#dataset-preparation-and-association","text":"Data Providers that want to be part of the benchmark can prepare their own datasets, register them and associate them with the benchmark. After downloading MLCube in their cloud or on-prem and verifying it is the right piece of software, Data Providers will run the Data Preparator MLCube so that they can extract, preprocess, label, and review the datasets for legal/ethical compliance. If the execution is successful and the dataset has successfully passed the compatibility test, and its association is approved by the Benchmark Committee, then it\u2019s registered with the benchmarking platform and associated with that specific benchmark.","title":"Dataset Preparation and Association"},{"location":"workflow/#executing-the-benchmark","text":"Once the benchmark, dataset, and models are registered to the benchmarking platform, the Benchmark Committee notifies Data Providers that models are available for benchmarking, thus they can generate results by executing a model with their local data (check the \u201c Running an Experiment \u201d section to understand how to run an experiment). This procedure retrieves the specified Model MLCube and runs it with the indicated prepared dataset to generate predictions. The Model MLCube will execute the machine learning inference task to generate predictions based on the prepared data. Finally, the Metrics MLCube is retrieved to compute metrics on the predictions. Once results are generated, the user can submit them to the platform.","title":"Executing the Benchmark"},{"location":"workflow/#release-results-to-participants","text":"The benchmarking platform aggregates the results of running the models against the datasets and shares them according to the Benchmark Committee's policy. The sharing policy controls how much of the data is shared, ranging from a single aggregated metric to a more detailed model-data cross product. A public leaderboard is available to Model Owners who produce the best performances.","title":"Release Results to Participants"},{"location":"cli/","text":"Medperf CLI The Medperf CLI is a command-line-interface that provides tools for interacting with the medperf platform, preparing datasets and executing benchmarks on such datasets. How to install: Clone this repo git clone https://github.com/mlcommons/medperf.git Go to the cli folder cd cli Install using pip pip install -e . How to run The CLI provides the following commands: Login: authenticates the CLI with the medperf backend server medperf login List Datasets: Lists all registered datasets by the user medperf dataset ls Create Dataset: Prepares a raw dataset for a specific benchmark medperf dataset create -b <BENCHMARK_UID> -d <DATA_PATH> -l <LABELS_PATH> Submit Dataset: Submits a prepared local dataset to the platform. medperf dataset submit -d <DATASET_UID> Associate Dataset: Associates a prepared dataset with a specific benchmark medperf associate -b <BENCHMARK_UID> -d <DATASET_UID> Run a benchmark: Alias for result create : Runs a specific model from a benchmark with a specified prepared dataset medperf run -b <BENCHMARK_UID> -d <DATASET_UID> -m <MODEL_UID> List Results: Displays all results created by the user medperf result ls Create Result: Runs a specific model from a benchmark with a specified prepared dataset medperf result create -b <BENCHMARK_UID> -d <DATASET_UID> -m <MODEL_UID> Submit Result: Submits already obtained results to the platform medperf result submit -b <BENCHMARK_UID> -d <DATASET_UID> -m <MODEL_UID> List MLCubes: Lists all mlcubes created by the user. Lists all mlcubes if --all is passed medperf mlcube ls [###-all] Submit MLCube: Submits a new mlcube to the platform medperf mlcube submit Associate MLCube: Associates an MLCube to a benchmark medperf mlcube associate -b <BENCHMARK_UID> -m <MODEL_UID> The CLI runs MLCubes behind the scene. These cubes require a container engine like docker, and so that engine must be running before running commands like dataset create or result create","title":"Medperf CLI"},{"location":"cli/#medperf-cli","text":"The Medperf CLI is a command-line-interface that provides tools for interacting with the medperf platform, preparing datasets and executing benchmarks on such datasets.","title":"Medperf CLI"},{"location":"cli/#how-to-install","text":"Clone this repo git clone https://github.com/mlcommons/medperf.git Go to the cli folder cd cli Install using pip pip install -e .","title":"How to install:"},{"location":"cli/#how-to-run","text":"The CLI provides the following commands:","title":"How to run"},{"location":"cli/#login","text":"authenticates the CLI with the medperf backend server medperf login","title":"Login:"},{"location":"cli/#list-datasets","text":"Lists all registered datasets by the user medperf dataset ls","title":"List Datasets:"},{"location":"cli/#create-dataset","text":"Prepares a raw dataset for a specific benchmark medperf dataset create -b <BENCHMARK_UID> -d <DATA_PATH> -l <LABELS_PATH>","title":"Create Dataset:"},{"location":"cli/#submit-dataset","text":"Submits a prepared local dataset to the platform. medperf dataset submit -d <DATASET_UID>","title":"Submit Dataset:"},{"location":"cli/#associate-dataset","text":"Associates a prepared dataset with a specific benchmark medperf associate -b <BENCHMARK_UID> -d <DATASET_UID>","title":"Associate Dataset:"},{"location":"cli/#run-a-benchmark","text":"Alias for result create : Runs a specific model from a benchmark with a specified prepared dataset medperf run -b <BENCHMARK_UID> -d <DATASET_UID> -m <MODEL_UID>","title":"Run a benchmark:"},{"location":"cli/#list-results","text":"Displays all results created by the user medperf result ls","title":"List Results:"},{"location":"cli/#create-result","text":"Runs a specific model from a benchmark with a specified prepared dataset medperf result create -b <BENCHMARK_UID> -d <DATASET_UID> -m <MODEL_UID>","title":"Create Result:"},{"location":"cli/#submit-result","text":"Submits already obtained results to the platform medperf result submit -b <BENCHMARK_UID> -d <DATASET_UID> -m <MODEL_UID>","title":"Submit Result:"},{"location":"cli/#list-mlcubes","text":"Lists all mlcubes created by the user. Lists all mlcubes if --all is passed medperf mlcube ls [###-all]","title":"List MLCubes:"},{"location":"cli/#submit-mlcube","text":"Submits a new mlcube to the platform medperf mlcube submit","title":"Submit MLCube:"},{"location":"cli/#associate-mlcube","text":"Associates an MLCube to a benchmark medperf mlcube associate -b <BENCHMARK_UID> -m <MODEL_UID> The CLI runs MLCubes behind the scene. These cubes require a container engine like docker, and so that engine must be running before running commands like dataset create or result create","title":"Associate MLCube:"},{"location":"reference/SUMMARY/","text":"commands association approval association list auth login password_change benchmark associate benchmark list submit compatibility_test dataset associate create dataset list submit mlcube associate list mlcube submit result create list result submit comms factory interface rest config decorators entities benchmark cube dataset interface result enums exceptions ui cli factory interface stdin utils","title":"SUMMARY"},{"location":"reference/config/","text":"","title":"config"},{"location":"reference/decorators/","text":"clean_except ( func ) Decorator for handling unexpected errors. It allows logging and cleaning the project's directory before throwing the error. Parameters: Name Type Description Default func Callable Function to handle for unexpected errors required Returns: Name Type Description Callable Callable Decorated function Source code in medperf/decorators.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def clean_except ( func : Callable ) -> Callable : \"\"\"Decorator for handling unexpected errors. It allows logging and cleaning the project's directory before throwing the error. Args: func (Callable): Function to handle for unexpected errors Returns: Callable: Decorated function \"\"\" @functools . wraps ( func ) def wrapper ( * args , ** kwargs ): try : logging . info ( f \"Running function ' { func . __name__ } '\" ) func ( * args , ** kwargs ) except Exception as e : logging . error ( \"An unexpected error occured. Terminating.\" ) logging . exception ( e ) pretty_error ( str ( e )) return wrapper","title":"decorators"},{"location":"reference/decorators/#decorators.clean_except","text":"Decorator for handling unexpected errors. It allows logging and cleaning the project's directory before throwing the error. Parameters: Name Type Description Default func Callable Function to handle for unexpected errors required Returns: Name Type Description Callable Callable Decorated function Source code in medperf/decorators.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def clean_except ( func : Callable ) -> Callable : \"\"\"Decorator for handling unexpected errors. It allows logging and cleaning the project's directory before throwing the error. Args: func (Callable): Function to handle for unexpected errors Returns: Callable: Decorated function \"\"\" @functools . wraps ( func ) def wrapper ( * args , ** kwargs ): try : logging . info ( f \"Running function ' { func . __name__ } '\" ) func ( * args , ** kwargs ) except Exception as e : logging . error ( \"An unexpected error occured. Terminating.\" ) logging . exception ( e ) pretty_error ( str ( e )) return wrapper","title":"clean_except()"},{"location":"reference/enums/","text":"","title":"enums"},{"location":"reference/exceptions/","text":"CommunicationAuthenticationError Bases: Exception Raised when the communication interface can't handle an authentication request Source code in medperf/exceptions.py 9 10 class CommunicationAuthenticationError ( Exception ): \"\"\"Raised when the communication interface can't handle an authentication request\"\"\" CommunicationRequestError Bases: Exception Raised when the communication interface can't handle a request appropiately Source code in medperf/exceptions.py 5 6 class CommunicationRequestError ( Exception ): \"\"\"Raised when the communication interface can't handle a request appropiately\"\"\" CommunicationRetrievalError Bases: Exception Raised when the communication interface can't retrieve an element Source code in medperf/exceptions.py 1 2 class CommunicationRetrievalError ( Exception ): \"\"\"Raised when the communication interface can't retrieve an element\"\"\" EntityRetrievalError Bases: Exception Raised when an entity could not be retrieved Source code in medperf/exceptions.py 21 22 class EntityRetrievalError ( Exception ): \"\"\"Raised when an entity could not be retrieved\"\"\" InvalidArgumentError Bases: Exception Raised when an argument or set of arguments are consided invalid Source code in medperf/exceptions.py 17 18 class InvalidArgumentError ( Exception ): \"\"\"Raised when an argument or set of arguments are consided invalid\"\"\" InvalidEntityError Bases: Exception Raised when an entity is considered invalid Source code in medperf/exceptions.py 13 14 class InvalidEntityError ( Exception ): \"\"\"Raised when an entity is considered invalid\"\"\"","title":"exceptions"},{"location":"reference/exceptions/#exceptions.CommunicationAuthenticationError","text":"Bases: Exception Raised when the communication interface can't handle an authentication request Source code in medperf/exceptions.py 9 10 class CommunicationAuthenticationError ( Exception ): \"\"\"Raised when the communication interface can't handle an authentication request\"\"\"","title":"CommunicationAuthenticationError"},{"location":"reference/exceptions/#exceptions.CommunicationRequestError","text":"Bases: Exception Raised when the communication interface can't handle a request appropiately Source code in medperf/exceptions.py 5 6 class CommunicationRequestError ( Exception ): \"\"\"Raised when the communication interface can't handle a request appropiately\"\"\"","title":"CommunicationRequestError"},{"location":"reference/exceptions/#exceptions.CommunicationRetrievalError","text":"Bases: Exception Raised when the communication interface can't retrieve an element Source code in medperf/exceptions.py 1 2 class CommunicationRetrievalError ( Exception ): \"\"\"Raised when the communication interface can't retrieve an element\"\"\"","title":"CommunicationRetrievalError"},{"location":"reference/exceptions/#exceptions.EntityRetrievalError","text":"Bases: Exception Raised when an entity could not be retrieved Source code in medperf/exceptions.py 21 22 class EntityRetrievalError ( Exception ): \"\"\"Raised when an entity could not be retrieved\"\"\"","title":"EntityRetrievalError"},{"location":"reference/exceptions/#exceptions.InvalidArgumentError","text":"Bases: Exception Raised when an argument or set of arguments are consided invalid Source code in medperf/exceptions.py 17 18 class InvalidArgumentError ( Exception ): \"\"\"Raised when an argument or set of arguments are consided invalid\"\"\"","title":"InvalidArgumentError"},{"location":"reference/exceptions/#exceptions.InvalidEntityError","text":"Bases: Exception Raised when an entity is considered invalid Source code in medperf/exceptions.py 13 14 class InvalidEntityError ( Exception ): \"\"\"Raised when an entity is considered invalid\"\"\"","title":"InvalidEntityError"},{"location":"reference/utils/","text":"approval_prompt ( msg ) Helper function for prompting the user for things they have to explicitly approve. Parameters: Name Type Description Default msg str What message to ask the user for approval. required Returns: Name Type Description bool bool Wether the user explicitly approved or not. Source code in medperf/utils.py 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 def approval_prompt ( msg : str ) -> bool : \"\"\"Helper function for prompting the user for things they have to explicitly approve. Args: msg (str): What message to ask the user for approval. Returns: bool: Wether the user explicitly approved or not. \"\"\" logging . info ( \"Prompting for user's approval\" ) ui = config . ui approval = None while approval is None or approval not in \"yn\" : approval = ui . prompt ( msg . strip () + \" \" ) . lower () logging . info ( f \"User answered approval with { approval } \" ) return approval == \"y\" check_cube_validity ( cube ) Helper function for pretty printing the cube validity process. Parameters: Name Type Description Default cube Cube Cube to check for validity required ui UI Instance of an UI implementation required Source code in medperf/utils.py 254 255 256 257 258 259 260 261 262 263 264 265 266 267 def check_cube_validity ( cube : \"Cube\" ): \"\"\"Helper function for pretty printing the cube validity process. Args: cube (Cube): Cube to check for validity ui (UI): Instance of an UI implementation \"\"\" logging . info ( f \"Checking cube { cube . name } validity\" ) ui = config . ui ui . text = \"Checking cube MD5 hash...\" if not cube . is_valid (): raise InvalidEntityError ( \"MD5 hash doesn't match\" ) logging . info ( f \"Cube { cube . name } is valid\" ) ui . print ( f \"> { cube . name } MD5 hash check complete\" ) cleanup ( extra_paths = []) Removes clutter and unused files from the medperf folder structure. Source code in medperf/utils.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def cleanup ( extra_paths : List [ str ] = []): \"\"\"Removes clutter and unused files from the medperf folder structure. \"\"\" if not config . cleanup : logging . info ( \"Cleanup disabled\" ) return tmp_path = storage_path ( config . tmp_storage ) extra_paths . append ( tmp_path ) for path in extra_paths : if os . path . exists ( path ): logging . info ( f \"Removing clutter path: { path } \" ) try : rmtree ( path ) except OSError as e : logging . error ( \"Could not remove clutter path\" ) raise e cleanup_dsets () cleanup_cubes () cleanup_benchmarks () cleanup_benchmarks () Removes clutter related to benchmarks Source code in medperf/utils.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def cleanup_benchmarks (): \"\"\"Removes clutter related to benchmarks \"\"\" bmks_path = storage_path ( config . benchmarks_storage ) bmks = os . listdir ( bmks_path ) clutter_bmks = [ bmk for bmk in bmks if bmk . startswith ( config . tmp_prefix )] for bmk in clutter_bmks : logging . info ( f \"Removing clutter benchmark: { bmk } \" ) bmk_path = os . path . join ( bmks_path , bmk ) if os . path . exists ( bmk_path ): try : rmtree ( bmk_path ) except OSError as e : logging . error ( f \"Could not remove benchmark { bmk } \" ) raise e cleanup_cubes () Removes clutter related to cubes Source code in medperf/utils.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 def cleanup_cubes (): \"\"\"Removes clutter related to cubes \"\"\" cubes_path = storage_path ( config . cubes_storage ) cubes = get_uids ( cubes_path ) test_prefix = config . test_cube_prefix submission = config . cube_submission_id clutter_cubes = [ cube for cube in cubes if cube . startswith ( test_prefix ) or cube == submission ] for cube in clutter_cubes : logging . info ( f \"Removing clutter cube: { cube } \" ) cube_path = os . path . join ( cubes_path , cube ) if os . path . exists ( cube_path ): try : if os . path . islink ( cube_path ): os . unlink ( cube_path ) else : rmtree ( cube_path ) except OSError as e : logging . error ( f \"Could not remove cube { cube } \" ) raise e cleanup_dsets () Removes clutter related to datsets Source code in medperf/utils.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def cleanup_dsets (): \"\"\"Removes clutter related to datsets \"\"\" dsets_path = storage_path ( config . data_storage ) dsets = get_uids ( dsets_path ) tmp_prefix = config . tmp_prefix test_prefix = config . test_dset_prefix clutter_dsets = [ dset for dset in dsets if dset . startswith ( tmp_prefix ) or dset . startswith ( test_prefix ) ] for dset in clutter_dsets : logging . info ( f \"Removing clutter dataset: { dset } \" ) dset_path = os . path . join ( dsets_path , dset ) if os . path . exists ( dset_path ): try : rmtree ( dset_path ) except OSError as e : logging . error ( f \"Could not remove dataset { dset } \" ) raise e combine_proc_sp_text ( proc ) Combines the output of a process and the spinner. Joins any string captured from the process with the spinner current text. Any strings ending with any other character from the subprocess will be returned later. Parameters: Name Type Description Default proc spawn a pexpect spawned child required ui UI An instance of an UI implementation required Returns: Name Type Description str str all non-carriage-return-ending string captured from proc Source code in medperf/utils.py 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 def combine_proc_sp_text ( proc : spawn ) -> str : \"\"\"Combines the output of a process and the spinner. Joins any string captured from the process with the spinner current text. Any strings ending with any other character from the subprocess will be returned later. Args: proc (spawn): a pexpect spawned child ui (UI): An instance of an UI implementation Returns: str: all non-carriage-return-ending string captured from proc \"\"\" ui = config . ui static_text = ui . text proc_out = \"\" while proc . isalive (): try : line = byte = proc . read ( 1 ) except TIMEOUT : logging . error ( \"Process timed out\" ) raise TIMEOUT ( \"Process timed out\" ) while byte and not re . match ( b \"[ \\r\\n ]\" , byte ): byte = proc . read ( 1 ) line += byte if not byte : break line = line . decode ( \"utf-8\" , \"ignore\" ) if line : # add to proc_out list for logging proc_out += line ui . text = ( f \" { static_text } { Fore . WHITE }{ Style . DIM }{ line . strip () }{ Style . RESET_ALL } \" ) return proc_out cube_path ( uid ) Gets the path for a given cube. Parameters: Name Type Description Default uid int Cube UID. required Returns: Name Type Description str str Location of the cube folder structure. Source code in medperf/utils.py 214 215 216 217 218 219 220 221 222 223 def cube_path ( uid : int ) -> str : \"\"\"Gets the path for a given cube. Args: uid (int): Cube UID. Returns: str: Location of the cube folder structure. \"\"\" return os . path . join ( storage_path ( config . cubes_storage ), str ( uid )) dict_pretty_print ( in_dict ) Helper function for distinctively printing dictionaries with yaml format. Parameters: Name Type Description Default in_dict dict dictionary to print required Source code in medperf/utils.py 315 316 317 318 319 320 321 322 323 324 325 326 327 328 def dict_pretty_print ( in_dict : dict ): \"\"\"Helper function for distinctively printing dictionaries with yaml format. Args: in_dict (dict): dictionary to print \"\"\" logging . debug ( f \"Printing dictionary to the user: { in_dict } \" ) ui = config . ui ui . print () ui . print ( \"=\" * 20 ) in_dict = { k : v for ( k , v ) in in_dict . items () if v is not None } ui . print ( yaml . dump ( in_dict )) logging . debug ( f \"Dictionary printed to the user: { in_dict } \" ) ui . print ( \"=\" * 20 ) generate_tmp_datapath () Builds a temporary folder for prepared but yet-to-register datasets. Returns: Name Type Description str str General temporary folder location str str Specific data path for the temporary dataset Source code in medperf/utils.py 226 227 228 229 230 231 232 233 234 235 236 237 def generate_tmp_datapath () -> Tuple [ str , str ]: \"\"\"Builds a temporary folder for prepared but yet-to-register datasets. Returns: str: General temporary folder location str: Specific data path for the temporary dataset \"\"\" uid = generate_tmp_uid () tmp = config . tmp_prefix + uid out_path = os . path . join ( storage_path ( config . data_storage ), tmp ) out_path = os . path . abspath ( out_path ) return out_path generate_tmp_uid () Generates a temporary uid by means of getting the current timestamp with a random salt Returns: Name Type Description str str generated temporary uid Source code in medperf/utils.py 240 241 242 243 244 245 246 247 248 249 250 251 def generate_tmp_uid () -> str : \"\"\"Generates a temporary uid by means of getting the current timestamp with a random salt Returns: str: generated temporary uid \"\"\" dt = datetime . utcnow () ts_int = int ( datetime . timestamp ( dt )) salt = random . randint ( - ts_int , ts_int ) ts = str ( ts_int + salt ) return ts get_file_sha1 ( path ) Calculates the sha1 hash for a given file. Parameters: Name Type Description Default path str Location of the file of interest. required Returns: Name Type Description str str Calculated hash Source code in medperf/utils.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def get_file_sha1 ( path : str ) -> str : \"\"\"Calculates the sha1 hash for a given file. Args: path (str): Location of the file of interest. Returns: str: Calculated hash \"\"\" logging . debug ( \"Calculating SHA1 hash for file {} \" . format ( path )) BUF_SIZE = 65536 sha1 = hashlib . sha1 () with open ( path , \"rb\" ) as f : while True : data = f . read ( BUF_SIZE ) if not data : break sha1 . update ( data ) sha_val = sha1 . hexdigest () logging . debug ( f \"SHA1 hash for file { path } : { sha_val } \" ) return sha_val get_folder_sha1 ( path ) Generates a hash for all the contents of the folder. This procedure hashes all of the files in the folder, sorts them and then hashes that list. Parameters: Name Type Description Default path str Folder to hash required Returns: Name Type Description str str sha1 hash of the whole folder Source code in medperf/utils.py 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 def get_folder_sha1 ( path : str ) -> str : \"\"\"Generates a hash for all the contents of the folder. This procedure hashes all of the files in the folder, sorts them and then hashes that list. Args: path (str): Folder to hash Returns: str: sha1 hash of the whole folder \"\"\" hashes = [] for root , _ , files in os . walk ( path , topdown = False ): for file in files : logging . debug ( f \"Hashing file { file } \" ) filepath = os . path . join ( root , file ) hashes . append ( get_file_sha1 ( filepath )) hashes = sorted ( hashes ) sha1 = hashlib . sha1 () for hash in hashes : sha1 . update ( hash . encode ( \"utf-8\" )) hash_val = sha1 . hexdigest () logging . debug ( f \"Folder hash: { hash_val } \" ) return hash_val get_uids ( path ) Retrieves the UID of all the elements in the specified path. Returns: Type Description List [ str ] List[str]: UIDs of objects in path. Source code in medperf/utils.py 177 178 179 180 181 182 183 184 185 186 187 def get_uids ( path : str ) -> List [ str ]: \"\"\"Retrieves the UID of all the elements in the specified path. Returns: List[str]: UIDs of objects in path. \"\"\" logging . debug ( \"Retrieving datasets\" ) uids = next ( os . walk ( path ))[ 1 ] logging . debug ( f \"Found { len ( uids ) } datasets\" ) logging . debug ( f \"Datasets: { uids } \" ) return uids init_storage () Builds the general medperf folder structure. Source code in medperf/utils.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def init_storage (): \"\"\"Builds the general medperf folder structure. \"\"\" logging . info ( \"Initializing storage\" ) parent = config . storage data = storage_path ( config . data_storage ) cubes = storage_path ( config . cubes_storage ) results = storage_path ( config . results_storage ) tmp = storage_path ( config . tmp_storage ) bmks = storage_path ( config . benchmarks_storage ) demo = storage_path ( config . demo_data_storage ) log = storage_path ( config . logs_storage ) dirs = [ parent , bmks , data , cubes , results , tmp , demo , log ] for dir in dirs : logging . info ( f \"Creating { dir } directory\" ) try : os . makedirs ( dir , exist_ok = True ) except FileExistsError : logging . warning ( f \"Tried to create existing folder { dir } \" ) pretty_error ( msg , clean = True , add_instructions = True ) Prints an error message with typer protocol and exits the script Parameters: Name Type Description Default msg str Error message to show to the user required clean bool Run the cleanup process before exiting. Defaults to True. True add_instructions bool Show additional instructions to the user. Defualts to True. True Source code in medperf/utils.py 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def pretty_error ( msg : str , clean : bool = True , add_instructions = True ): \"\"\"Prints an error message with typer protocol and exits the script Args: msg (str): Error message to show to the user clean (bool, optional): Run the cleanup process before exiting. Defaults to True. add_instructions (bool, optional): Show additional instructions to the user. Defualts to True. \"\"\" ui = config . ui logging . warning ( \"MedPerf had to stop execution. See logs above for more information\" ) if msg [ - 1 ] != \".\" : msg = msg + \".\" if add_instructions : msg += f \" See logs at { config . log_file } for more information\" ui . print_error ( msg ) if clean : cleanup () sys . exit ( 1 ) sanitize_json ( data ) Makes sure the input data is JSON compliant. Parameters: Name Type Description Default data dict dictionary containing data to be represented as JSON. required Returns: Name Type Description dict dict sanitized dictionary Source code in medperf/utils.py 450 451 452 453 454 455 456 457 458 459 460 461 462 463 def sanitize_json ( data : dict ) -> dict : \"\"\"Makes sure the input data is JSON compliant. Args: data (dict): dictionary containing data to be represented as JSON. Returns: dict: sanitized dictionary \"\"\" json_string = json . dumps ( data ) json_string = re . sub ( r \"\\bNaN\\b\" , '\"nan\"' , json_string ) json_string = re . sub ( r \"(-?)\\bInfinity\\b\" , r '\"\\1Infinity\"' , json_string ) data = json . loads ( json_string ) return data set_unique_tmp_config () Set current process' temporary unique names Enables simultaneous execution without cleanup collision Source code in medperf/utils.py 76 77 78 79 80 81 82 83 84 85 def set_unique_tmp_config (): \"\"\"Set current process' temporary unique names Enables simultaneous execution without cleanup collision \"\"\" pid = str ( os . getpid ()) config . tmp_storage += pid config . tmp_prefix += pid config . test_dset_prefix += pid config . test_cube_prefix += pid config . cube_submission_id += pid storage_path ( subpath ) Helper function that converts a path to storage-related path Source code in medperf/utils.py 25 26 27 def storage_path ( subpath : str ): \"\"\"Helper function that converts a path to storage-related path\"\"\" return os . path . join ( config . storage , subpath ) untar ( filepath , remove = True ) Untars and optionally removes the tar.gz file Parameters: Name Type Description Default filepath str Path where the tar.gz file can be found. required remove bool Wether to delete the tar.gz file. Defaults to True. True Returns: Name Type Description str str location where the untared files can be found. Source code in medperf/utils.py 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 def untar ( filepath : str , remove : bool = True ) -> str : \"\"\"Untars and optionally removes the tar.gz file Args: filepath (str): Path where the tar.gz file can be found. remove (bool): Wether to delete the tar.gz file. Defaults to True. Returns: str: location where the untared files can be found. \"\"\" logging . info ( f \"Uncompressing tar.gz at { filepath } \" ) addpath = str ( Path ( filepath ) . parent ) tar = tarfile . open ( filepath ) tar . extractall ( addpath ) tar . close () # OS Specific issue: Mac Creates superfluous files with tarfile library [ os . remove ( spurious_file ) for spurious_file in glob ( addpath + \"/**/._*\" , recursive = True ) ] if remove : logging . info ( f \"Deleting { filepath } \" ) os . remove ( filepath ) return addpath","title":"utils"},{"location":"reference/utils/#utils.approval_prompt","text":"Helper function for prompting the user for things they have to explicitly approve. Parameters: Name Type Description Default msg str What message to ask the user for approval. required Returns: Name Type Description bool bool Wether the user explicitly approved or not. Source code in medperf/utils.py 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 def approval_prompt ( msg : str ) -> bool : \"\"\"Helper function for prompting the user for things they have to explicitly approve. Args: msg (str): What message to ask the user for approval. Returns: bool: Wether the user explicitly approved or not. \"\"\" logging . info ( \"Prompting for user's approval\" ) ui = config . ui approval = None while approval is None or approval not in \"yn\" : approval = ui . prompt ( msg . strip () + \" \" ) . lower () logging . info ( f \"User answered approval with { approval } \" ) return approval == \"y\"","title":"approval_prompt()"},{"location":"reference/utils/#utils.check_cube_validity","text":"Helper function for pretty printing the cube validity process. Parameters: Name Type Description Default cube Cube Cube to check for validity required ui UI Instance of an UI implementation required Source code in medperf/utils.py 254 255 256 257 258 259 260 261 262 263 264 265 266 267 def check_cube_validity ( cube : \"Cube\" ): \"\"\"Helper function for pretty printing the cube validity process. Args: cube (Cube): Cube to check for validity ui (UI): Instance of an UI implementation \"\"\" logging . info ( f \"Checking cube { cube . name } validity\" ) ui = config . ui ui . text = \"Checking cube MD5 hash...\" if not cube . is_valid (): raise InvalidEntityError ( \"MD5 hash doesn't match\" ) logging . info ( f \"Cube { cube . name } is valid\" ) ui . print ( f \"> { cube . name } MD5 hash check complete\" )","title":"check_cube_validity()"},{"location":"reference/utils/#utils.cleanup","text":"Removes clutter and unused files from the medperf folder structure. Source code in medperf/utils.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def cleanup ( extra_paths : List [ str ] = []): \"\"\"Removes clutter and unused files from the medperf folder structure. \"\"\" if not config . cleanup : logging . info ( \"Cleanup disabled\" ) return tmp_path = storage_path ( config . tmp_storage ) extra_paths . append ( tmp_path ) for path in extra_paths : if os . path . exists ( path ): logging . info ( f \"Removing clutter path: { path } \" ) try : rmtree ( path ) except OSError as e : logging . error ( \"Could not remove clutter path\" ) raise e cleanup_dsets () cleanup_cubes () cleanup_benchmarks ()","title":"cleanup()"},{"location":"reference/utils/#utils.cleanup_benchmarks","text":"Removes clutter related to benchmarks Source code in medperf/utils.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def cleanup_benchmarks (): \"\"\"Removes clutter related to benchmarks \"\"\" bmks_path = storage_path ( config . benchmarks_storage ) bmks = os . listdir ( bmks_path ) clutter_bmks = [ bmk for bmk in bmks if bmk . startswith ( config . tmp_prefix )] for bmk in clutter_bmks : logging . info ( f \"Removing clutter benchmark: { bmk } \" ) bmk_path = os . path . join ( bmks_path , bmk ) if os . path . exists ( bmk_path ): try : rmtree ( bmk_path ) except OSError as e : logging . error ( f \"Could not remove benchmark { bmk } \" ) raise e","title":"cleanup_benchmarks()"},{"location":"reference/utils/#utils.cleanup_cubes","text":"Removes clutter related to cubes Source code in medperf/utils.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 def cleanup_cubes (): \"\"\"Removes clutter related to cubes \"\"\" cubes_path = storage_path ( config . cubes_storage ) cubes = get_uids ( cubes_path ) test_prefix = config . test_cube_prefix submission = config . cube_submission_id clutter_cubes = [ cube for cube in cubes if cube . startswith ( test_prefix ) or cube == submission ] for cube in clutter_cubes : logging . info ( f \"Removing clutter cube: { cube } \" ) cube_path = os . path . join ( cubes_path , cube ) if os . path . exists ( cube_path ): try : if os . path . islink ( cube_path ): os . unlink ( cube_path ) else : rmtree ( cube_path ) except OSError as e : logging . error ( f \"Could not remove cube { cube } \" ) raise e","title":"cleanup_cubes()"},{"location":"reference/utils/#utils.cleanup_dsets","text":"Removes clutter related to datsets Source code in medperf/utils.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def cleanup_dsets (): \"\"\"Removes clutter related to datsets \"\"\" dsets_path = storage_path ( config . data_storage ) dsets = get_uids ( dsets_path ) tmp_prefix = config . tmp_prefix test_prefix = config . test_dset_prefix clutter_dsets = [ dset for dset in dsets if dset . startswith ( tmp_prefix ) or dset . startswith ( test_prefix ) ] for dset in clutter_dsets : logging . info ( f \"Removing clutter dataset: { dset } \" ) dset_path = os . path . join ( dsets_path , dset ) if os . path . exists ( dset_path ): try : rmtree ( dset_path ) except OSError as e : logging . error ( f \"Could not remove dataset { dset } \" ) raise e","title":"cleanup_dsets()"},{"location":"reference/utils/#utils.combine_proc_sp_text","text":"Combines the output of a process and the spinner. Joins any string captured from the process with the spinner current text. Any strings ending with any other character from the subprocess will be returned later. Parameters: Name Type Description Default proc spawn a pexpect spawned child required ui UI An instance of an UI implementation required Returns: Name Type Description str str all non-carriage-return-ending string captured from proc Source code in medperf/utils.py 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 def combine_proc_sp_text ( proc : spawn ) -> str : \"\"\"Combines the output of a process and the spinner. Joins any string captured from the process with the spinner current text. Any strings ending with any other character from the subprocess will be returned later. Args: proc (spawn): a pexpect spawned child ui (UI): An instance of an UI implementation Returns: str: all non-carriage-return-ending string captured from proc \"\"\" ui = config . ui static_text = ui . text proc_out = \"\" while proc . isalive (): try : line = byte = proc . read ( 1 ) except TIMEOUT : logging . error ( \"Process timed out\" ) raise TIMEOUT ( \"Process timed out\" ) while byte and not re . match ( b \"[ \\r\\n ]\" , byte ): byte = proc . read ( 1 ) line += byte if not byte : break line = line . decode ( \"utf-8\" , \"ignore\" ) if line : # add to proc_out list for logging proc_out += line ui . text = ( f \" { static_text } { Fore . WHITE }{ Style . DIM }{ line . strip () }{ Style . RESET_ALL } \" ) return proc_out","title":"combine_proc_sp_text()"},{"location":"reference/utils/#utils.cube_path","text":"Gets the path for a given cube. Parameters: Name Type Description Default uid int Cube UID. required Returns: Name Type Description str str Location of the cube folder structure. Source code in medperf/utils.py 214 215 216 217 218 219 220 221 222 223 def cube_path ( uid : int ) -> str : \"\"\"Gets the path for a given cube. Args: uid (int): Cube UID. Returns: str: Location of the cube folder structure. \"\"\" return os . path . join ( storage_path ( config . cubes_storage ), str ( uid ))","title":"cube_path()"},{"location":"reference/utils/#utils.dict_pretty_print","text":"Helper function for distinctively printing dictionaries with yaml format. Parameters: Name Type Description Default in_dict dict dictionary to print required Source code in medperf/utils.py 315 316 317 318 319 320 321 322 323 324 325 326 327 328 def dict_pretty_print ( in_dict : dict ): \"\"\"Helper function for distinctively printing dictionaries with yaml format. Args: in_dict (dict): dictionary to print \"\"\" logging . debug ( f \"Printing dictionary to the user: { in_dict } \" ) ui = config . ui ui . print () ui . print ( \"=\" * 20 ) in_dict = { k : v for ( k , v ) in in_dict . items () if v is not None } ui . print ( yaml . dump ( in_dict )) logging . debug ( f \"Dictionary printed to the user: { in_dict } \" ) ui . print ( \"=\" * 20 )","title":"dict_pretty_print()"},{"location":"reference/utils/#utils.generate_tmp_datapath","text":"Builds a temporary folder for prepared but yet-to-register datasets. Returns: Name Type Description str str General temporary folder location str str Specific data path for the temporary dataset Source code in medperf/utils.py 226 227 228 229 230 231 232 233 234 235 236 237 def generate_tmp_datapath () -> Tuple [ str , str ]: \"\"\"Builds a temporary folder for prepared but yet-to-register datasets. Returns: str: General temporary folder location str: Specific data path for the temporary dataset \"\"\" uid = generate_tmp_uid () tmp = config . tmp_prefix + uid out_path = os . path . join ( storage_path ( config . data_storage ), tmp ) out_path = os . path . abspath ( out_path ) return out_path","title":"generate_tmp_datapath()"},{"location":"reference/utils/#utils.generate_tmp_uid","text":"Generates a temporary uid by means of getting the current timestamp with a random salt Returns: Name Type Description str str generated temporary uid Source code in medperf/utils.py 240 241 242 243 244 245 246 247 248 249 250 251 def generate_tmp_uid () -> str : \"\"\"Generates a temporary uid by means of getting the current timestamp with a random salt Returns: str: generated temporary uid \"\"\" dt = datetime . utcnow () ts_int = int ( datetime . timestamp ( dt )) salt = random . randint ( - ts_int , ts_int ) ts = str ( ts_int + salt ) return ts","title":"generate_tmp_uid()"},{"location":"reference/utils/#utils.get_file_sha1","text":"Calculates the sha1 hash for a given file. Parameters: Name Type Description Default path str Location of the file of interest. required Returns: Name Type Description str str Calculated hash Source code in medperf/utils.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def get_file_sha1 ( path : str ) -> str : \"\"\"Calculates the sha1 hash for a given file. Args: path (str): Location of the file of interest. Returns: str: Calculated hash \"\"\" logging . debug ( \"Calculating SHA1 hash for file {} \" . format ( path )) BUF_SIZE = 65536 sha1 = hashlib . sha1 () with open ( path , \"rb\" ) as f : while True : data = f . read ( BUF_SIZE ) if not data : break sha1 . update ( data ) sha_val = sha1 . hexdigest () logging . debug ( f \"SHA1 hash for file { path } : { sha_val } \" ) return sha_val","title":"get_file_sha1()"},{"location":"reference/utils/#utils.get_folder_sha1","text":"Generates a hash for all the contents of the folder. This procedure hashes all of the files in the folder, sorts them and then hashes that list. Parameters: Name Type Description Default path str Folder to hash required Returns: Name Type Description str str sha1 hash of the whole folder Source code in medperf/utils.py 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 def get_folder_sha1 ( path : str ) -> str : \"\"\"Generates a hash for all the contents of the folder. This procedure hashes all of the files in the folder, sorts them and then hashes that list. Args: path (str): Folder to hash Returns: str: sha1 hash of the whole folder \"\"\" hashes = [] for root , _ , files in os . walk ( path , topdown = False ): for file in files : logging . debug ( f \"Hashing file { file } \" ) filepath = os . path . join ( root , file ) hashes . append ( get_file_sha1 ( filepath )) hashes = sorted ( hashes ) sha1 = hashlib . sha1 () for hash in hashes : sha1 . update ( hash . encode ( \"utf-8\" )) hash_val = sha1 . hexdigest () logging . debug ( f \"Folder hash: { hash_val } \" ) return hash_val","title":"get_folder_sha1()"},{"location":"reference/utils/#utils.get_uids","text":"Retrieves the UID of all the elements in the specified path. Returns: Type Description List [ str ] List[str]: UIDs of objects in path. Source code in medperf/utils.py 177 178 179 180 181 182 183 184 185 186 187 def get_uids ( path : str ) -> List [ str ]: \"\"\"Retrieves the UID of all the elements in the specified path. Returns: List[str]: UIDs of objects in path. \"\"\" logging . debug ( \"Retrieving datasets\" ) uids = next ( os . walk ( path ))[ 1 ] logging . debug ( f \"Found { len ( uids ) } datasets\" ) logging . debug ( f \"Datasets: { uids } \" ) return uids","title":"get_uids()"},{"location":"reference/utils/#utils.init_storage","text":"Builds the general medperf folder structure. Source code in medperf/utils.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def init_storage (): \"\"\"Builds the general medperf folder structure. \"\"\" logging . info ( \"Initializing storage\" ) parent = config . storage data = storage_path ( config . data_storage ) cubes = storage_path ( config . cubes_storage ) results = storage_path ( config . results_storage ) tmp = storage_path ( config . tmp_storage ) bmks = storage_path ( config . benchmarks_storage ) demo = storage_path ( config . demo_data_storage ) log = storage_path ( config . logs_storage ) dirs = [ parent , bmks , data , cubes , results , tmp , demo , log ] for dir in dirs : logging . info ( f \"Creating { dir } directory\" ) try : os . makedirs ( dir , exist_ok = True ) except FileExistsError : logging . warning ( f \"Tried to create existing folder { dir } \" )","title":"init_storage()"},{"location":"reference/utils/#utils.pretty_error","text":"Prints an error message with typer protocol and exits the script Parameters: Name Type Description Default msg str Error message to show to the user required clean bool Run the cleanup process before exiting. Defaults to True. True add_instructions bool Show additional instructions to the user. Defualts to True. True Source code in medperf/utils.py 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def pretty_error ( msg : str , clean : bool = True , add_instructions = True ): \"\"\"Prints an error message with typer protocol and exits the script Args: msg (str): Error message to show to the user clean (bool, optional): Run the cleanup process before exiting. Defaults to True. add_instructions (bool, optional): Show additional instructions to the user. Defualts to True. \"\"\" ui = config . ui logging . warning ( \"MedPerf had to stop execution. See logs above for more information\" ) if msg [ - 1 ] != \".\" : msg = msg + \".\" if add_instructions : msg += f \" See logs at { config . log_file } for more information\" ui . print_error ( msg ) if clean : cleanup () sys . exit ( 1 )","title":"pretty_error()"},{"location":"reference/utils/#utils.sanitize_json","text":"Makes sure the input data is JSON compliant. Parameters: Name Type Description Default data dict dictionary containing data to be represented as JSON. required Returns: Name Type Description dict dict sanitized dictionary Source code in medperf/utils.py 450 451 452 453 454 455 456 457 458 459 460 461 462 463 def sanitize_json ( data : dict ) -> dict : \"\"\"Makes sure the input data is JSON compliant. Args: data (dict): dictionary containing data to be represented as JSON. Returns: dict: sanitized dictionary \"\"\" json_string = json . dumps ( data ) json_string = re . sub ( r \"\\bNaN\\b\" , '\"nan\"' , json_string ) json_string = re . sub ( r \"(-?)\\bInfinity\\b\" , r '\"\\1Infinity\"' , json_string ) data = json . loads ( json_string ) return data","title":"sanitize_json()"},{"location":"reference/utils/#utils.set_unique_tmp_config","text":"Set current process' temporary unique names Enables simultaneous execution without cleanup collision Source code in medperf/utils.py 76 77 78 79 80 81 82 83 84 85 def set_unique_tmp_config (): \"\"\"Set current process' temporary unique names Enables simultaneous execution without cleanup collision \"\"\" pid = str ( os . getpid ()) config . tmp_storage += pid config . tmp_prefix += pid config . test_dset_prefix += pid config . test_cube_prefix += pid config . cube_submission_id += pid","title":"set_unique_tmp_config()"},{"location":"reference/utils/#utils.storage_path","text":"Helper function that converts a path to storage-related path Source code in medperf/utils.py 25 26 27 def storage_path ( subpath : str ): \"\"\"Helper function that converts a path to storage-related path\"\"\" return os . path . join ( config . storage , subpath )","title":"storage_path()"},{"location":"reference/utils/#utils.untar","text":"Untars and optionally removes the tar.gz file Parameters: Name Type Description Default filepath str Path where the tar.gz file can be found. required remove bool Wether to delete the tar.gz file. Defaults to True. True Returns: Name Type Description str str location where the untared files can be found. Source code in medperf/utils.py 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 def untar ( filepath : str , remove : bool = True ) -> str : \"\"\"Untars and optionally removes the tar.gz file Args: filepath (str): Path where the tar.gz file can be found. remove (bool): Wether to delete the tar.gz file. Defaults to True. Returns: str: location where the untared files can be found. \"\"\" logging . info ( f \"Uncompressing tar.gz at { filepath } \" ) addpath = str ( Path ( filepath ) . parent ) tar = tarfile . open ( filepath ) tar . extractall ( addpath ) tar . close () # OS Specific issue: Mac Creates superfluous files with tarfile library [ os . remove ( spurious_file ) for spurious_file in glob ( addpath + \"/**/._*\" , recursive = True ) ] if remove : logging . info ( f \"Deleting { filepath } \" ) os . remove ( filepath ) return addpath","title":"untar()"},{"location":"reference/commands/compatibility_test/","text":"CompatibilityTestExecution Source code in commands/compatibility_test.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 class CompatibilityTestExecution : @classmethod def run ( cls , benchmark_uid : int , data_uid : str = None , data_prep : str = None , model : str = None , evaluator : str = None , force_test : bool = False , ) -> List : \"\"\"Execute a test workflow for a specific benchmark Args: benchmark_uid (int): Benchmark to run the test workflow for data_uid (str, optional): registered dataset uid. If none provided, it defaults to benchmark test dataset. data_prep (str, optional): data prep mlcube uid or local path. If none provided, it defaults to benchmark data prep mlcube. model (str, optional): model mlcube uid or local path. If none provided, it defaults to benchmark reference model. evaluator (str, optional): evaluator mlcube uid or local path. If none provided, it defaults to benchmark evaluator mlcube. Returns: (str): Benchmark UID used for the test. Could be the one provided or a generated one. (str): Dataset UID used for the test. Could be the one provided or a generated one. (str): Model UID used for the test. Could be the one provided or a generated one. (Result): Results generated by the test. \"\"\" logging . info ( \"Starting test execution\" ) test_exec = cls ( benchmark_uid , data_uid , data_prep , model , evaluator , force_test ) test_exec . validate () test_exec . prepare_test () test_exec . set_data_uid () result = test_exec . cached_result () or test_exec . execute_benchmark () return test_exec . benchmark_uid , test_exec . data_uid , test_exec . model , result def __init__ ( self , benchmark_uid : int , data_uid : str , data_prep : str , model : str , evaluator : str , force_test : bool = False , ): self . benchmark_uid = benchmark_uid self . demo_dataset_url = None self . demo_dataset_hash = None self . data_uid = data_uid self . dataset = None self . data_prep = data_prep self . model = model self . evaluator = evaluator self . comms = config . comms self . ui = config . ui self . force_test = force_test def validate ( self ): \"\"\"Ensures test has been passed a valid combination of parameters. Specifically, a benchmark must be passed if any other workflow parameter is not passed. \"\"\" params = [ self . data_uid , self . model , self . evaluator ] none_params = [ param is None for param in params ] if self . benchmark_uid is None and any ( none_params ): pretty_error ( \"Invalid combination of arguments to test. Ensure you pass a benchmark or a complete mlcube flow\" ) # a redundant data preparation cube if self . data_uid is not None and self . data_prep is not None : pretty_error ( \"Invalid combination of arguments to test. The passed preparation cube will not be used\" ) # a redundant benchmark if self . benchmark_uid is not None and not any ( none_params ): pretty_error ( \"Invalid combination of arguments to test. The passed benchmark will not be used\" ) def prepare_test ( self ): \"\"\"Prepares all parameters so a test can be executed. Paths to cubes are transformed to cube uids and benchmark is mocked/obtained. \"\"\" if self . benchmark_uid : benchmark = Benchmark . get ( self . benchmark_uid ) self . set_cube_uid ( \"data_prep\" , benchmark . data_preparation ) self . set_cube_uid ( \"model\" , benchmark . reference_model ) self . set_cube_uid ( \"evaluator\" , benchmark . evaluator ) self . demo_dataset_url = benchmark . demo_dataset_url self . demo_dataset_hash = benchmark . demo_dataset_hash else : self . set_cube_uid ( \"data_prep\" ) self . set_cube_uid ( \"model\" ) self . set_cube_uid ( \"evaluator\" ) def execute_benchmark ( self ): \"\"\"Runs the benchmark execution flow given the specified testing parameters \"\"\" benchmark = Benchmark . tmp ( self . data_prep , self . model , self . evaluator ) BenchmarkExecution . run ( benchmark . uid , self . data_uid , self . model , run_test = True , ) # Datasets associated with results of compatibility-test are identified # by the generated uid. Server uid is not be applicable in the case # of unregistered datasets. return Result . from_entities_uids ( benchmark . uid , self . model , self . dataset . generated_uid ) def set_cube_uid ( self , attr : str , fallback : any = None ): \"\"\"Assigns the attr used for testing according to the initialization parameters. If the value is a path, it will create a temporary uid and link the cube path to the medperf storage path. Arguments: attr (str): Attribute to check and/or reassign. fallback (any): Value to assign if attribute is empty. Defaults to None. \"\"\" logging . info ( f \"Establishing { attr } _uid for test execution\" ) val = getattr ( self , attr ) if val is None : logging . info ( f \"Empty attribute: { attr } . Assigning fallback: { fallback } \" ) setattr ( self , attr , fallback ) return # Test if value looks like an mlcube_uid, if so skip path validation if str ( val ) . isdigit (): logging . info ( f \"MLCube value { val } for { attr } resembles an mlcube_uid\" ) return # Check if value is a local mlcube path = Path ( val ) if path . is_file (): path = path . parent path = path . resolve () if os . path . exists ( path ): logging . info ( \"local path provided. Creating symbolic link\" ) temp_uid = config . test_cube_prefix + str ( int ( time ())) setattr ( self , attr , temp_uid ) cubes_storage = storage_path ( config . cubes_storage ) dst = os . path . join ( cubes_storage , temp_uid ) os . symlink ( path , dst ) logging . info ( f \"local cube will be linked to path: { dst } \" ) cube_metadata_file = os . path . join ( path , config . cube_metadata_filename ) cube_hashes_filename = os . path . join ( path , config . cube_hashes_filename ) if not os . path . exists ( cube_metadata_file ): metadata = { \"name\" : temp_uid , \"is_valid\" : True } with open ( cube_metadata_file , \"w\" ) as f : yaml . dump ( metadata , f ) if not os . path . exists ( cube_hashes_filename ): hashes = { \"additional_files_tarball_hash\" : \"\" , \"image_tarball_hash\" : \"\" } with open ( cube_hashes_filename , \"w\" ) as f : yaml . dump ( hashes , f ) return logging . warning ( f \"mlcube { val } was not found as an existing mlcube\" ) pretty_error ( f \"The provided mlcube ( { val } ) for { attr } could not be found as a local or remote mlcube\" ) def set_data_uid ( self ): \"\"\"Assigns the data_uid used for testing according to the initialization parameters. If no data_uid is provided, it will retrieve the demo data and execute the data preparation flow. \"\"\" logging . info ( \"Establishing data_uid for test execution\" ) logging . info ( \"Looking if dataset exists as a prepared dataset\" ) if self . data_uid is not None : self . dataset = Dataset . from_generated_uid ( self . data_uid ) # to avoid 'None' as a uid self . data_prep = self . dataset . preparation_cube_uid else : logging . info ( \"Using benchmark demo dataset\" ) data_path , labels_path = self . download_demo_data () self . data_uid = DataPreparation . run ( None , self . data_prep , data_path , labels_path , run_test = True , ) self . dataset = Dataset . from_generated_uid ( self . data_uid ) def download_demo_data ( self ): \"\"\"Retrieves the demo dataset associated to the specified benchmark Returns: data_path (str): Location of the downloaded data labels_path (str): Location of the downloaded labels \"\"\" dset_hash = self . demo_dataset_hash dset_url = self . demo_dataset_url file_path = self . comms . get_benchmark_demo_dataset ( dset_url , dset_hash ) # Check demo dataset integrity file_hash = get_file_sha1 ( file_path ) # Alllow for empty datset hashes for benchmark registration purposes if dset_hash and file_hash != dset_hash : pretty_error ( \"Demo dataset hash doesn't match expected hash\" ) untar_path = untar ( file_path , remove = False ) # It is assumed that all demo datasets contain a file # which specifies the input of the data preparation step paths_file = os . path . join ( untar_path , config . demo_dset_paths_file ) with open ( paths_file , \"r\" ) as f : paths = yaml . safe_load ( f ) data_path = os . path . join ( untar_path , paths [ \"data_path\" ]) labels_path = os . path . join ( untar_path , paths [ \"labels_path\" ]) return data_path , labels_path def cached_result ( self ): \"\"\"checks the existance of, and retrieves if possible, the compatibility test result. This method is called prior to the test execution. Returns: (Result|None): None if the result does not exist or if self.force_test is True, otherwise it returns the found result. \"\"\" if self . force_test : return tmp_benchmark_uid = ( f \" { config . tmp_prefix }{ self . data_prep } _ { self . model } _ { self . evaluator } \" ) try : result = Result . from_entities_uids ( tmp_benchmark_uid , self . model , self . dataset . generated_uid ) except FileNotFoundError : return logging . info ( f \"Existing results at { result . path } were detected.\" ) logging . info ( \"The compatibilty test will not be re-executed.\" ) return result cached_result () checks the existance of, and retrieves if possible, the compatibility test result. This method is called prior to the test execution. Returns: Type Description Result | None None if the result does not exist or if self.force_test is True, otherwise it returns the found result. Source code in commands/compatibility_test.py 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 def cached_result ( self ): \"\"\"checks the existance of, and retrieves if possible, the compatibility test result. This method is called prior to the test execution. Returns: (Result|None): None if the result does not exist or if self.force_test is True, otherwise it returns the found result. \"\"\" if self . force_test : return tmp_benchmark_uid = ( f \" { config . tmp_prefix }{ self . data_prep } _ { self . model } _ { self . evaluator } \" ) try : result = Result . from_entities_uids ( tmp_benchmark_uid , self . model , self . dataset . generated_uid ) except FileNotFoundError : return logging . info ( f \"Existing results at { result . path } were detected.\" ) logging . info ( \"The compatibilty test will not be re-executed.\" ) return result download_demo_data () Retrieves the demo dataset associated to the specified benchmark Returns: Name Type Description data_path str Location of the downloaded data labels_path str Location of the downloaded labels Source code in commands/compatibility_test.py 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 def download_demo_data ( self ): \"\"\"Retrieves the demo dataset associated to the specified benchmark Returns: data_path (str): Location of the downloaded data labels_path (str): Location of the downloaded labels \"\"\" dset_hash = self . demo_dataset_hash dset_url = self . demo_dataset_url file_path = self . comms . get_benchmark_demo_dataset ( dset_url , dset_hash ) # Check demo dataset integrity file_hash = get_file_sha1 ( file_path ) # Alllow for empty datset hashes for benchmark registration purposes if dset_hash and file_hash != dset_hash : pretty_error ( \"Demo dataset hash doesn't match expected hash\" ) untar_path = untar ( file_path , remove = False ) # It is assumed that all demo datasets contain a file # which specifies the input of the data preparation step paths_file = os . path . join ( untar_path , config . demo_dset_paths_file ) with open ( paths_file , \"r\" ) as f : paths = yaml . safe_load ( f ) data_path = os . path . join ( untar_path , paths [ \"data_path\" ]) labels_path = os . path . join ( untar_path , paths [ \"labels_path\" ]) return data_path , labels_path execute_benchmark () Runs the benchmark execution flow given the specified testing parameters Source code in commands/compatibility_test.py 115 116 117 118 119 120 121 122 123 124 125 126 127 def execute_benchmark ( self ): \"\"\"Runs the benchmark execution flow given the specified testing parameters \"\"\" benchmark = Benchmark . tmp ( self . data_prep , self . model , self . evaluator ) BenchmarkExecution . run ( benchmark . uid , self . data_uid , self . model , run_test = True , ) # Datasets associated with results of compatibility-test are identified # by the generated uid. Server uid is not be applicable in the case # of unregistered datasets. return Result . from_entities_uids ( benchmark . uid , self . model , self . dataset . generated_uid ) prepare_test () Prepares all parameters so a test can be executed. Paths to cubes are transformed to cube uids and benchmark is mocked/obtained. Source code in commands/compatibility_test.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def prepare_test ( self ): \"\"\"Prepares all parameters so a test can be executed. Paths to cubes are transformed to cube uids and benchmark is mocked/obtained. \"\"\" if self . benchmark_uid : benchmark = Benchmark . get ( self . benchmark_uid ) self . set_cube_uid ( \"data_prep\" , benchmark . data_preparation ) self . set_cube_uid ( \"model\" , benchmark . reference_model ) self . set_cube_uid ( \"evaluator\" , benchmark . evaluator ) self . demo_dataset_url = benchmark . demo_dataset_url self . demo_dataset_hash = benchmark . demo_dataset_hash else : self . set_cube_uid ( \"data_prep\" ) self . set_cube_uid ( \"model\" ) self . set_cube_uid ( \"evaluator\" ) run ( benchmark_uid , data_uid = None , data_prep = None , model = None , evaluator = None , force_test = False ) classmethod Execute a test workflow for a specific benchmark Parameters: Name Type Description Default benchmark_uid int Benchmark to run the test workflow for required data_uid str registered dataset uid. If none provided, it defaults to benchmark test dataset. None data_prep str data prep mlcube uid or local path. If none provided, it defaults to benchmark data prep mlcube. None model str model mlcube uid or local path. If none provided, it defaults to benchmark reference model. None evaluator str evaluator mlcube uid or local path. If none provided, it defaults to benchmark evaluator mlcube. None Returns: Type Description str Benchmark UID used for the test. Could be the one provided or a generated one. str Dataset UID used for the test. Could be the one provided or a generated one. str Model UID used for the test. Could be the one provided or a generated one. Result Results generated by the test. Source code in commands/compatibility_test.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 @classmethod def run ( cls , benchmark_uid : int , data_uid : str = None , data_prep : str = None , model : str = None , evaluator : str = None , force_test : bool = False , ) -> List : \"\"\"Execute a test workflow for a specific benchmark Args: benchmark_uid (int): Benchmark to run the test workflow for data_uid (str, optional): registered dataset uid. If none provided, it defaults to benchmark test dataset. data_prep (str, optional): data prep mlcube uid or local path. If none provided, it defaults to benchmark data prep mlcube. model (str, optional): model mlcube uid or local path. If none provided, it defaults to benchmark reference model. evaluator (str, optional): evaluator mlcube uid or local path. If none provided, it defaults to benchmark evaluator mlcube. Returns: (str): Benchmark UID used for the test. Could be the one provided or a generated one. (str): Dataset UID used for the test. Could be the one provided or a generated one. (str): Model UID used for the test. Could be the one provided or a generated one. (Result): Results generated by the test. \"\"\" logging . info ( \"Starting test execution\" ) test_exec = cls ( benchmark_uid , data_uid , data_prep , model , evaluator , force_test ) test_exec . validate () test_exec . prepare_test () test_exec . set_data_uid () result = test_exec . cached_result () or test_exec . execute_benchmark () return test_exec . benchmark_uid , test_exec . data_uid , test_exec . model , result set_cube_uid ( attr , fallback = None ) Assigns the attr used for testing according to the initialization parameters. If the value is a path, it will create a temporary uid and link the cube path to the medperf storage path. Parameters: Name Type Description Default attr str Attribute to check and/or reassign. required fallback any Value to assign if attribute is empty. Defaults to None. None Source code in commands/compatibility_test.py 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 def set_cube_uid ( self , attr : str , fallback : any = None ): \"\"\"Assigns the attr used for testing according to the initialization parameters. If the value is a path, it will create a temporary uid and link the cube path to the medperf storage path. Arguments: attr (str): Attribute to check and/or reassign. fallback (any): Value to assign if attribute is empty. Defaults to None. \"\"\" logging . info ( f \"Establishing { attr } _uid for test execution\" ) val = getattr ( self , attr ) if val is None : logging . info ( f \"Empty attribute: { attr } . Assigning fallback: { fallback } \" ) setattr ( self , attr , fallback ) return # Test if value looks like an mlcube_uid, if so skip path validation if str ( val ) . isdigit (): logging . info ( f \"MLCube value { val } for { attr } resembles an mlcube_uid\" ) return # Check if value is a local mlcube path = Path ( val ) if path . is_file (): path = path . parent path = path . resolve () if os . path . exists ( path ): logging . info ( \"local path provided. Creating symbolic link\" ) temp_uid = config . test_cube_prefix + str ( int ( time ())) setattr ( self , attr , temp_uid ) cubes_storage = storage_path ( config . cubes_storage ) dst = os . path . join ( cubes_storage , temp_uid ) os . symlink ( path , dst ) logging . info ( f \"local cube will be linked to path: { dst } \" ) cube_metadata_file = os . path . join ( path , config . cube_metadata_filename ) cube_hashes_filename = os . path . join ( path , config . cube_hashes_filename ) if not os . path . exists ( cube_metadata_file ): metadata = { \"name\" : temp_uid , \"is_valid\" : True } with open ( cube_metadata_file , \"w\" ) as f : yaml . dump ( metadata , f ) if not os . path . exists ( cube_hashes_filename ): hashes = { \"additional_files_tarball_hash\" : \"\" , \"image_tarball_hash\" : \"\" } with open ( cube_hashes_filename , \"w\" ) as f : yaml . dump ( hashes , f ) return logging . warning ( f \"mlcube { val } was not found as an existing mlcube\" ) pretty_error ( f \"The provided mlcube ( { val } ) for { attr } could not be found as a local or remote mlcube\" ) set_data_uid () Assigns the data_uid used for testing according to the initialization parameters. If no data_uid is provided, it will retrieve the demo data and execute the data preparation flow. Source code in commands/compatibility_test.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 def set_data_uid ( self ): \"\"\"Assigns the data_uid used for testing according to the initialization parameters. If no data_uid is provided, it will retrieve the demo data and execute the data preparation flow. \"\"\" logging . info ( \"Establishing data_uid for test execution\" ) logging . info ( \"Looking if dataset exists as a prepared dataset\" ) if self . data_uid is not None : self . dataset = Dataset . from_generated_uid ( self . data_uid ) # to avoid 'None' as a uid self . data_prep = self . dataset . preparation_cube_uid else : logging . info ( \"Using benchmark demo dataset\" ) data_path , labels_path = self . download_demo_data () self . data_uid = DataPreparation . run ( None , self . data_prep , data_path , labels_path , run_test = True , ) self . dataset = Dataset . from_generated_uid ( self . data_uid ) validate () Ensures test has been passed a valid combination of parameters. Specifically, a benchmark must be passed if any other workflow parameter is not passed. Source code in commands/compatibility_test.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def validate ( self ): \"\"\"Ensures test has been passed a valid combination of parameters. Specifically, a benchmark must be passed if any other workflow parameter is not passed. \"\"\" params = [ self . data_uid , self . model , self . evaluator ] none_params = [ param is None for param in params ] if self . benchmark_uid is None and any ( none_params ): pretty_error ( \"Invalid combination of arguments to test. Ensure you pass a benchmark or a complete mlcube flow\" ) # a redundant data preparation cube if self . data_uid is not None and self . data_prep is not None : pretty_error ( \"Invalid combination of arguments to test. The passed preparation cube will not be used\" ) # a redundant benchmark if self . benchmark_uid is not None and not any ( none_params ): pretty_error ( \"Invalid combination of arguments to test. The passed benchmark will not be used\" )","title":"compatibility_test"},{"location":"reference/commands/compatibility_test/#commands.compatibility_test.CompatibilityTestExecution","text":"Source code in commands/compatibility_test.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 class CompatibilityTestExecution : @classmethod def run ( cls , benchmark_uid : int , data_uid : str = None , data_prep : str = None , model : str = None , evaluator : str = None , force_test : bool = False , ) -> List : \"\"\"Execute a test workflow for a specific benchmark Args: benchmark_uid (int): Benchmark to run the test workflow for data_uid (str, optional): registered dataset uid. If none provided, it defaults to benchmark test dataset. data_prep (str, optional): data prep mlcube uid or local path. If none provided, it defaults to benchmark data prep mlcube. model (str, optional): model mlcube uid or local path. If none provided, it defaults to benchmark reference model. evaluator (str, optional): evaluator mlcube uid or local path. If none provided, it defaults to benchmark evaluator mlcube. Returns: (str): Benchmark UID used for the test. Could be the one provided or a generated one. (str): Dataset UID used for the test. Could be the one provided or a generated one. (str): Model UID used for the test. Could be the one provided or a generated one. (Result): Results generated by the test. \"\"\" logging . info ( \"Starting test execution\" ) test_exec = cls ( benchmark_uid , data_uid , data_prep , model , evaluator , force_test ) test_exec . validate () test_exec . prepare_test () test_exec . set_data_uid () result = test_exec . cached_result () or test_exec . execute_benchmark () return test_exec . benchmark_uid , test_exec . data_uid , test_exec . model , result def __init__ ( self , benchmark_uid : int , data_uid : str , data_prep : str , model : str , evaluator : str , force_test : bool = False , ): self . benchmark_uid = benchmark_uid self . demo_dataset_url = None self . demo_dataset_hash = None self . data_uid = data_uid self . dataset = None self . data_prep = data_prep self . model = model self . evaluator = evaluator self . comms = config . comms self . ui = config . ui self . force_test = force_test def validate ( self ): \"\"\"Ensures test has been passed a valid combination of parameters. Specifically, a benchmark must be passed if any other workflow parameter is not passed. \"\"\" params = [ self . data_uid , self . model , self . evaluator ] none_params = [ param is None for param in params ] if self . benchmark_uid is None and any ( none_params ): pretty_error ( \"Invalid combination of arguments to test. Ensure you pass a benchmark or a complete mlcube flow\" ) # a redundant data preparation cube if self . data_uid is not None and self . data_prep is not None : pretty_error ( \"Invalid combination of arguments to test. The passed preparation cube will not be used\" ) # a redundant benchmark if self . benchmark_uid is not None and not any ( none_params ): pretty_error ( \"Invalid combination of arguments to test. The passed benchmark will not be used\" ) def prepare_test ( self ): \"\"\"Prepares all parameters so a test can be executed. Paths to cubes are transformed to cube uids and benchmark is mocked/obtained. \"\"\" if self . benchmark_uid : benchmark = Benchmark . get ( self . benchmark_uid ) self . set_cube_uid ( \"data_prep\" , benchmark . data_preparation ) self . set_cube_uid ( \"model\" , benchmark . reference_model ) self . set_cube_uid ( \"evaluator\" , benchmark . evaluator ) self . demo_dataset_url = benchmark . demo_dataset_url self . demo_dataset_hash = benchmark . demo_dataset_hash else : self . set_cube_uid ( \"data_prep\" ) self . set_cube_uid ( \"model\" ) self . set_cube_uid ( \"evaluator\" ) def execute_benchmark ( self ): \"\"\"Runs the benchmark execution flow given the specified testing parameters \"\"\" benchmark = Benchmark . tmp ( self . data_prep , self . model , self . evaluator ) BenchmarkExecution . run ( benchmark . uid , self . data_uid , self . model , run_test = True , ) # Datasets associated with results of compatibility-test are identified # by the generated uid. Server uid is not be applicable in the case # of unregistered datasets. return Result . from_entities_uids ( benchmark . uid , self . model , self . dataset . generated_uid ) def set_cube_uid ( self , attr : str , fallback : any = None ): \"\"\"Assigns the attr used for testing according to the initialization parameters. If the value is a path, it will create a temporary uid and link the cube path to the medperf storage path. Arguments: attr (str): Attribute to check and/or reassign. fallback (any): Value to assign if attribute is empty. Defaults to None. \"\"\" logging . info ( f \"Establishing { attr } _uid for test execution\" ) val = getattr ( self , attr ) if val is None : logging . info ( f \"Empty attribute: { attr } . Assigning fallback: { fallback } \" ) setattr ( self , attr , fallback ) return # Test if value looks like an mlcube_uid, if so skip path validation if str ( val ) . isdigit (): logging . info ( f \"MLCube value { val } for { attr } resembles an mlcube_uid\" ) return # Check if value is a local mlcube path = Path ( val ) if path . is_file (): path = path . parent path = path . resolve () if os . path . exists ( path ): logging . info ( \"local path provided. Creating symbolic link\" ) temp_uid = config . test_cube_prefix + str ( int ( time ())) setattr ( self , attr , temp_uid ) cubes_storage = storage_path ( config . cubes_storage ) dst = os . path . join ( cubes_storage , temp_uid ) os . symlink ( path , dst ) logging . info ( f \"local cube will be linked to path: { dst } \" ) cube_metadata_file = os . path . join ( path , config . cube_metadata_filename ) cube_hashes_filename = os . path . join ( path , config . cube_hashes_filename ) if not os . path . exists ( cube_metadata_file ): metadata = { \"name\" : temp_uid , \"is_valid\" : True } with open ( cube_metadata_file , \"w\" ) as f : yaml . dump ( metadata , f ) if not os . path . exists ( cube_hashes_filename ): hashes = { \"additional_files_tarball_hash\" : \"\" , \"image_tarball_hash\" : \"\" } with open ( cube_hashes_filename , \"w\" ) as f : yaml . dump ( hashes , f ) return logging . warning ( f \"mlcube { val } was not found as an existing mlcube\" ) pretty_error ( f \"The provided mlcube ( { val } ) for { attr } could not be found as a local or remote mlcube\" ) def set_data_uid ( self ): \"\"\"Assigns the data_uid used for testing according to the initialization parameters. If no data_uid is provided, it will retrieve the demo data and execute the data preparation flow. \"\"\" logging . info ( \"Establishing data_uid for test execution\" ) logging . info ( \"Looking if dataset exists as a prepared dataset\" ) if self . data_uid is not None : self . dataset = Dataset . from_generated_uid ( self . data_uid ) # to avoid 'None' as a uid self . data_prep = self . dataset . preparation_cube_uid else : logging . info ( \"Using benchmark demo dataset\" ) data_path , labels_path = self . download_demo_data () self . data_uid = DataPreparation . run ( None , self . data_prep , data_path , labels_path , run_test = True , ) self . dataset = Dataset . from_generated_uid ( self . data_uid ) def download_demo_data ( self ): \"\"\"Retrieves the demo dataset associated to the specified benchmark Returns: data_path (str): Location of the downloaded data labels_path (str): Location of the downloaded labels \"\"\" dset_hash = self . demo_dataset_hash dset_url = self . demo_dataset_url file_path = self . comms . get_benchmark_demo_dataset ( dset_url , dset_hash ) # Check demo dataset integrity file_hash = get_file_sha1 ( file_path ) # Alllow for empty datset hashes for benchmark registration purposes if dset_hash and file_hash != dset_hash : pretty_error ( \"Demo dataset hash doesn't match expected hash\" ) untar_path = untar ( file_path , remove = False ) # It is assumed that all demo datasets contain a file # which specifies the input of the data preparation step paths_file = os . path . join ( untar_path , config . demo_dset_paths_file ) with open ( paths_file , \"r\" ) as f : paths = yaml . safe_load ( f ) data_path = os . path . join ( untar_path , paths [ \"data_path\" ]) labels_path = os . path . join ( untar_path , paths [ \"labels_path\" ]) return data_path , labels_path def cached_result ( self ): \"\"\"checks the existance of, and retrieves if possible, the compatibility test result. This method is called prior to the test execution. Returns: (Result|None): None if the result does not exist or if self.force_test is True, otherwise it returns the found result. \"\"\" if self . force_test : return tmp_benchmark_uid = ( f \" { config . tmp_prefix }{ self . data_prep } _ { self . model } _ { self . evaluator } \" ) try : result = Result . from_entities_uids ( tmp_benchmark_uid , self . model , self . dataset . generated_uid ) except FileNotFoundError : return logging . info ( f \"Existing results at { result . path } were detected.\" ) logging . info ( \"The compatibilty test will not be re-executed.\" ) return result","title":"CompatibilityTestExecution"},{"location":"reference/commands/compatibility_test/#commands.compatibility_test.CompatibilityTestExecution.cached_result","text":"checks the existance of, and retrieves if possible, the compatibility test result. This method is called prior to the test execution. Returns: Type Description Result | None None if the result does not exist or if self.force_test is True, otherwise it returns the found result. Source code in commands/compatibility_test.py 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 def cached_result ( self ): \"\"\"checks the existance of, and retrieves if possible, the compatibility test result. This method is called prior to the test execution. Returns: (Result|None): None if the result does not exist or if self.force_test is True, otherwise it returns the found result. \"\"\" if self . force_test : return tmp_benchmark_uid = ( f \" { config . tmp_prefix }{ self . data_prep } _ { self . model } _ { self . evaluator } \" ) try : result = Result . from_entities_uids ( tmp_benchmark_uid , self . model , self . dataset . generated_uid ) except FileNotFoundError : return logging . info ( f \"Existing results at { result . path } were detected.\" ) logging . info ( \"The compatibilty test will not be re-executed.\" ) return result","title":"cached_result()"},{"location":"reference/commands/compatibility_test/#commands.compatibility_test.CompatibilityTestExecution.download_demo_data","text":"Retrieves the demo dataset associated to the specified benchmark Returns: Name Type Description data_path str Location of the downloaded data labels_path str Location of the downloaded labels Source code in commands/compatibility_test.py 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 def download_demo_data ( self ): \"\"\"Retrieves the demo dataset associated to the specified benchmark Returns: data_path (str): Location of the downloaded data labels_path (str): Location of the downloaded labels \"\"\" dset_hash = self . demo_dataset_hash dset_url = self . demo_dataset_url file_path = self . comms . get_benchmark_demo_dataset ( dset_url , dset_hash ) # Check demo dataset integrity file_hash = get_file_sha1 ( file_path ) # Alllow for empty datset hashes for benchmark registration purposes if dset_hash and file_hash != dset_hash : pretty_error ( \"Demo dataset hash doesn't match expected hash\" ) untar_path = untar ( file_path , remove = False ) # It is assumed that all demo datasets contain a file # which specifies the input of the data preparation step paths_file = os . path . join ( untar_path , config . demo_dset_paths_file ) with open ( paths_file , \"r\" ) as f : paths = yaml . safe_load ( f ) data_path = os . path . join ( untar_path , paths [ \"data_path\" ]) labels_path = os . path . join ( untar_path , paths [ \"labels_path\" ]) return data_path , labels_path","title":"download_demo_data()"},{"location":"reference/commands/compatibility_test/#commands.compatibility_test.CompatibilityTestExecution.execute_benchmark","text":"Runs the benchmark execution flow given the specified testing parameters Source code in commands/compatibility_test.py 115 116 117 118 119 120 121 122 123 124 125 126 127 def execute_benchmark ( self ): \"\"\"Runs the benchmark execution flow given the specified testing parameters \"\"\" benchmark = Benchmark . tmp ( self . data_prep , self . model , self . evaluator ) BenchmarkExecution . run ( benchmark . uid , self . data_uid , self . model , run_test = True , ) # Datasets associated with results of compatibility-test are identified # by the generated uid. Server uid is not be applicable in the case # of unregistered datasets. return Result . from_entities_uids ( benchmark . uid , self . model , self . dataset . generated_uid )","title":"execute_benchmark()"},{"location":"reference/commands/compatibility_test/#commands.compatibility_test.CompatibilityTestExecution.prepare_test","text":"Prepares all parameters so a test can be executed. Paths to cubes are transformed to cube uids and benchmark is mocked/obtained. Source code in commands/compatibility_test.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def prepare_test ( self ): \"\"\"Prepares all parameters so a test can be executed. Paths to cubes are transformed to cube uids and benchmark is mocked/obtained. \"\"\" if self . benchmark_uid : benchmark = Benchmark . get ( self . benchmark_uid ) self . set_cube_uid ( \"data_prep\" , benchmark . data_preparation ) self . set_cube_uid ( \"model\" , benchmark . reference_model ) self . set_cube_uid ( \"evaluator\" , benchmark . evaluator ) self . demo_dataset_url = benchmark . demo_dataset_url self . demo_dataset_hash = benchmark . demo_dataset_hash else : self . set_cube_uid ( \"data_prep\" ) self . set_cube_uid ( \"model\" ) self . set_cube_uid ( \"evaluator\" )","title":"prepare_test()"},{"location":"reference/commands/compatibility_test/#commands.compatibility_test.CompatibilityTestExecution.run","text":"Execute a test workflow for a specific benchmark Parameters: Name Type Description Default benchmark_uid int Benchmark to run the test workflow for required data_uid str registered dataset uid. If none provided, it defaults to benchmark test dataset. None data_prep str data prep mlcube uid or local path. If none provided, it defaults to benchmark data prep mlcube. None model str model mlcube uid or local path. If none provided, it defaults to benchmark reference model. None evaluator str evaluator mlcube uid or local path. If none provided, it defaults to benchmark evaluator mlcube. None Returns: Type Description str Benchmark UID used for the test. Could be the one provided or a generated one. str Dataset UID used for the test. Could be the one provided or a generated one. str Model UID used for the test. Could be the one provided or a generated one. Result Results generated by the test. Source code in commands/compatibility_test.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 @classmethod def run ( cls , benchmark_uid : int , data_uid : str = None , data_prep : str = None , model : str = None , evaluator : str = None , force_test : bool = False , ) -> List : \"\"\"Execute a test workflow for a specific benchmark Args: benchmark_uid (int): Benchmark to run the test workflow for data_uid (str, optional): registered dataset uid. If none provided, it defaults to benchmark test dataset. data_prep (str, optional): data prep mlcube uid or local path. If none provided, it defaults to benchmark data prep mlcube. model (str, optional): model mlcube uid or local path. If none provided, it defaults to benchmark reference model. evaluator (str, optional): evaluator mlcube uid or local path. If none provided, it defaults to benchmark evaluator mlcube. Returns: (str): Benchmark UID used for the test. Could be the one provided or a generated one. (str): Dataset UID used for the test. Could be the one provided or a generated one. (str): Model UID used for the test. Could be the one provided or a generated one. (Result): Results generated by the test. \"\"\" logging . info ( \"Starting test execution\" ) test_exec = cls ( benchmark_uid , data_uid , data_prep , model , evaluator , force_test ) test_exec . validate () test_exec . prepare_test () test_exec . set_data_uid () result = test_exec . cached_result () or test_exec . execute_benchmark () return test_exec . benchmark_uid , test_exec . data_uid , test_exec . model , result","title":"run()"},{"location":"reference/commands/compatibility_test/#commands.compatibility_test.CompatibilityTestExecution.set_cube_uid","text":"Assigns the attr used for testing according to the initialization parameters. If the value is a path, it will create a temporary uid and link the cube path to the medperf storage path. Parameters: Name Type Description Default attr str Attribute to check and/or reassign. required fallback any Value to assign if attribute is empty. Defaults to None. None Source code in commands/compatibility_test.py 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 def set_cube_uid ( self , attr : str , fallback : any = None ): \"\"\"Assigns the attr used for testing according to the initialization parameters. If the value is a path, it will create a temporary uid and link the cube path to the medperf storage path. Arguments: attr (str): Attribute to check and/or reassign. fallback (any): Value to assign if attribute is empty. Defaults to None. \"\"\" logging . info ( f \"Establishing { attr } _uid for test execution\" ) val = getattr ( self , attr ) if val is None : logging . info ( f \"Empty attribute: { attr } . Assigning fallback: { fallback } \" ) setattr ( self , attr , fallback ) return # Test if value looks like an mlcube_uid, if so skip path validation if str ( val ) . isdigit (): logging . info ( f \"MLCube value { val } for { attr } resembles an mlcube_uid\" ) return # Check if value is a local mlcube path = Path ( val ) if path . is_file (): path = path . parent path = path . resolve () if os . path . exists ( path ): logging . info ( \"local path provided. Creating symbolic link\" ) temp_uid = config . test_cube_prefix + str ( int ( time ())) setattr ( self , attr , temp_uid ) cubes_storage = storage_path ( config . cubes_storage ) dst = os . path . join ( cubes_storage , temp_uid ) os . symlink ( path , dst ) logging . info ( f \"local cube will be linked to path: { dst } \" ) cube_metadata_file = os . path . join ( path , config . cube_metadata_filename ) cube_hashes_filename = os . path . join ( path , config . cube_hashes_filename ) if not os . path . exists ( cube_metadata_file ): metadata = { \"name\" : temp_uid , \"is_valid\" : True } with open ( cube_metadata_file , \"w\" ) as f : yaml . dump ( metadata , f ) if not os . path . exists ( cube_hashes_filename ): hashes = { \"additional_files_tarball_hash\" : \"\" , \"image_tarball_hash\" : \"\" } with open ( cube_hashes_filename , \"w\" ) as f : yaml . dump ( hashes , f ) return logging . warning ( f \"mlcube { val } was not found as an existing mlcube\" ) pretty_error ( f \"The provided mlcube ( { val } ) for { attr } could not be found as a local or remote mlcube\" )","title":"set_cube_uid()"},{"location":"reference/commands/compatibility_test/#commands.compatibility_test.CompatibilityTestExecution.set_data_uid","text":"Assigns the data_uid used for testing according to the initialization parameters. If no data_uid is provided, it will retrieve the demo data and execute the data preparation flow. Source code in commands/compatibility_test.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 def set_data_uid ( self ): \"\"\"Assigns the data_uid used for testing according to the initialization parameters. If no data_uid is provided, it will retrieve the demo data and execute the data preparation flow. \"\"\" logging . info ( \"Establishing data_uid for test execution\" ) logging . info ( \"Looking if dataset exists as a prepared dataset\" ) if self . data_uid is not None : self . dataset = Dataset . from_generated_uid ( self . data_uid ) # to avoid 'None' as a uid self . data_prep = self . dataset . preparation_cube_uid else : logging . info ( \"Using benchmark demo dataset\" ) data_path , labels_path = self . download_demo_data () self . data_uid = DataPreparation . run ( None , self . data_prep , data_path , labels_path , run_test = True , ) self . dataset = Dataset . from_generated_uid ( self . data_uid )","title":"set_data_uid()"},{"location":"reference/commands/compatibility_test/#commands.compatibility_test.CompatibilityTestExecution.validate","text":"Ensures test has been passed a valid combination of parameters. Specifically, a benchmark must be passed if any other workflow parameter is not passed. Source code in commands/compatibility_test.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def validate ( self ): \"\"\"Ensures test has been passed a valid combination of parameters. Specifically, a benchmark must be passed if any other workflow parameter is not passed. \"\"\" params = [ self . data_uid , self . model , self . evaluator ] none_params = [ param is None for param in params ] if self . benchmark_uid is None and any ( none_params ): pretty_error ( \"Invalid combination of arguments to test. Ensure you pass a benchmark or a complete mlcube flow\" ) # a redundant data preparation cube if self . data_uid is not None and self . data_prep is not None : pretty_error ( \"Invalid combination of arguments to test. The passed preparation cube will not be used\" ) # a redundant benchmark if self . benchmark_uid is not None and not any ( none_params ): pretty_error ( \"Invalid combination of arguments to test. The passed benchmark will not be used\" )","title":"validate()"},{"location":"reference/commands/association/approval/","text":"Approval Source code in commands/association/approval.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class Approval : @staticmethod def run ( benchmark_uid : str , approval_status : str , dataset_uid : str = None , mlcube_uid : str = None , ): \"\"\"Sets approval status for an association between a benchmark and a dataset or mlcube Args: benchmark_uid (str): Benchmark UID. approval_status (str): Desired approval status to set for the association. comms (Comms): Instance of Comms interface. ui (UI): Instance of UI interface. dataset_uid (str, optional): Dataset UID. Defaults to None. mlcube_uid (str, optional): MLCube UID. Defaults to None. \"\"\" comms = config . comms too_many_resources = dataset_uid and mlcube_uid no_resource = dataset_uid is None and mlcube_uid is None if no_resource or too_many_resources : pretty_error ( \"Invalid arguments. Must provide either a dataset or mlcube\" ) if dataset_uid : comms . set_dataset_association_approval ( benchmark_uid , dataset_uid , approval_status . value ) if mlcube_uid : comms . set_mlcube_association_approval ( benchmark_uid , mlcube_uid , approval_status . value ) run ( benchmark_uid , approval_status , dataset_uid = None , mlcube_uid = None ) staticmethod Sets approval status for an association between a benchmark and a dataset or mlcube Parameters: Name Type Description Default benchmark_uid str Benchmark UID. required approval_status str Desired approval status to set for the association. required comms Comms Instance of Comms interface. required ui UI Instance of UI interface. required dataset_uid str Dataset UID. Defaults to None. None mlcube_uid str MLCube UID. Defaults to None. None Source code in commands/association/approval.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 @staticmethod def run ( benchmark_uid : str , approval_status : str , dataset_uid : str = None , mlcube_uid : str = None , ): \"\"\"Sets approval status for an association between a benchmark and a dataset or mlcube Args: benchmark_uid (str): Benchmark UID. approval_status (str): Desired approval status to set for the association. comms (Comms): Instance of Comms interface. ui (UI): Instance of UI interface. dataset_uid (str, optional): Dataset UID. Defaults to None. mlcube_uid (str, optional): MLCube UID. Defaults to None. \"\"\" comms = config . comms too_many_resources = dataset_uid and mlcube_uid no_resource = dataset_uid is None and mlcube_uid is None if no_resource or too_many_resources : pretty_error ( \"Invalid arguments. Must provide either a dataset or mlcube\" ) if dataset_uid : comms . set_dataset_association_approval ( benchmark_uid , dataset_uid , approval_status . value ) if mlcube_uid : comms . set_mlcube_association_approval ( benchmark_uid , mlcube_uid , approval_status . value )","title":"approval"},{"location":"reference/commands/association/approval/#commands.association.approval.Approval","text":"Source code in commands/association/approval.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class Approval : @staticmethod def run ( benchmark_uid : str , approval_status : str , dataset_uid : str = None , mlcube_uid : str = None , ): \"\"\"Sets approval status for an association between a benchmark and a dataset or mlcube Args: benchmark_uid (str): Benchmark UID. approval_status (str): Desired approval status to set for the association. comms (Comms): Instance of Comms interface. ui (UI): Instance of UI interface. dataset_uid (str, optional): Dataset UID. Defaults to None. mlcube_uid (str, optional): MLCube UID. Defaults to None. \"\"\" comms = config . comms too_many_resources = dataset_uid and mlcube_uid no_resource = dataset_uid is None and mlcube_uid is None if no_resource or too_many_resources : pretty_error ( \"Invalid arguments. Must provide either a dataset or mlcube\" ) if dataset_uid : comms . set_dataset_association_approval ( benchmark_uid , dataset_uid , approval_status . value ) if mlcube_uid : comms . set_mlcube_association_approval ( benchmark_uid , mlcube_uid , approval_status . value )","title":"Approval"},{"location":"reference/commands/association/approval/#commands.association.approval.Approval.run","text":"Sets approval status for an association between a benchmark and a dataset or mlcube Parameters: Name Type Description Default benchmark_uid str Benchmark UID. required approval_status str Desired approval status to set for the association. required comms Comms Instance of Comms interface. required ui UI Instance of UI interface. required dataset_uid str Dataset UID. Defaults to None. None mlcube_uid str MLCube UID. Defaults to None. None Source code in commands/association/approval.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 @staticmethod def run ( benchmark_uid : str , approval_status : str , dataset_uid : str = None , mlcube_uid : str = None , ): \"\"\"Sets approval status for an association between a benchmark and a dataset or mlcube Args: benchmark_uid (str): Benchmark UID. approval_status (str): Desired approval status to set for the association. comms (Comms): Instance of Comms interface. ui (UI): Instance of UI interface. dataset_uid (str, optional): Dataset UID. Defaults to None. mlcube_uid (str, optional): MLCube UID. Defaults to None. \"\"\" comms = config . comms too_many_resources = dataset_uid and mlcube_uid no_resource = dataset_uid is None and mlcube_uid is None if no_resource or too_many_resources : pretty_error ( \"Invalid arguments. Must provide either a dataset or mlcube\" ) if dataset_uid : comms . set_dataset_association_approval ( benchmark_uid , dataset_uid , approval_status . value ) if mlcube_uid : comms . set_mlcube_association_approval ( benchmark_uid , mlcube_uid , approval_status . value )","title":"run()"},{"location":"reference/commands/association/association/","text":"approve ( benchmark_uid = typer . Option ( Ellipsis , '--benchmark' , '-b' , help = 'Benchmark UID' ), dataset_uid = typer . Option ( None , '--dataset' , '-d' , help = 'Dataset UID' ), mlcube_uid = typer . Option ( None , '--mlcube' , '-m' , help = 'MLCube UID' )) Approves an association between a benchmark and a dataset or model mlcube Parameters: Name Type Description Default benchmark_uid int Benchmark UID. typer.Option(Ellipsis, '--benchmark', '-b', help='Benchmark UID') dataset_uid int Dataset UID. typer.Option(None, '--dataset', '-d', help='Dataset UID') mlcube_uid int Model MLCube UID. typer.Option(None, '--mlcube', '-m', help='MLCube UID') Source code in commands/association/association.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 @clean_except @app . command ( \"approve\" ) def approve ( benchmark_uid : int = typer . Option ( ... , \"--benchmark\" , \"-b\" , help = \"Benchmark UID\" ), dataset_uid : int = typer . Option ( None , \"--dataset\" , \"-d\" , help = \"Dataset UID\" ), mlcube_uid : int = typer . Option ( None , \"--mlcube\" , \"-m\" , help = \"MLCube UID\" ), ): \"\"\"Approves an association between a benchmark and a dataset or model mlcube Args: benchmark_uid (int): Benchmark UID. dataset_uid (int, optional): Dataset UID. mlcube_uid (int, optional): Model MLCube UID. \"\"\" Approval . run ( benchmark_uid , Status . APPROVED , dataset_uid , mlcube_uid ) config . ui . print ( \"\u2705 Done!\" ) list ( filter = typer . Argument ( None )) Display all associations related to the current user. Parameters: Name Type Description Default filter str Filter associations by approval status. Defaults to displaying all user associations. typer.Argument(None) Source code in commands/association/association.py 13 14 15 16 17 18 19 20 21 22 @clean_except @app . command ( \"ls\" ) def list ( filter : Optional [ str ] = typer . Argument ( None )): \"\"\"Display all associations related to the current user. Args: filter (str, optional): Filter associations by approval status. Defaults to displaying all user associations. \"\"\" ListAssociations . run ( filter ) reject ( benchmark_uid = typer . Option ( Ellipsis , '--benchmark' , '-b' , help = 'Benchmark UID' ), dataset_uid = typer . Option ( None , '--dataset' , '-d' , help = 'Dataset UID' ), mlcube_uid = typer . Option ( None , '--mlcube' , '-m' , help = 'MLCube UID' )) Rejects an association between a benchmark and a dataset or model mlcube Parameters: Name Type Description Default benchmark_uid int Benchmark UID. typer.Option(Ellipsis, '--benchmark', '-b', help='Benchmark UID') dataset_uid int Dataset UID. typer.Option(None, '--dataset', '-d', help='Dataset UID') mlcube_uid int Model MLCube UID. typer.Option(None, '--mlcube', '-m', help='MLCube UID') Source code in commands/association/association.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 @clean_except @app . command ( \"reject\" ) def reject ( benchmark_uid : int = typer . Option ( ... , \"--benchmark\" , \"-b\" , help = \"Benchmark UID\" ), dataset_uid : int = typer . Option ( None , \"--dataset\" , \"-d\" , help = \"Dataset UID\" ), mlcube_uid : int = typer . Option ( None , \"--mlcube\" , \"-m\" , help = \"MLCube UID\" ), ): \"\"\"Rejects an association between a benchmark and a dataset or model mlcube Args: benchmark_uid (int): Benchmark UID. dataset_uid (int, optional): Dataset UID. mlcube_uid (int, optional): Model MLCube UID. \"\"\" Approval . run ( benchmark_uid , Status . REJECTED , dataset_uid , mlcube_uid ) config . ui . print ( \"\u2705 Done!\" )","title":"association"},{"location":"reference/commands/association/association/#commands.association.association.approve","text":"Approves an association between a benchmark and a dataset or model mlcube Parameters: Name Type Description Default benchmark_uid int Benchmark UID. typer.Option(Ellipsis, '--benchmark', '-b', help='Benchmark UID') dataset_uid int Dataset UID. typer.Option(None, '--dataset', '-d', help='Dataset UID') mlcube_uid int Model MLCube UID. typer.Option(None, '--mlcube', '-m', help='MLCube UID') Source code in commands/association/association.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 @clean_except @app . command ( \"approve\" ) def approve ( benchmark_uid : int = typer . Option ( ... , \"--benchmark\" , \"-b\" , help = \"Benchmark UID\" ), dataset_uid : int = typer . Option ( None , \"--dataset\" , \"-d\" , help = \"Dataset UID\" ), mlcube_uid : int = typer . Option ( None , \"--mlcube\" , \"-m\" , help = \"MLCube UID\" ), ): \"\"\"Approves an association between a benchmark and a dataset or model mlcube Args: benchmark_uid (int): Benchmark UID. dataset_uid (int, optional): Dataset UID. mlcube_uid (int, optional): Model MLCube UID. \"\"\" Approval . run ( benchmark_uid , Status . APPROVED , dataset_uid , mlcube_uid ) config . ui . print ( \"\u2705 Done!\" )","title":"approve()"},{"location":"reference/commands/association/association/#commands.association.association.list","text":"Display all associations related to the current user. Parameters: Name Type Description Default filter str Filter associations by approval status. Defaults to displaying all user associations. typer.Argument(None) Source code in commands/association/association.py 13 14 15 16 17 18 19 20 21 22 @clean_except @app . command ( \"ls\" ) def list ( filter : Optional [ str ] = typer . Argument ( None )): \"\"\"Display all associations related to the current user. Args: filter (str, optional): Filter associations by approval status. Defaults to displaying all user associations. \"\"\" ListAssociations . run ( filter )","title":"list()"},{"location":"reference/commands/association/association/#commands.association.association.reject","text":"Rejects an association between a benchmark and a dataset or model mlcube Parameters: Name Type Description Default benchmark_uid int Benchmark UID. typer.Option(Ellipsis, '--benchmark', '-b', help='Benchmark UID') dataset_uid int Dataset UID. typer.Option(None, '--dataset', '-d', help='Dataset UID') mlcube_uid int Model MLCube UID. typer.Option(None, '--mlcube', '-m', help='MLCube UID') Source code in commands/association/association.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 @clean_except @app . command ( \"reject\" ) def reject ( benchmark_uid : int = typer . Option ( ... , \"--benchmark\" , \"-b\" , help = \"Benchmark UID\" ), dataset_uid : int = typer . Option ( None , \"--dataset\" , \"-d\" , help = \"Dataset UID\" ), mlcube_uid : int = typer . Option ( None , \"--mlcube\" , \"-m\" , help = \"MLCube UID\" ), ): \"\"\"Rejects an association between a benchmark and a dataset or model mlcube Args: benchmark_uid (int): Benchmark UID. dataset_uid (int, optional): Dataset UID. mlcube_uid (int, optional): Model MLCube UID. \"\"\" Approval . run ( benchmark_uid , Status . REJECTED , dataset_uid , mlcube_uid ) config . ui . print ( \"\u2705 Done!\" )","title":"reject()"},{"location":"reference/commands/association/list/","text":"ListAssociations Source code in commands/association/list.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 class ListAssociations : @staticmethod def run ( filter : str = None ): \"\"\"Get Pending association requests\"\"\" comms = config . comms ui = config . ui dset_assocs = comms . get_datasets_associations () cube_assocs = comms . get_cubes_associations () # Might be worth seeing if creating an association class that encapsulates # most of the logic here is useful assocs = dset_assocs + cube_assocs if filter : filter = filter . upper () assocs = [ assoc for assoc in assocs if assoc [ \"approval_status\" ] == filter ] assocs_info = [] for assoc in assocs : assoc_info = ( assoc . get ( \"dataset\" , None ), assoc . get ( \"model_mlcube\" , None ), assoc [ \"benchmark\" ], assoc [ \"initiated_by\" ], assoc [ \"approval_status\" ], ) assocs_info . append ( assoc_info ) headers = [ \"Dataset UID\" , \"MLCube UID\" , \"Benchmark UID\" , \"Initiated by\" , \"Status\" , ] tab = tabulate ( assocs_info , headers = headers ) ui . print ( tab ) run ( filter = None ) staticmethod Get Pending association requests Source code in commands/association/list.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 @staticmethod def run ( filter : str = None ): \"\"\"Get Pending association requests\"\"\" comms = config . comms ui = config . ui dset_assocs = comms . get_datasets_associations () cube_assocs = comms . get_cubes_associations () # Might be worth seeing if creating an association class that encapsulates # most of the logic here is useful assocs = dset_assocs + cube_assocs if filter : filter = filter . upper () assocs = [ assoc for assoc in assocs if assoc [ \"approval_status\" ] == filter ] assocs_info = [] for assoc in assocs : assoc_info = ( assoc . get ( \"dataset\" , None ), assoc . get ( \"model_mlcube\" , None ), assoc [ \"benchmark\" ], assoc [ \"initiated_by\" ], assoc [ \"approval_status\" ], ) assocs_info . append ( assoc_info ) headers = [ \"Dataset UID\" , \"MLCube UID\" , \"Benchmark UID\" , \"Initiated by\" , \"Status\" , ] tab = tabulate ( assocs_info , headers = headers ) ui . print ( tab )","title":"list"},{"location":"reference/commands/association/list/#commands.association.list.ListAssociations","text":"Source code in commands/association/list.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 class ListAssociations : @staticmethod def run ( filter : str = None ): \"\"\"Get Pending association requests\"\"\" comms = config . comms ui = config . ui dset_assocs = comms . get_datasets_associations () cube_assocs = comms . get_cubes_associations () # Might be worth seeing if creating an association class that encapsulates # most of the logic here is useful assocs = dset_assocs + cube_assocs if filter : filter = filter . upper () assocs = [ assoc for assoc in assocs if assoc [ \"approval_status\" ] == filter ] assocs_info = [] for assoc in assocs : assoc_info = ( assoc . get ( \"dataset\" , None ), assoc . get ( \"model_mlcube\" , None ), assoc [ \"benchmark\" ], assoc [ \"initiated_by\" ], assoc [ \"approval_status\" ], ) assocs_info . append ( assoc_info ) headers = [ \"Dataset UID\" , \"MLCube UID\" , \"Benchmark UID\" , \"Initiated by\" , \"Status\" , ] tab = tabulate ( assocs_info , headers = headers ) ui . print ( tab )","title":"ListAssociations"},{"location":"reference/commands/association/list/#commands.association.list.ListAssociations.run","text":"Get Pending association requests Source code in commands/association/list.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 @staticmethod def run ( filter : str = None ): \"\"\"Get Pending association requests\"\"\" comms = config . comms ui = config . ui dset_assocs = comms . get_datasets_associations () cube_assocs = comms . get_cubes_associations () # Might be worth seeing if creating an association class that encapsulates # most of the logic here is useful assocs = dset_assocs + cube_assocs if filter : filter = filter . upper () assocs = [ assoc for assoc in assocs if assoc [ \"approval_status\" ] == filter ] assocs_info = [] for assoc in assocs : assoc_info = ( assoc . get ( \"dataset\" , None ), assoc . get ( \"model_mlcube\" , None ), assoc [ \"benchmark\" ], assoc [ \"initiated_by\" ], assoc [ \"approval_status\" ], ) assocs_info . append ( assoc_info ) headers = [ \"Dataset UID\" , \"MLCube UID\" , \"Benchmark UID\" , \"Initiated by\" , \"Status\" , ] tab = tabulate ( assocs_info , headers = headers ) ui . print ( tab )","title":"run()"},{"location":"reference/commands/auth/login/","text":"Login Source code in commands/auth/login.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 class Login : @staticmethod def run ( username : str = None , password : str = None ): \"\"\"Login to the medperf server. Must be done only once. \"\"\" comms = config . comms ui = config . ui cred_path = storage_path ( config . credentials_path ) user = username if username else ui . prompt ( \"username: \" ) pwd = password if password else ui . hidden_prompt ( \"password: \" ) comms . login ( user , pwd ) token = comms . token if os . path . exists ( cred_path ): os . remove ( cred_path ) with open ( cred_path , \"w\" ) as f : f . write ( token ) os . chmod ( cred_path , stat . S_IREAD ) run ( username = None , password = None ) staticmethod Login to the medperf server. Must be done only once. Source code in commands/auth/login.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 @staticmethod def run ( username : str = None , password : str = None ): \"\"\"Login to the medperf server. Must be done only once. \"\"\" comms = config . comms ui = config . ui cred_path = storage_path ( config . credentials_path ) user = username if username else ui . prompt ( \"username: \" ) pwd = password if password else ui . hidden_prompt ( \"password: \" ) comms . login ( user , pwd ) token = comms . token if os . path . exists ( cred_path ): os . remove ( cred_path ) with open ( cred_path , \"w\" ) as f : f . write ( token ) os . chmod ( cred_path , stat . S_IREAD )","title":"login"},{"location":"reference/commands/auth/login/#commands.auth.login.Login","text":"Source code in commands/auth/login.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 class Login : @staticmethod def run ( username : str = None , password : str = None ): \"\"\"Login to the medperf server. Must be done only once. \"\"\" comms = config . comms ui = config . ui cred_path = storage_path ( config . credentials_path ) user = username if username else ui . prompt ( \"username: \" ) pwd = password if password else ui . hidden_prompt ( \"password: \" ) comms . login ( user , pwd ) token = comms . token if os . path . exists ( cred_path ): os . remove ( cred_path ) with open ( cred_path , \"w\" ) as f : f . write ( token ) os . chmod ( cred_path , stat . S_IREAD )","title":"Login"},{"location":"reference/commands/auth/login/#commands.auth.login.Login.run","text":"Login to the medperf server. Must be done only once. Source code in commands/auth/login.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 @staticmethod def run ( username : str = None , password : str = None ): \"\"\"Login to the medperf server. Must be done only once. \"\"\" comms = config . comms ui = config . ui cred_path = storage_path ( config . credentials_path ) user = username if username else ui . prompt ( \"username: \" ) pwd = password if password else ui . hidden_prompt ( \"password: \" ) comms . login ( user , pwd ) token = comms . token if os . path . exists ( cred_path ): os . remove ( cred_path ) with open ( cred_path , \"w\" ) as f : f . write ( token ) os . chmod ( cred_path , stat . S_IREAD )","title":"run()"},{"location":"reference/commands/auth/password_change/","text":"PasswordChange Source code in commands/auth/password_change.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class PasswordChange : @staticmethod def run (): \"\"\"Change the user's password. Must be logged in \"\"\" comms = config . comms ui = config . ui pwd = ui . hidden_prompt ( \"Please type your new password: \" ) pwd_repeat = ui . hidden_prompt ( \"Please retype your new password: \" ) if pwd != pwd_repeat : pretty_error ( \"The passwords you typed don't match. Please try again.\" , clean = False , add_instructions = False , ) passchange_successful = comms . change_password ( pwd ) if passchange_successful : cred_path = storage_path ( config . credentials_path ) os . remove ( cred_path ) ui . print ( \"Password changed. Please log back in with medperf login\" ) else : pretty_error ( \"Unable to change the current password\" ) run () staticmethod Change the user's password. Must be logged in Source code in commands/auth/password_change.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 @staticmethod def run (): \"\"\"Change the user's password. Must be logged in \"\"\" comms = config . comms ui = config . ui pwd = ui . hidden_prompt ( \"Please type your new password: \" ) pwd_repeat = ui . hidden_prompt ( \"Please retype your new password: \" ) if pwd != pwd_repeat : pretty_error ( \"The passwords you typed don't match. Please try again.\" , clean = False , add_instructions = False , ) passchange_successful = comms . change_password ( pwd ) if passchange_successful : cred_path = storage_path ( config . credentials_path ) os . remove ( cred_path ) ui . print ( \"Password changed. Please log back in with medperf login\" ) else : pretty_error ( \"Unable to change the current password\" )","title":"password_change"},{"location":"reference/commands/auth/password_change/#commands.auth.password_change.PasswordChange","text":"Source code in commands/auth/password_change.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class PasswordChange : @staticmethod def run (): \"\"\"Change the user's password. Must be logged in \"\"\" comms = config . comms ui = config . ui pwd = ui . hidden_prompt ( \"Please type your new password: \" ) pwd_repeat = ui . hidden_prompt ( \"Please retype your new password: \" ) if pwd != pwd_repeat : pretty_error ( \"The passwords you typed don't match. Please try again.\" , clean = False , add_instructions = False , ) passchange_successful = comms . change_password ( pwd ) if passchange_successful : cred_path = storage_path ( config . credentials_path ) os . remove ( cred_path ) ui . print ( \"Password changed. Please log back in with medperf login\" ) else : pretty_error ( \"Unable to change the current password\" )","title":"PasswordChange"},{"location":"reference/commands/auth/password_change/#commands.auth.password_change.PasswordChange.run","text":"Change the user's password. Must be logged in Source code in commands/auth/password_change.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 @staticmethod def run (): \"\"\"Change the user's password. Must be logged in \"\"\" comms = config . comms ui = config . ui pwd = ui . hidden_prompt ( \"Please type your new password: \" ) pwd_repeat = ui . hidden_prompt ( \"Please retype your new password: \" ) if pwd != pwd_repeat : pretty_error ( \"The passwords you typed don't match. Please try again.\" , clean = False , add_instructions = False , ) passchange_successful = comms . change_password ( pwd ) if passchange_successful : cred_path = storage_path ( config . credentials_path ) os . remove ( cred_path ) ui . print ( \"Password changed. Please log back in with medperf login\" ) else : pretty_error ( \"Unable to change the current password\" )","title":"run()"},{"location":"reference/commands/benchmark/associate/","text":"AssociateBenchmark Source code in commands/benchmark/associate.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 class AssociateBenchmark : @classmethod def run ( cls , benchmark_uid : str , model_uid : str , data_uid : str , approved = False , force_test = False , ): \"\"\"Associates a dataset or model to the given benchmark Args: benchmark_uid (str): UID of benchmark to associate entities with model_uid (str): UID of model to associate with benchmark data_uid (str): UID of dataset to associate with benchmark comms (Comms): Instance of Communications interface ui (UI): Instance of UI interface approved (bool): Skip approval step. Defaults to False \"\"\" too_many_resources = data_uid and model_uid no_resource = data_uid is None and model_uid is None if no_resource or too_many_resources : pretty_error ( \"Invalid arguments. Must provide either a dataset or mlcube\" ) if model_uid is not None : AssociateCube . run ( model_uid , benchmark_uid , approved = approved , force_test = force_test ) if data_uid is not None : AssociateDataset . run ( data_uid , benchmark_uid , approved = approved , force_test = force_test ) run ( benchmark_uid , model_uid , data_uid , approved = False , force_test = False ) classmethod Associates a dataset or model to the given benchmark Parameters: Name Type Description Default benchmark_uid str UID of benchmark to associate entities with required model_uid str UID of model to associate with benchmark required data_uid str UID of dataset to associate with benchmark required comms Comms Instance of Communications interface required ui UI Instance of UI interface required approved bool Skip approval step. Defaults to False False Source code in commands/benchmark/associate.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 @classmethod def run ( cls , benchmark_uid : str , model_uid : str , data_uid : str , approved = False , force_test = False , ): \"\"\"Associates a dataset or model to the given benchmark Args: benchmark_uid (str): UID of benchmark to associate entities with model_uid (str): UID of model to associate with benchmark data_uid (str): UID of dataset to associate with benchmark comms (Comms): Instance of Communications interface ui (UI): Instance of UI interface approved (bool): Skip approval step. Defaults to False \"\"\" too_many_resources = data_uid and model_uid no_resource = data_uid is None and model_uid is None if no_resource or too_many_resources : pretty_error ( \"Invalid arguments. Must provide either a dataset or mlcube\" ) if model_uid is not None : AssociateCube . run ( model_uid , benchmark_uid , approved = approved , force_test = force_test ) if data_uid is not None : AssociateDataset . run ( data_uid , benchmark_uid , approved = approved , force_test = force_test )","title":"associate"},{"location":"reference/commands/benchmark/associate/#commands.benchmark.associate.AssociateBenchmark","text":"Source code in commands/benchmark/associate.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 class AssociateBenchmark : @classmethod def run ( cls , benchmark_uid : str , model_uid : str , data_uid : str , approved = False , force_test = False , ): \"\"\"Associates a dataset or model to the given benchmark Args: benchmark_uid (str): UID of benchmark to associate entities with model_uid (str): UID of model to associate with benchmark data_uid (str): UID of dataset to associate with benchmark comms (Comms): Instance of Communications interface ui (UI): Instance of UI interface approved (bool): Skip approval step. Defaults to False \"\"\" too_many_resources = data_uid and model_uid no_resource = data_uid is None and model_uid is None if no_resource or too_many_resources : pretty_error ( \"Invalid arguments. Must provide either a dataset or mlcube\" ) if model_uid is not None : AssociateCube . run ( model_uid , benchmark_uid , approved = approved , force_test = force_test ) if data_uid is not None : AssociateDataset . run ( data_uid , benchmark_uid , approved = approved , force_test = force_test )","title":"AssociateBenchmark"},{"location":"reference/commands/benchmark/associate/#commands.benchmark.associate.AssociateBenchmark.run","text":"Associates a dataset or model to the given benchmark Parameters: Name Type Description Default benchmark_uid str UID of benchmark to associate entities with required model_uid str UID of model to associate with benchmark required data_uid str UID of dataset to associate with benchmark required comms Comms Instance of Communications interface required ui UI Instance of UI interface required approved bool Skip approval step. Defaults to False False Source code in commands/benchmark/associate.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 @classmethod def run ( cls , benchmark_uid : str , model_uid : str , data_uid : str , approved = False , force_test = False , ): \"\"\"Associates a dataset or model to the given benchmark Args: benchmark_uid (str): UID of benchmark to associate entities with model_uid (str): UID of model to associate with benchmark data_uid (str): UID of dataset to associate with benchmark comms (Comms): Instance of Communications interface ui (UI): Instance of UI interface approved (bool): Skip approval step. Defaults to False \"\"\" too_many_resources = data_uid and model_uid no_resource = data_uid is None and model_uid is None if no_resource or too_many_resources : pretty_error ( \"Invalid arguments. Must provide either a dataset or mlcube\" ) if model_uid is not None : AssociateCube . run ( model_uid , benchmark_uid , approved = approved , force_test = force_test ) if data_uid is not None : AssociateDataset . run ( data_uid , benchmark_uid , approved = approved , force_test = force_test )","title":"run()"},{"location":"reference/commands/benchmark/benchmark/","text":"associate ( benchmark_uid = typer . Option ( Ellipsis , '--benchmark_uid' , '-b' , help = 'UID of benchmark to associate with' ), model_uid = typer . Option ( None , '--model_uid' , '-m' , help = 'UID of model MLCube to associate' ), dataset_uid = typer . Option ( None , '--data_uid' , '-d' , help = 'Server UID of registered dataset to associate' ), approval = typer . Option ( False , '-y' , help = 'Skip approval step' ), force_test = typer . Option ( False , '--force-test' , help = 'Execute the test even if results already exist' )) Associates a benchmark with a given mlcube or dataset. Only one option at a time. Source code in commands/benchmark/benchmark.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 @app . command ( \"associate\" ) @clean_except def associate ( benchmark_uid : str = typer . Option ( ... , \"--benchmark_uid\" , \"-b\" , help = \"UID of benchmark to associate with\" ), model_uid : str = typer . Option ( None , \"--model_uid\" , \"-m\" , help = \"UID of model MLCube to associate\" ), dataset_uid : str = typer . Option ( None , \"--data_uid\" , \"-d\" , help = \"Server UID of registered dataset to associate\" ), approval : bool = typer . Option ( False , \"-y\" , help = \"Skip approval step\" ), force_test : bool = typer . Option ( False , \"--force-test\" , help = \"Execute the test even if results already exist\" , ), ): \"\"\"Associates a benchmark with a given mlcube or dataset. Only one option at a time. \"\"\" AssociateBenchmark . run ( benchmark_uid , model_uid , dataset_uid , approved = approval , force_test = force_test ) config . ui . print ( \"\u2705 Done!\" ) list ( all = typer . Option ( False , help = 'Display all benchmarks in the platform' )) Lists all benchmarks created by the user If --all is used, displays all benchmarks in the platform Source code in commands/benchmark/benchmark.py 13 14 15 16 17 18 19 20 21 @app . command ( \"ls\" ) @clean_except def list ( all : bool = typer . Option ( False , help = \"Display all benchmarks in the platform\" ) ): \"\"\"Lists all benchmarks created by the user If --all is used, displays all benchmarks in the platform \"\"\" BenchmarksList . run ( all ) submit ( name = typer . Option ( Ellipsis , '--name' , '-n' , help = 'Name of the benchmark' ), description = typer . Option ( Ellipsis , '--description' , '-d' , help = 'Description of the benchmark' ), docs_url = typer . Option ( '' , '--docs-url' , '-u' , help = 'URL to documentation' ), demo_url = typer . Option ( '' , '--demo-url' , help = 'URL to demonstration dataset tarball file' ), demo_hash = typer . Option ( '' , '--demo-hash' , help = 'SHA1 of demonstration dataset tarball file' ), data_preparation_mlcube = typer . Option ( Ellipsis , '--data-preparation-mlcube' , '-p' , help = 'Data Preparation MLCube UID' ), reference_model_mlcube = typer . Option ( Ellipsis , '--reference-model-mlcube' , '-m' , help = 'Reference Model MLCube UID' ), evaluator_mlcube = typer . Option ( Ellipsis , '--evaluator-mlcube' , '-e' , help = 'Evaluator MLCube UID' )) Submits a new benchmark to the platform Source code in commands/benchmark/benchmark.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 @app . command ( \"submit\" ) @clean_except def submit ( name : str = typer . Option ( ... , \"--name\" , \"-n\" , help = \"Name of the benchmark\" ), description : str = typer . Option ( ... , \"--description\" , \"-d\" , help = \"Description of the benchmark\" ), docs_url : str = typer . Option ( \"\" , \"--docs-url\" , \"-u\" , help = \"URL to documentation\" ), demo_url : str = typer . Option ( \"\" , \"--demo-url\" , help = \"URL to demonstration dataset tarball file\" ), demo_hash : str = typer . Option ( \"\" , \"--demo-hash\" , help = \"SHA1 of demonstration dataset tarball file\" ), data_preparation_mlcube : str = typer . Option ( ... , \"--data-preparation-mlcube\" , \"-p\" , help = \"Data Preparation MLCube UID\" ), reference_model_mlcube : str = typer . Option ( ... , \"--reference-model-mlcube\" , \"-m\" , help = \"Reference Model MLCube UID\" ), evaluator_mlcube : str = typer . Option ( ... , \"--evaluator-mlcube\" , \"-e\" , help = \"Evaluator MLCube UID\" ), ): \"\"\"Submits a new benchmark to the platform\"\"\" benchmark_info = { \"name\" : name , \"description\" : description , \"docs_url\" : docs_url , \"demo_url\" : demo_url , \"demo_hash\" : demo_hash , \"data_preparation_mlcube\" : data_preparation_mlcube , \"reference_model_mlcube\" : reference_model_mlcube , \"evaluator_mlcube\" : evaluator_mlcube , } SubmitBenchmark . run ( benchmark_info ) cleanup () config . ui . print ( \"\u2705 Done!\" )","title":"benchmark"},{"location":"reference/commands/benchmark/benchmark/#commands.benchmark.benchmark.associate","text":"Associates a benchmark with a given mlcube or dataset. Only one option at a time. Source code in commands/benchmark/benchmark.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 @app . command ( \"associate\" ) @clean_except def associate ( benchmark_uid : str = typer . Option ( ... , \"--benchmark_uid\" , \"-b\" , help = \"UID of benchmark to associate with\" ), model_uid : str = typer . Option ( None , \"--model_uid\" , \"-m\" , help = \"UID of model MLCube to associate\" ), dataset_uid : str = typer . Option ( None , \"--data_uid\" , \"-d\" , help = \"Server UID of registered dataset to associate\" ), approval : bool = typer . Option ( False , \"-y\" , help = \"Skip approval step\" ), force_test : bool = typer . Option ( False , \"--force-test\" , help = \"Execute the test even if results already exist\" , ), ): \"\"\"Associates a benchmark with a given mlcube or dataset. Only one option at a time. \"\"\" AssociateBenchmark . run ( benchmark_uid , model_uid , dataset_uid , approved = approval , force_test = force_test ) config . ui . print ( \"\u2705 Done!\" )","title":"associate()"},{"location":"reference/commands/benchmark/benchmark/#commands.benchmark.benchmark.list","text":"Lists all benchmarks created by the user If --all is used, displays all benchmarks in the platform Source code in commands/benchmark/benchmark.py 13 14 15 16 17 18 19 20 21 @app . command ( \"ls\" ) @clean_except def list ( all : bool = typer . Option ( False , help = \"Display all benchmarks in the platform\" ) ): \"\"\"Lists all benchmarks created by the user If --all is used, displays all benchmarks in the platform \"\"\" BenchmarksList . run ( all )","title":"list()"},{"location":"reference/commands/benchmark/benchmark/#commands.benchmark.benchmark.submit","text":"Submits a new benchmark to the platform Source code in commands/benchmark/benchmark.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 @app . command ( \"submit\" ) @clean_except def submit ( name : str = typer . Option ( ... , \"--name\" , \"-n\" , help = \"Name of the benchmark\" ), description : str = typer . Option ( ... , \"--description\" , \"-d\" , help = \"Description of the benchmark\" ), docs_url : str = typer . Option ( \"\" , \"--docs-url\" , \"-u\" , help = \"URL to documentation\" ), demo_url : str = typer . Option ( \"\" , \"--demo-url\" , help = \"URL to demonstration dataset tarball file\" ), demo_hash : str = typer . Option ( \"\" , \"--demo-hash\" , help = \"SHA1 of demonstration dataset tarball file\" ), data_preparation_mlcube : str = typer . Option ( ... , \"--data-preparation-mlcube\" , \"-p\" , help = \"Data Preparation MLCube UID\" ), reference_model_mlcube : str = typer . Option ( ... , \"--reference-model-mlcube\" , \"-m\" , help = \"Reference Model MLCube UID\" ), evaluator_mlcube : str = typer . Option ( ... , \"--evaluator-mlcube\" , \"-e\" , help = \"Evaluator MLCube UID\" ), ): \"\"\"Submits a new benchmark to the platform\"\"\" benchmark_info = { \"name\" : name , \"description\" : description , \"docs_url\" : docs_url , \"demo_url\" : demo_url , \"demo_hash\" : demo_hash , \"data_preparation_mlcube\" : data_preparation_mlcube , \"reference_model_mlcube\" : reference_model_mlcube , \"evaluator_mlcube\" : evaluator_mlcube , } SubmitBenchmark . run ( benchmark_info ) cleanup () config . ui . print ( \"\u2705 Done!\" )","title":"submit()"},{"location":"reference/commands/benchmark/list/","text":"BenchmarksList Source code in commands/benchmark/list.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class BenchmarksList : @staticmethod def run ( all : bool = False ): \"\"\"Lists all benchmarks created by the user by default. Use \"all\" to display all benchmarks in the platform Args: comms (Comms): Communications instance ui (UI): UI instance all (bool): Display all benchmarks in the platform. Defaults to False. \"\"\" comms = config . comms ui = config . ui if all : benchmarks = comms . get_benchmarks () else : benchmarks = comms . get_user_benchmarks () headers = [ \"UID\" , \"Name\" , \"Description\" , \"State\" , \"Approval Status\" ] formatted_bmarks = [] desc_max_len = 20 for bmark in benchmarks : if len ( bmark [ \"description\" ]) > desc_max_len : desc = bmark [ \"description\" ][: 22 ] + \"...\" bmark [ \"description\" ] == desc formatted_bmarks . append ( bmark ) data = [ [ bmark [ \"id\" ], bmark [ \"name\" ], bmark [ \"description\" ], bmark [ \"state\" ], bmark [ \"approval_status\" ], ] for bmark in formatted_bmarks ] tab = tabulate ( data , headers = headers ) ui . print ( tab ) run ( all = False ) staticmethod Lists all benchmarks created by the user by default. Use \"all\" to display all benchmarks in the platform Parameters: Name Type Description Default comms Comms Communications instance required ui UI UI instance required all bool Display all benchmarks in the platform. Defaults to False. False Source code in commands/benchmark/list.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 @staticmethod def run ( all : bool = False ): \"\"\"Lists all benchmarks created by the user by default. Use \"all\" to display all benchmarks in the platform Args: comms (Comms): Communications instance ui (UI): UI instance all (bool): Display all benchmarks in the platform. Defaults to False. \"\"\" comms = config . comms ui = config . ui if all : benchmarks = comms . get_benchmarks () else : benchmarks = comms . get_user_benchmarks () headers = [ \"UID\" , \"Name\" , \"Description\" , \"State\" , \"Approval Status\" ] formatted_bmarks = [] desc_max_len = 20 for bmark in benchmarks : if len ( bmark [ \"description\" ]) > desc_max_len : desc = bmark [ \"description\" ][: 22 ] + \"...\" bmark [ \"description\" ] == desc formatted_bmarks . append ( bmark ) data = [ [ bmark [ \"id\" ], bmark [ \"name\" ], bmark [ \"description\" ], bmark [ \"state\" ], bmark [ \"approval_status\" ], ] for bmark in formatted_bmarks ] tab = tabulate ( data , headers = headers ) ui . print ( tab )","title":"list"},{"location":"reference/commands/benchmark/list/#commands.benchmark.list.BenchmarksList","text":"Source code in commands/benchmark/list.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class BenchmarksList : @staticmethod def run ( all : bool = False ): \"\"\"Lists all benchmarks created by the user by default. Use \"all\" to display all benchmarks in the platform Args: comms (Comms): Communications instance ui (UI): UI instance all (bool): Display all benchmarks in the platform. Defaults to False. \"\"\" comms = config . comms ui = config . ui if all : benchmarks = comms . get_benchmarks () else : benchmarks = comms . get_user_benchmarks () headers = [ \"UID\" , \"Name\" , \"Description\" , \"State\" , \"Approval Status\" ] formatted_bmarks = [] desc_max_len = 20 for bmark in benchmarks : if len ( bmark [ \"description\" ]) > desc_max_len : desc = bmark [ \"description\" ][: 22 ] + \"...\" bmark [ \"description\" ] == desc formatted_bmarks . append ( bmark ) data = [ [ bmark [ \"id\" ], bmark [ \"name\" ], bmark [ \"description\" ], bmark [ \"state\" ], bmark [ \"approval_status\" ], ] for bmark in formatted_bmarks ] tab = tabulate ( data , headers = headers ) ui . print ( tab )","title":"BenchmarksList"},{"location":"reference/commands/benchmark/list/#commands.benchmark.list.BenchmarksList.run","text":"Lists all benchmarks created by the user by default. Use \"all\" to display all benchmarks in the platform Parameters: Name Type Description Default comms Comms Communications instance required ui UI UI instance required all bool Display all benchmarks in the platform. Defaults to False. False Source code in commands/benchmark/list.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 @staticmethod def run ( all : bool = False ): \"\"\"Lists all benchmarks created by the user by default. Use \"all\" to display all benchmarks in the platform Args: comms (Comms): Communications instance ui (UI): UI instance all (bool): Display all benchmarks in the platform. Defaults to False. \"\"\" comms = config . comms ui = config . ui if all : benchmarks = comms . get_benchmarks () else : benchmarks = comms . get_user_benchmarks () headers = [ \"UID\" , \"Name\" , \"Description\" , \"State\" , \"Approval Status\" ] formatted_bmarks = [] desc_max_len = 20 for bmark in benchmarks : if len ( bmark [ \"description\" ]) > desc_max_len : desc = bmark [ \"description\" ][: 22 ] + \"...\" bmark [ \"description\" ] == desc formatted_bmarks . append ( bmark ) data = [ [ bmark [ \"id\" ], bmark [ \"name\" ], bmark [ \"description\" ], bmark [ \"state\" ], bmark [ \"approval_status\" ], ] for bmark in formatted_bmarks ] tab = tabulate ( data , headers = headers ) ui . print ( tab )","title":"run()"},{"location":"reference/commands/benchmark/submit/","text":"SubmitBenchmark Source code in commands/benchmark/submit.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 class SubmitBenchmark : @classmethod def run ( cls , benchmark_info : dict , force_test : bool = True ): \"\"\"Submits a new cube to the medperf platform Args: benchmark_info (dict): benchmark information expected keys: name (str): benchmark name description (str): benchmark description docs_url (str): benchmark documentation url demo_url (str): benchmark demo dataset url demo_hash (str): benchmark demo dataset hash data_preparation_mlcube (str): benchmark data preparation mlcube uid reference_model_mlcube (str): benchmark reference model mlcube uid evaluator_mlcube (str): benchmark data evaluator mlcube uid \"\"\" ui = config . ui submission = cls ( benchmark_info , force_test ) if not submission . is_valid (): pretty_error ( \"Invalid benchmark information\" ) with ui . interactive (): ui . text = \"Getting additional information\" submission . get_extra_information () ui . print ( \"> Completed benchmark registration information\" ) ui . text = \"Submitting Benchmark to MedPerf\" updated_benchmark_body = submission . submit () ui . print ( \"Uploaded\" ) submission . write ( updated_benchmark_body ) def __init__ ( self , benchmark_info : dict , force_test : bool = True ): self . comms = config . comms self . ui = config . ui self . name = benchmark_info [ \"name\" ] self . description = benchmark_info [ \"description\" ] self . docs_url = benchmark_info [ \"docs_url\" ] self . demo_url = benchmark_info [ \"demo_url\" ] self . demo_hash = benchmark_info [ \"demo_hash\" ] self . demo_uid = None self . data_preparation_mlcube = benchmark_info [ \"data_preparation_mlcube\" ] self . reference_model_mlcube = benchmark_info [ \"reference_model_mlcube\" ] self . data_evaluator_mlcube = benchmark_info [ \"evaluator_mlcube\" ] self . results = None self . force_test = force_test def is_valid ( self ) -> bool : \"\"\"Validates that user-provided benchmark information is correct Returns: bool: Wether or not the benchmark information is valid \"\"\" name_valid_length = 0 < len ( self . name ) < 20 desc_valid_length = len ( self . description ) < 100 docs_url_valid = self . docs_url == \"\" or validators . url ( self . docs_url ) demo_url_valid = self . demo_url == \"\" or validators . url ( self . demo_url ) demo_hash_if_no_url = self . demo_url or self . demo_hash prep_uid_valid = self . data_preparation_mlcube . isdigit () model_uid_valid = self . reference_model_mlcube . isdigit () eval_uid_valid = self . data_evaluator_mlcube . isdigit () valid_tests = [ ( \"name\" , name_valid_length , \"Name is invalid\" ), ( \"description\" , desc_valid_length , \"Description is too long\" ), ( \"docs_url\" , docs_url_valid , \"Documentation URL is invalid\" ), ( \"demo_url\" , demo_url_valid , \"Demo Dataset Tarball URL is invalid\" ), ( \"demo_hash\" , demo_hash_if_no_url , \"Demo Datset Hash must be provided if no URL passed\" , ), ( \"data_preparation_mlcube\" , prep_uid_valid , \"Data Preparation MLCube UID is invalid\" , ), ( \"reference_model_mlcube\" , model_uid_valid , \"Reference Model MLCube is invalid\" , ), ( \"data_evaluator_mlcube\" , eval_uid_valid , \"Data Evaluator MLCube is invalid\" , ), ] valid = True for attr , test , error_msg in valid_tests : if not test : valid = False self . ui . print_error ( error_msg ) return valid def get_extra_information ( self ): \"\"\"Retrieves information that must be populated automatically, like hash, generated uid and test results \"\"\" tmp_uid = self . demo_hash if self . demo_hash else generate_tmp_uid () demo_dset_path = self . comms . get_benchmark_demo_dataset ( self . demo_url , tmp_uid ) demo_hash = get_file_sha1 ( demo_dset_path ) if self . demo_hash and demo_hash != self . demo_hash : logging . error ( f \"Demo dataset hash mismatch: { demo_hash } != { self . demo_hash } \" ) pretty_error ( \"Demo dataset hash does not match the provided hash\" ) self . demo_hash = demo_hash demo_uid , results = self . run_compatibility_test () self . demo_uid = demo_uid self . results = results def run_compatibility_test ( self ): \"\"\"Runs a compatibility test to ensure elements are compatible, and to extract additional information required for submission \"\"\" self . ui . print ( \"Running compatibility test\" ) data_prep = self . data_preparation_mlcube model = self . reference_model_mlcube evaluator = self . data_evaluator_mlcube demo_url = self . demo_url demo_hash = self . demo_hash benchmark = Benchmark . tmp ( data_prep , model , evaluator , demo_url , demo_hash ) _ , data_uid , _ , results = CompatibilityTestExecution . run ( benchmark . uid , force_test = self . force_test ) # Datasets generated by the compatibility test come with a prefix to identify them # This is not what we need for benchmark submissions, so we need to remove it prefix_len = len ( config . test_dset_prefix ) data_uid = data_uid [ prefix_len :] return data_uid , results def todict ( self ): return { \"name\" : self . name , \"description\" : self . description , \"docs_url\" : self . docs_url , \"demo_dataset_tarball_url\" : self . demo_url , \"demo_dataset_tarball_hash\" : self . demo_hash , \"demo_dataset_generated_uid\" : self . demo_uid , \"data_preparation_mlcube\" : int ( self . data_preparation_mlcube ), \"reference_model_mlcube\" : int ( self . reference_model_mlcube ), \"data_evaluator_mlcube\" : int ( self . data_evaluator_mlcube ), \"state\" : \"OPERATION\" , \"is_valid\" : True , \"approval_status\" : Status . PENDING . value , \"metadata\" : { \"results\" : self . results . results }, \"id\" : None , \"models\" : [ int ( self . reference_model_mlcube )], # not in the server (OK) \"created_at\" : None , \"modified_at\" : None , \"approved_at\" : None , \"owner\" : None , \"is_active\" : True , \"user_metadata\" : {}, } def submit ( self ): body = self . todict () updated_body = Benchmark ( body ) . upload () return updated_body def write ( self , updated_body ): bmk = Benchmark ( updated_body ) bmk . write () get_extra_information () Retrieves information that must be populated automatically, like hash, generated uid and test results Source code in commands/benchmark/submit.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def get_extra_information ( self ): \"\"\"Retrieves information that must be populated automatically, like hash, generated uid and test results \"\"\" tmp_uid = self . demo_hash if self . demo_hash else generate_tmp_uid () demo_dset_path = self . comms . get_benchmark_demo_dataset ( self . demo_url , tmp_uid ) demo_hash = get_file_sha1 ( demo_dset_path ) if self . demo_hash and demo_hash != self . demo_hash : logging . error ( f \"Demo dataset hash mismatch: { demo_hash } != { self . demo_hash } \" ) pretty_error ( \"Demo dataset hash does not match the provided hash\" ) self . demo_hash = demo_hash demo_uid , results = self . run_compatibility_test () self . demo_uid = demo_uid self . results = results is_valid () Validates that user-provided benchmark information is correct Returns: Name Type Description bool bool Wether or not the benchmark information is valid Source code in commands/benchmark/submit.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def is_valid ( self ) -> bool : \"\"\"Validates that user-provided benchmark information is correct Returns: bool: Wether or not the benchmark information is valid \"\"\" name_valid_length = 0 < len ( self . name ) < 20 desc_valid_length = len ( self . description ) < 100 docs_url_valid = self . docs_url == \"\" or validators . url ( self . docs_url ) demo_url_valid = self . demo_url == \"\" or validators . url ( self . demo_url ) demo_hash_if_no_url = self . demo_url or self . demo_hash prep_uid_valid = self . data_preparation_mlcube . isdigit () model_uid_valid = self . reference_model_mlcube . isdigit () eval_uid_valid = self . data_evaluator_mlcube . isdigit () valid_tests = [ ( \"name\" , name_valid_length , \"Name is invalid\" ), ( \"description\" , desc_valid_length , \"Description is too long\" ), ( \"docs_url\" , docs_url_valid , \"Documentation URL is invalid\" ), ( \"demo_url\" , demo_url_valid , \"Demo Dataset Tarball URL is invalid\" ), ( \"demo_hash\" , demo_hash_if_no_url , \"Demo Datset Hash must be provided if no URL passed\" , ), ( \"data_preparation_mlcube\" , prep_uid_valid , \"Data Preparation MLCube UID is invalid\" , ), ( \"reference_model_mlcube\" , model_uid_valid , \"Reference Model MLCube is invalid\" , ), ( \"data_evaluator_mlcube\" , eval_uid_valid , \"Data Evaluator MLCube is invalid\" , ), ] valid = True for attr , test , error_msg in valid_tests : if not test : valid = False self . ui . print_error ( error_msg ) return valid run ( benchmark_info , force_test = True ) classmethod Submits a new cube to the medperf platform Parameters: Name Type Description Default benchmark_info dict benchmark information expected keys: name (str): benchmark name description (str): benchmark description docs_url (str): benchmark documentation url demo_url (str): benchmark demo dataset url demo_hash (str): benchmark demo dataset hash data_preparation_mlcube (str): benchmark data preparation mlcube uid reference_model_mlcube (str): benchmark reference model mlcube uid evaluator_mlcube (str): benchmark data evaluator mlcube uid required Source code in commands/benchmark/submit.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 @classmethod def run ( cls , benchmark_info : dict , force_test : bool = True ): \"\"\"Submits a new cube to the medperf platform Args: benchmark_info (dict): benchmark information expected keys: name (str): benchmark name description (str): benchmark description docs_url (str): benchmark documentation url demo_url (str): benchmark demo dataset url demo_hash (str): benchmark demo dataset hash data_preparation_mlcube (str): benchmark data preparation mlcube uid reference_model_mlcube (str): benchmark reference model mlcube uid evaluator_mlcube (str): benchmark data evaluator mlcube uid \"\"\" ui = config . ui submission = cls ( benchmark_info , force_test ) if not submission . is_valid (): pretty_error ( \"Invalid benchmark information\" ) with ui . interactive (): ui . text = \"Getting additional information\" submission . get_extra_information () ui . print ( \"> Completed benchmark registration information\" ) ui . text = \"Submitting Benchmark to MedPerf\" updated_benchmark_body = submission . submit () ui . print ( \"Uploaded\" ) submission . write ( updated_benchmark_body ) run_compatibility_test () Runs a compatibility test to ensure elements are compatible, and to extract additional information required for submission Source code in commands/benchmark/submit.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def run_compatibility_test ( self ): \"\"\"Runs a compatibility test to ensure elements are compatible, and to extract additional information required for submission \"\"\" self . ui . print ( \"Running compatibility test\" ) data_prep = self . data_preparation_mlcube model = self . reference_model_mlcube evaluator = self . data_evaluator_mlcube demo_url = self . demo_url demo_hash = self . demo_hash benchmark = Benchmark . tmp ( data_prep , model , evaluator , demo_url , demo_hash ) _ , data_uid , _ , results = CompatibilityTestExecution . run ( benchmark . uid , force_test = self . force_test ) # Datasets generated by the compatibility test come with a prefix to identify them # This is not what we need for benchmark submissions, so we need to remove it prefix_len = len ( config . test_dset_prefix ) data_uid = data_uid [ prefix_len :] return data_uid , results","title":"submit"},{"location":"reference/commands/benchmark/submit/#commands.benchmark.submit.SubmitBenchmark","text":"Source code in commands/benchmark/submit.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 class SubmitBenchmark : @classmethod def run ( cls , benchmark_info : dict , force_test : bool = True ): \"\"\"Submits a new cube to the medperf platform Args: benchmark_info (dict): benchmark information expected keys: name (str): benchmark name description (str): benchmark description docs_url (str): benchmark documentation url demo_url (str): benchmark demo dataset url demo_hash (str): benchmark demo dataset hash data_preparation_mlcube (str): benchmark data preparation mlcube uid reference_model_mlcube (str): benchmark reference model mlcube uid evaluator_mlcube (str): benchmark data evaluator mlcube uid \"\"\" ui = config . ui submission = cls ( benchmark_info , force_test ) if not submission . is_valid (): pretty_error ( \"Invalid benchmark information\" ) with ui . interactive (): ui . text = \"Getting additional information\" submission . get_extra_information () ui . print ( \"> Completed benchmark registration information\" ) ui . text = \"Submitting Benchmark to MedPerf\" updated_benchmark_body = submission . submit () ui . print ( \"Uploaded\" ) submission . write ( updated_benchmark_body ) def __init__ ( self , benchmark_info : dict , force_test : bool = True ): self . comms = config . comms self . ui = config . ui self . name = benchmark_info [ \"name\" ] self . description = benchmark_info [ \"description\" ] self . docs_url = benchmark_info [ \"docs_url\" ] self . demo_url = benchmark_info [ \"demo_url\" ] self . demo_hash = benchmark_info [ \"demo_hash\" ] self . demo_uid = None self . data_preparation_mlcube = benchmark_info [ \"data_preparation_mlcube\" ] self . reference_model_mlcube = benchmark_info [ \"reference_model_mlcube\" ] self . data_evaluator_mlcube = benchmark_info [ \"evaluator_mlcube\" ] self . results = None self . force_test = force_test def is_valid ( self ) -> bool : \"\"\"Validates that user-provided benchmark information is correct Returns: bool: Wether or not the benchmark information is valid \"\"\" name_valid_length = 0 < len ( self . name ) < 20 desc_valid_length = len ( self . description ) < 100 docs_url_valid = self . docs_url == \"\" or validators . url ( self . docs_url ) demo_url_valid = self . demo_url == \"\" or validators . url ( self . demo_url ) demo_hash_if_no_url = self . demo_url or self . demo_hash prep_uid_valid = self . data_preparation_mlcube . isdigit () model_uid_valid = self . reference_model_mlcube . isdigit () eval_uid_valid = self . data_evaluator_mlcube . isdigit () valid_tests = [ ( \"name\" , name_valid_length , \"Name is invalid\" ), ( \"description\" , desc_valid_length , \"Description is too long\" ), ( \"docs_url\" , docs_url_valid , \"Documentation URL is invalid\" ), ( \"demo_url\" , demo_url_valid , \"Demo Dataset Tarball URL is invalid\" ), ( \"demo_hash\" , demo_hash_if_no_url , \"Demo Datset Hash must be provided if no URL passed\" , ), ( \"data_preparation_mlcube\" , prep_uid_valid , \"Data Preparation MLCube UID is invalid\" , ), ( \"reference_model_mlcube\" , model_uid_valid , \"Reference Model MLCube is invalid\" , ), ( \"data_evaluator_mlcube\" , eval_uid_valid , \"Data Evaluator MLCube is invalid\" , ), ] valid = True for attr , test , error_msg in valid_tests : if not test : valid = False self . ui . print_error ( error_msg ) return valid def get_extra_information ( self ): \"\"\"Retrieves information that must be populated automatically, like hash, generated uid and test results \"\"\" tmp_uid = self . demo_hash if self . demo_hash else generate_tmp_uid () demo_dset_path = self . comms . get_benchmark_demo_dataset ( self . demo_url , tmp_uid ) demo_hash = get_file_sha1 ( demo_dset_path ) if self . demo_hash and demo_hash != self . demo_hash : logging . error ( f \"Demo dataset hash mismatch: { demo_hash } != { self . demo_hash } \" ) pretty_error ( \"Demo dataset hash does not match the provided hash\" ) self . demo_hash = demo_hash demo_uid , results = self . run_compatibility_test () self . demo_uid = demo_uid self . results = results def run_compatibility_test ( self ): \"\"\"Runs a compatibility test to ensure elements are compatible, and to extract additional information required for submission \"\"\" self . ui . print ( \"Running compatibility test\" ) data_prep = self . data_preparation_mlcube model = self . reference_model_mlcube evaluator = self . data_evaluator_mlcube demo_url = self . demo_url demo_hash = self . demo_hash benchmark = Benchmark . tmp ( data_prep , model , evaluator , demo_url , demo_hash ) _ , data_uid , _ , results = CompatibilityTestExecution . run ( benchmark . uid , force_test = self . force_test ) # Datasets generated by the compatibility test come with a prefix to identify them # This is not what we need for benchmark submissions, so we need to remove it prefix_len = len ( config . test_dset_prefix ) data_uid = data_uid [ prefix_len :] return data_uid , results def todict ( self ): return { \"name\" : self . name , \"description\" : self . description , \"docs_url\" : self . docs_url , \"demo_dataset_tarball_url\" : self . demo_url , \"demo_dataset_tarball_hash\" : self . demo_hash , \"demo_dataset_generated_uid\" : self . demo_uid , \"data_preparation_mlcube\" : int ( self . data_preparation_mlcube ), \"reference_model_mlcube\" : int ( self . reference_model_mlcube ), \"data_evaluator_mlcube\" : int ( self . data_evaluator_mlcube ), \"state\" : \"OPERATION\" , \"is_valid\" : True , \"approval_status\" : Status . PENDING . value , \"metadata\" : { \"results\" : self . results . results }, \"id\" : None , \"models\" : [ int ( self . reference_model_mlcube )], # not in the server (OK) \"created_at\" : None , \"modified_at\" : None , \"approved_at\" : None , \"owner\" : None , \"is_active\" : True , \"user_metadata\" : {}, } def submit ( self ): body = self . todict () updated_body = Benchmark ( body ) . upload () return updated_body def write ( self , updated_body ): bmk = Benchmark ( updated_body ) bmk . write ()","title":"SubmitBenchmark"},{"location":"reference/commands/benchmark/submit/#commands.benchmark.submit.SubmitBenchmark.get_extra_information","text":"Retrieves information that must be populated automatically, like hash, generated uid and test results Source code in commands/benchmark/submit.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def get_extra_information ( self ): \"\"\"Retrieves information that must be populated automatically, like hash, generated uid and test results \"\"\" tmp_uid = self . demo_hash if self . demo_hash else generate_tmp_uid () demo_dset_path = self . comms . get_benchmark_demo_dataset ( self . demo_url , tmp_uid ) demo_hash = get_file_sha1 ( demo_dset_path ) if self . demo_hash and demo_hash != self . demo_hash : logging . error ( f \"Demo dataset hash mismatch: { demo_hash } != { self . demo_hash } \" ) pretty_error ( \"Demo dataset hash does not match the provided hash\" ) self . demo_hash = demo_hash demo_uid , results = self . run_compatibility_test () self . demo_uid = demo_uid self . results = results","title":"get_extra_information()"},{"location":"reference/commands/benchmark/submit/#commands.benchmark.submit.SubmitBenchmark.is_valid","text":"Validates that user-provided benchmark information is correct Returns: Name Type Description bool bool Wether or not the benchmark information is valid Source code in commands/benchmark/submit.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def is_valid ( self ) -> bool : \"\"\"Validates that user-provided benchmark information is correct Returns: bool: Wether or not the benchmark information is valid \"\"\" name_valid_length = 0 < len ( self . name ) < 20 desc_valid_length = len ( self . description ) < 100 docs_url_valid = self . docs_url == \"\" or validators . url ( self . docs_url ) demo_url_valid = self . demo_url == \"\" or validators . url ( self . demo_url ) demo_hash_if_no_url = self . demo_url or self . demo_hash prep_uid_valid = self . data_preparation_mlcube . isdigit () model_uid_valid = self . reference_model_mlcube . isdigit () eval_uid_valid = self . data_evaluator_mlcube . isdigit () valid_tests = [ ( \"name\" , name_valid_length , \"Name is invalid\" ), ( \"description\" , desc_valid_length , \"Description is too long\" ), ( \"docs_url\" , docs_url_valid , \"Documentation URL is invalid\" ), ( \"demo_url\" , demo_url_valid , \"Demo Dataset Tarball URL is invalid\" ), ( \"demo_hash\" , demo_hash_if_no_url , \"Demo Datset Hash must be provided if no URL passed\" , ), ( \"data_preparation_mlcube\" , prep_uid_valid , \"Data Preparation MLCube UID is invalid\" , ), ( \"reference_model_mlcube\" , model_uid_valid , \"Reference Model MLCube is invalid\" , ), ( \"data_evaluator_mlcube\" , eval_uid_valid , \"Data Evaluator MLCube is invalid\" , ), ] valid = True for attr , test , error_msg in valid_tests : if not test : valid = False self . ui . print_error ( error_msg ) return valid","title":"is_valid()"},{"location":"reference/commands/benchmark/submit/#commands.benchmark.submit.SubmitBenchmark.run","text":"Submits a new cube to the medperf platform Parameters: Name Type Description Default benchmark_info dict benchmark information expected keys: name (str): benchmark name description (str): benchmark description docs_url (str): benchmark documentation url demo_url (str): benchmark demo dataset url demo_hash (str): benchmark demo dataset hash data_preparation_mlcube (str): benchmark data preparation mlcube uid reference_model_mlcube (str): benchmark reference model mlcube uid evaluator_mlcube (str): benchmark data evaluator mlcube uid required Source code in commands/benchmark/submit.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 @classmethod def run ( cls , benchmark_info : dict , force_test : bool = True ): \"\"\"Submits a new cube to the medperf platform Args: benchmark_info (dict): benchmark information expected keys: name (str): benchmark name description (str): benchmark description docs_url (str): benchmark documentation url demo_url (str): benchmark demo dataset url demo_hash (str): benchmark demo dataset hash data_preparation_mlcube (str): benchmark data preparation mlcube uid reference_model_mlcube (str): benchmark reference model mlcube uid evaluator_mlcube (str): benchmark data evaluator mlcube uid \"\"\" ui = config . ui submission = cls ( benchmark_info , force_test ) if not submission . is_valid (): pretty_error ( \"Invalid benchmark information\" ) with ui . interactive (): ui . text = \"Getting additional information\" submission . get_extra_information () ui . print ( \"> Completed benchmark registration information\" ) ui . text = \"Submitting Benchmark to MedPerf\" updated_benchmark_body = submission . submit () ui . print ( \"Uploaded\" ) submission . write ( updated_benchmark_body )","title":"run()"},{"location":"reference/commands/benchmark/submit/#commands.benchmark.submit.SubmitBenchmark.run_compatibility_test","text":"Runs a compatibility test to ensure elements are compatible, and to extract additional information required for submission Source code in commands/benchmark/submit.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def run_compatibility_test ( self ): \"\"\"Runs a compatibility test to ensure elements are compatible, and to extract additional information required for submission \"\"\" self . ui . print ( \"Running compatibility test\" ) data_prep = self . data_preparation_mlcube model = self . reference_model_mlcube evaluator = self . data_evaluator_mlcube demo_url = self . demo_url demo_hash = self . demo_hash benchmark = Benchmark . tmp ( data_prep , model , evaluator , demo_url , demo_hash ) _ , data_uid , _ , results = CompatibilityTestExecution . run ( benchmark . uid , force_test = self . force_test ) # Datasets generated by the compatibility test come with a prefix to identify them # This is not what we need for benchmark submissions, so we need to remove it prefix_len = len ( config . test_dset_prefix ) data_uid = data_uid [ prefix_len :] return data_uid , results","title":"run_compatibility_test()"},{"location":"reference/commands/dataset/associate/","text":"AssociateDataset Source code in commands/dataset/associate.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 class AssociateDataset : @staticmethod def run ( data_uid : str , benchmark_uid : int , approved = False , force_test = False ): \"\"\"Associates a registered dataset with a benchmark Args: data_uid (int): UID of the registered dataset to associate benchmark_uid (int): UID of the benchmark to associate with \"\"\" comms = config . comms ui = config . ui dset = Dataset . from_generated_uid ( data_uid ) if dset . uid is None : msg = \"The provided dataset is not registered.\" pretty_error ( msg ) benchmark = Benchmark . get ( benchmark_uid ) if str ( dset . preparation_cube_uid ) != str ( benchmark . data_preparation ): pretty_error ( \"The specified dataset wasn't prepared for this benchmark\" ) _ , _ , _ , result = CompatibilityTestExecution . run ( benchmark_uid , data_uid = data_uid , force_test = force_test ) ui . print ( \"These are the results generated by the compatibility test. \" ) ui . print ( \"This will be sent along the association request.\" ) ui . print ( \"They will not be part of the benchmark.\" ) dict_pretty_print ( result . results ) msg = \"Please confirm that you would like to associate\" msg += f \" the dataset { dset . name } with the benchmark { benchmark . name } .\" msg += \" [Y/n]\" approved = approved or approval_prompt ( msg ) if approved : ui . print ( \"Generating dataset benchmark association\" ) metadata = { \"test_result\" : result . results } comms . associate_dset ( dset . uid , benchmark_uid , metadata ) else : pretty_error ( \"Dataset association operation cancelled\" , add_instructions = False ) run ( data_uid , benchmark_uid , approved = False , force_test = False ) staticmethod Associates a registered dataset with a benchmark Parameters: Name Type Description Default data_uid int UID of the registered dataset to associate required benchmark_uid int UID of the benchmark to associate with required Source code in commands/dataset/associate.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 @staticmethod def run ( data_uid : str , benchmark_uid : int , approved = False , force_test = False ): \"\"\"Associates a registered dataset with a benchmark Args: data_uid (int): UID of the registered dataset to associate benchmark_uid (int): UID of the benchmark to associate with \"\"\" comms = config . comms ui = config . ui dset = Dataset . from_generated_uid ( data_uid ) if dset . uid is None : msg = \"The provided dataset is not registered.\" pretty_error ( msg ) benchmark = Benchmark . get ( benchmark_uid ) if str ( dset . preparation_cube_uid ) != str ( benchmark . data_preparation ): pretty_error ( \"The specified dataset wasn't prepared for this benchmark\" ) _ , _ , _ , result = CompatibilityTestExecution . run ( benchmark_uid , data_uid = data_uid , force_test = force_test ) ui . print ( \"These are the results generated by the compatibility test. \" ) ui . print ( \"This will be sent along the association request.\" ) ui . print ( \"They will not be part of the benchmark.\" ) dict_pretty_print ( result . results ) msg = \"Please confirm that you would like to associate\" msg += f \" the dataset { dset . name } with the benchmark { benchmark . name } .\" msg += \" [Y/n]\" approved = approved or approval_prompt ( msg ) if approved : ui . print ( \"Generating dataset benchmark association\" ) metadata = { \"test_result\" : result . results } comms . associate_dset ( dset . uid , benchmark_uid , metadata ) else : pretty_error ( \"Dataset association operation cancelled\" , add_instructions = False )","title":"associate"},{"location":"reference/commands/dataset/associate/#commands.dataset.associate.AssociateDataset","text":"Source code in commands/dataset/associate.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 class AssociateDataset : @staticmethod def run ( data_uid : str , benchmark_uid : int , approved = False , force_test = False ): \"\"\"Associates a registered dataset with a benchmark Args: data_uid (int): UID of the registered dataset to associate benchmark_uid (int): UID of the benchmark to associate with \"\"\" comms = config . comms ui = config . ui dset = Dataset . from_generated_uid ( data_uid ) if dset . uid is None : msg = \"The provided dataset is not registered.\" pretty_error ( msg ) benchmark = Benchmark . get ( benchmark_uid ) if str ( dset . preparation_cube_uid ) != str ( benchmark . data_preparation ): pretty_error ( \"The specified dataset wasn't prepared for this benchmark\" ) _ , _ , _ , result = CompatibilityTestExecution . run ( benchmark_uid , data_uid = data_uid , force_test = force_test ) ui . print ( \"These are the results generated by the compatibility test. \" ) ui . print ( \"This will be sent along the association request.\" ) ui . print ( \"They will not be part of the benchmark.\" ) dict_pretty_print ( result . results ) msg = \"Please confirm that you would like to associate\" msg += f \" the dataset { dset . name } with the benchmark { benchmark . name } .\" msg += \" [Y/n]\" approved = approved or approval_prompt ( msg ) if approved : ui . print ( \"Generating dataset benchmark association\" ) metadata = { \"test_result\" : result . results } comms . associate_dset ( dset . uid , benchmark_uid , metadata ) else : pretty_error ( \"Dataset association operation cancelled\" , add_instructions = False )","title":"AssociateDataset"},{"location":"reference/commands/dataset/associate/#commands.dataset.associate.AssociateDataset.run","text":"Associates a registered dataset with a benchmark Parameters: Name Type Description Default data_uid int UID of the registered dataset to associate required benchmark_uid int UID of the benchmark to associate with required Source code in commands/dataset/associate.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 @staticmethod def run ( data_uid : str , benchmark_uid : int , approved = False , force_test = False ): \"\"\"Associates a registered dataset with a benchmark Args: data_uid (int): UID of the registered dataset to associate benchmark_uid (int): UID of the benchmark to associate with \"\"\" comms = config . comms ui = config . ui dset = Dataset . from_generated_uid ( data_uid ) if dset . uid is None : msg = \"The provided dataset is not registered.\" pretty_error ( msg ) benchmark = Benchmark . get ( benchmark_uid ) if str ( dset . preparation_cube_uid ) != str ( benchmark . data_preparation ): pretty_error ( \"The specified dataset wasn't prepared for this benchmark\" ) _ , _ , _ , result = CompatibilityTestExecution . run ( benchmark_uid , data_uid = data_uid , force_test = force_test ) ui . print ( \"These are the results generated by the compatibility test. \" ) ui . print ( \"This will be sent along the association request.\" ) ui . print ( \"They will not be part of the benchmark.\" ) dict_pretty_print ( result . results ) msg = \"Please confirm that you would like to associate\" msg += f \" the dataset { dset . name } with the benchmark { benchmark . name } .\" msg += \" [Y/n]\" approved = approved or approval_prompt ( msg ) if approved : ui . print ( \"Generating dataset benchmark association\" ) metadata = { \"test_result\" : result . results } comms . associate_dset ( dset . uid , benchmark_uid , metadata ) else : pretty_error ( \"Dataset association operation cancelled\" , add_instructions = False )","title":"run()"},{"location":"reference/commands/dataset/create/","text":"DataPreparation Source code in commands/dataset/create.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 class DataPreparation : @classmethod def run ( cls , benchmark_uid : str , prep_cube_uid : str , data_path : str , labels_path : str , run_test = False , name : str = None , description : str = None , location : str = None , ): preparation = cls ( benchmark_uid , prep_cube_uid , data_path , labels_path , name , description , location , run_test , ) preparation . validate () with preparation . ui . interactive (): preparation . get_prep_cube () preparation . run_cube_tasks () preparation . generate_uids () preparation . to_permanent_path () preparation . write () preparation . remove_temp_stats () return preparation . generated_uid def __init__ ( self , benchmark_uid : str , prep_cube_uid : str , data_path : str , labels_path : str , name : str , description : str , location : str , run_test = False , ): self . comms = config . comms self . ui = config . ui self . data_path = str ( Path ( data_path ) . resolve ()) self . labels_path = str ( Path ( labels_path ) . resolve ()) out_path = generate_tmp_datapath () self . out_path = out_path self . name = name self . description = description self . location = location self . out_datapath = os . path . join ( out_path , \"data\" ) self . out_labelspath = os . path . join ( out_path , \"labels\" ) self . labels_specified = False self . run_test = run_test self . benchmark_uid = benchmark_uid self . prep_cube_uid = prep_cube_uid self . in_uid = None self . generated_uid = None init_storage () def validate ( self ): if not os . path . exists ( self . data_path ): pretty_error ( \"The provided data path doesn't exist\" ) if not os . path . exists ( self . labels_path ): pretty_error ( \"The provided labels path doesn't exist\" ) too_many_resources = self . benchmark_uid and self . prep_cube_uid no_resource = self . benchmark_uid is None and self . prep_cube_uid is None if no_resource or too_many_resources : pretty_error ( \"Invalid arguments. Must provide either a benchmark or a preparation mlcube\" ) def get_prep_cube ( self ): cube_uid = self . prep_cube_uid if cube_uid is None : benchmark = Benchmark . get ( self . benchmark_uid ) cube_uid = benchmark . data_preparation self . ui . print ( f \"Benchmark Data Preparation: { benchmark . name } \" ) self . ui . text = f \"Retrieving data preparation cube: ' { cube_uid } '\" self . cube = Cube . get ( cube_uid ) self . ui . print ( \"> Preparation cube download complete\" ) check_cube_validity ( self . cube ) def run_cube_tasks ( self ): prepare_timeout = config . prepare_timeout sanity_check_timeout = config . sanity_check_timeout statistics_timeout = config . statistics_timeout data_path = self . data_path labels_path = self . labels_path out_datapath = self . out_datapath out_labelspath = self . out_labelspath out_statistics_path = os . path . join ( self . out_path , config . statistics_filename ) # Specify parameters for the tasks prepare_params = { \"data_path\" : data_path , \"labels_path\" : labels_path , \"output_path\" : out_datapath , } prepare_str_params = { \"Ptasks.prepare.parameters.input.data_path.opts\" : \"ro\" , \"Ptasks.prepare.parameters.input.labels_path.opts\" : \"ro\" } sanity_params = { \"data_path\" : out_datapath , } sanity_str_params = { \"Ptasks.sanity_check.parameters.input.data_path.opts\" : \"ro\" } statistics_params = { \"data_path\" : out_datapath , \"output_path\" : out_statistics_path , } statistics_str_params = { \"Ptasks.statistics.parameters.input.data_path.opts\" : \"ro\" } # Check if labels_path is specified self . labels_specified = ( self . cube . get_default_output ( \"prepare\" , \"output_labels_path\" ) is not None ) if self . labels_specified : # Add the labels parameter prepare_params [ \"output_labels_path\" ] = out_labelspath sanity_params [ \"labels_path\" ] = out_labelspath statistics_params [ \"labels_path\" ] = out_labelspath # Run the tasks self . ui . text = \"Running preparation step...\" try : self . cube . run ( task = \"prepare\" , string_params = prepare_str_params , timeout = prepare_timeout , ** prepare_params , ) self . ui . print ( \"> Cube execution complete\" ) self . ui . text = \"Running sanity check...\" self . cube . run ( task = \"sanity_check\" , string_params = sanity_str_params , timeout = sanity_check_timeout , ** sanity_params , ) self . ui . print ( \"> Sanity checks complete\" ) self . ui . text = \"Generating statistics...\" self . cube . run ( task = \"statistics\" , string_params = statistics_str_params , timeout = statistics_timeout , ** statistics_params ) self . ui . print ( \"> Statistics complete\" ) except RuntimeError as e : logging . error ( f \"MLCube Execution failed: { e } \" ) cleanup ([ self . out_path ]) pretty_error ( \"Data preparation failed\" ) def generate_uids ( self ): \"\"\"Auto-generates dataset UIDs for both input and output paths \"\"\" self . in_uid = get_folder_sha1 ( self . data_path ) self . generated_uid = get_folder_sha1 ( self . out_datapath ) if self . run_test : self . in_uid = config . test_dset_prefix + self . in_uid self . generated_uid = config . test_dset_prefix + self . generated_uid def to_permanent_path ( self ) -> str : \"\"\"Renames the temporary data folder to permanent one using the hash of the registration file \"\"\" new_path = os . path . join ( str ( Path ( self . out_path ) . parent ), self . generated_uid ) if os . path . exists ( new_path ): shutil . rmtree ( new_path ) os . rename ( self . out_path , new_path ) self . out_path = new_path def todict ( self ) -> dict : \"\"\"Dictionary representation of the dataset Returns: dict: dictionary containing information pertaining the dataset. \"\"\" return { \"id\" : None , \"name\" : self . name , \"description\" : self . description , \"location\" : self . location , \"data_preparation_mlcube\" : self . cube . uid , \"input_data_hash\" : self . in_uid , \"generated_uid\" : self . generated_uid , \"split_seed\" : 0 , # Currently this is not used \"generated_metadata\" : self . get_temp_stats (), \"status\" : Status . PENDING . value , # not in the server \"state\" : \"OPERATION\" , \"separate_labels\" : self . labels_specified , # not in the server \"is_valid\" : True , \"user_metadata\" : {}, \"created_at\" : None , \"modified_at\" : None , \"owner\" : None , } def get_temp_stats ( self ): stats_path = os . path . join ( self . out_path , config . statistics_filename ) with open ( stats_path , \"r\" ) as f : stats = yaml . safe_load ( f ) return stats def remove_temp_stats ( self ): stats_path = os . path . join ( self . out_path , config . statistics_filename ) os . remove ( stats_path ) def write ( self ) -> str : \"\"\"Writes the registration into disk Args: filename (str, optional): name of the file. Defaults to config.reg_file. \"\"\" dataset_dict = self . todict () dataset = Dataset ( dataset_dict ) dataset . write () generate_uids () Auto-generates dataset UIDs for both input and output paths Source code in commands/dataset/create.py 188 189 190 191 192 193 194 195 def generate_uids ( self ): \"\"\"Auto-generates dataset UIDs for both input and output paths \"\"\" self . in_uid = get_folder_sha1 ( self . data_path ) self . generated_uid = get_folder_sha1 ( self . out_datapath ) if self . run_test : self . in_uid = config . test_dset_prefix + self . in_uid self . generated_uid = config . test_dset_prefix + self . generated_uid to_permanent_path () Renames the temporary data folder to permanent one using the hash of the registration file Source code in commands/dataset/create.py 197 198 199 200 201 202 203 204 205 def to_permanent_path ( self ) -> str : \"\"\"Renames the temporary data folder to permanent one using the hash of the registration file \"\"\" new_path = os . path . join ( str ( Path ( self . out_path ) . parent ), self . generated_uid ) if os . path . exists ( new_path ): shutil . rmtree ( new_path ) os . rename ( self . out_path , new_path ) self . out_path = new_path todict () Dictionary representation of the dataset Returns: Name Type Description dict dict dictionary containing information pertaining the dataset. Source code in commands/dataset/create.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 def todict ( self ) -> dict : \"\"\"Dictionary representation of the dataset Returns: dict: dictionary containing information pertaining the dataset. \"\"\" return { \"id\" : None , \"name\" : self . name , \"description\" : self . description , \"location\" : self . location , \"data_preparation_mlcube\" : self . cube . uid , \"input_data_hash\" : self . in_uid , \"generated_uid\" : self . generated_uid , \"split_seed\" : 0 , # Currently this is not used \"generated_metadata\" : self . get_temp_stats (), \"status\" : Status . PENDING . value , # not in the server \"state\" : \"OPERATION\" , \"separate_labels\" : self . labels_specified , # not in the server \"is_valid\" : True , \"user_metadata\" : {}, \"created_at\" : None , \"modified_at\" : None , \"owner\" : None , } write () Writes the registration into disk Parameters: Name Type Description Default filename str name of the file. Defaults to config.reg_file. required Source code in commands/dataset/create.py 243 244 245 246 247 248 249 250 def write ( self ) -> str : \"\"\"Writes the registration into disk Args: filename (str, optional): name of the file. Defaults to config.reg_file. \"\"\" dataset_dict = self . todict () dataset = Dataset ( dataset_dict ) dataset . write ()","title":"create"},{"location":"reference/commands/dataset/create/#commands.dataset.create.DataPreparation","text":"Source code in commands/dataset/create.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 class DataPreparation : @classmethod def run ( cls , benchmark_uid : str , prep_cube_uid : str , data_path : str , labels_path : str , run_test = False , name : str = None , description : str = None , location : str = None , ): preparation = cls ( benchmark_uid , prep_cube_uid , data_path , labels_path , name , description , location , run_test , ) preparation . validate () with preparation . ui . interactive (): preparation . get_prep_cube () preparation . run_cube_tasks () preparation . generate_uids () preparation . to_permanent_path () preparation . write () preparation . remove_temp_stats () return preparation . generated_uid def __init__ ( self , benchmark_uid : str , prep_cube_uid : str , data_path : str , labels_path : str , name : str , description : str , location : str , run_test = False , ): self . comms = config . comms self . ui = config . ui self . data_path = str ( Path ( data_path ) . resolve ()) self . labels_path = str ( Path ( labels_path ) . resolve ()) out_path = generate_tmp_datapath () self . out_path = out_path self . name = name self . description = description self . location = location self . out_datapath = os . path . join ( out_path , \"data\" ) self . out_labelspath = os . path . join ( out_path , \"labels\" ) self . labels_specified = False self . run_test = run_test self . benchmark_uid = benchmark_uid self . prep_cube_uid = prep_cube_uid self . in_uid = None self . generated_uid = None init_storage () def validate ( self ): if not os . path . exists ( self . data_path ): pretty_error ( \"The provided data path doesn't exist\" ) if not os . path . exists ( self . labels_path ): pretty_error ( \"The provided labels path doesn't exist\" ) too_many_resources = self . benchmark_uid and self . prep_cube_uid no_resource = self . benchmark_uid is None and self . prep_cube_uid is None if no_resource or too_many_resources : pretty_error ( \"Invalid arguments. Must provide either a benchmark or a preparation mlcube\" ) def get_prep_cube ( self ): cube_uid = self . prep_cube_uid if cube_uid is None : benchmark = Benchmark . get ( self . benchmark_uid ) cube_uid = benchmark . data_preparation self . ui . print ( f \"Benchmark Data Preparation: { benchmark . name } \" ) self . ui . text = f \"Retrieving data preparation cube: ' { cube_uid } '\" self . cube = Cube . get ( cube_uid ) self . ui . print ( \"> Preparation cube download complete\" ) check_cube_validity ( self . cube ) def run_cube_tasks ( self ): prepare_timeout = config . prepare_timeout sanity_check_timeout = config . sanity_check_timeout statistics_timeout = config . statistics_timeout data_path = self . data_path labels_path = self . labels_path out_datapath = self . out_datapath out_labelspath = self . out_labelspath out_statistics_path = os . path . join ( self . out_path , config . statistics_filename ) # Specify parameters for the tasks prepare_params = { \"data_path\" : data_path , \"labels_path\" : labels_path , \"output_path\" : out_datapath , } prepare_str_params = { \"Ptasks.prepare.parameters.input.data_path.opts\" : \"ro\" , \"Ptasks.prepare.parameters.input.labels_path.opts\" : \"ro\" } sanity_params = { \"data_path\" : out_datapath , } sanity_str_params = { \"Ptasks.sanity_check.parameters.input.data_path.opts\" : \"ro\" } statistics_params = { \"data_path\" : out_datapath , \"output_path\" : out_statistics_path , } statistics_str_params = { \"Ptasks.statistics.parameters.input.data_path.opts\" : \"ro\" } # Check if labels_path is specified self . labels_specified = ( self . cube . get_default_output ( \"prepare\" , \"output_labels_path\" ) is not None ) if self . labels_specified : # Add the labels parameter prepare_params [ \"output_labels_path\" ] = out_labelspath sanity_params [ \"labels_path\" ] = out_labelspath statistics_params [ \"labels_path\" ] = out_labelspath # Run the tasks self . ui . text = \"Running preparation step...\" try : self . cube . run ( task = \"prepare\" , string_params = prepare_str_params , timeout = prepare_timeout , ** prepare_params , ) self . ui . print ( \"> Cube execution complete\" ) self . ui . text = \"Running sanity check...\" self . cube . run ( task = \"sanity_check\" , string_params = sanity_str_params , timeout = sanity_check_timeout , ** sanity_params , ) self . ui . print ( \"> Sanity checks complete\" ) self . ui . text = \"Generating statistics...\" self . cube . run ( task = \"statistics\" , string_params = statistics_str_params , timeout = statistics_timeout , ** statistics_params ) self . ui . print ( \"> Statistics complete\" ) except RuntimeError as e : logging . error ( f \"MLCube Execution failed: { e } \" ) cleanup ([ self . out_path ]) pretty_error ( \"Data preparation failed\" ) def generate_uids ( self ): \"\"\"Auto-generates dataset UIDs for both input and output paths \"\"\" self . in_uid = get_folder_sha1 ( self . data_path ) self . generated_uid = get_folder_sha1 ( self . out_datapath ) if self . run_test : self . in_uid = config . test_dset_prefix + self . in_uid self . generated_uid = config . test_dset_prefix + self . generated_uid def to_permanent_path ( self ) -> str : \"\"\"Renames the temporary data folder to permanent one using the hash of the registration file \"\"\" new_path = os . path . join ( str ( Path ( self . out_path ) . parent ), self . generated_uid ) if os . path . exists ( new_path ): shutil . rmtree ( new_path ) os . rename ( self . out_path , new_path ) self . out_path = new_path def todict ( self ) -> dict : \"\"\"Dictionary representation of the dataset Returns: dict: dictionary containing information pertaining the dataset. \"\"\" return { \"id\" : None , \"name\" : self . name , \"description\" : self . description , \"location\" : self . location , \"data_preparation_mlcube\" : self . cube . uid , \"input_data_hash\" : self . in_uid , \"generated_uid\" : self . generated_uid , \"split_seed\" : 0 , # Currently this is not used \"generated_metadata\" : self . get_temp_stats (), \"status\" : Status . PENDING . value , # not in the server \"state\" : \"OPERATION\" , \"separate_labels\" : self . labels_specified , # not in the server \"is_valid\" : True , \"user_metadata\" : {}, \"created_at\" : None , \"modified_at\" : None , \"owner\" : None , } def get_temp_stats ( self ): stats_path = os . path . join ( self . out_path , config . statistics_filename ) with open ( stats_path , \"r\" ) as f : stats = yaml . safe_load ( f ) return stats def remove_temp_stats ( self ): stats_path = os . path . join ( self . out_path , config . statistics_filename ) os . remove ( stats_path ) def write ( self ) -> str : \"\"\"Writes the registration into disk Args: filename (str, optional): name of the file. Defaults to config.reg_file. \"\"\" dataset_dict = self . todict () dataset = Dataset ( dataset_dict ) dataset . write ()","title":"DataPreparation"},{"location":"reference/commands/dataset/create/#commands.dataset.create.DataPreparation.generate_uids","text":"Auto-generates dataset UIDs for both input and output paths Source code in commands/dataset/create.py 188 189 190 191 192 193 194 195 def generate_uids ( self ): \"\"\"Auto-generates dataset UIDs for both input and output paths \"\"\" self . in_uid = get_folder_sha1 ( self . data_path ) self . generated_uid = get_folder_sha1 ( self . out_datapath ) if self . run_test : self . in_uid = config . test_dset_prefix + self . in_uid self . generated_uid = config . test_dset_prefix + self . generated_uid","title":"generate_uids()"},{"location":"reference/commands/dataset/create/#commands.dataset.create.DataPreparation.to_permanent_path","text":"Renames the temporary data folder to permanent one using the hash of the registration file Source code in commands/dataset/create.py 197 198 199 200 201 202 203 204 205 def to_permanent_path ( self ) -> str : \"\"\"Renames the temporary data folder to permanent one using the hash of the registration file \"\"\" new_path = os . path . join ( str ( Path ( self . out_path ) . parent ), self . generated_uid ) if os . path . exists ( new_path ): shutil . rmtree ( new_path ) os . rename ( self . out_path , new_path ) self . out_path = new_path","title":"to_permanent_path()"},{"location":"reference/commands/dataset/create/#commands.dataset.create.DataPreparation.todict","text":"Dictionary representation of the dataset Returns: Name Type Description dict dict dictionary containing information pertaining the dataset. Source code in commands/dataset/create.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 def todict ( self ) -> dict : \"\"\"Dictionary representation of the dataset Returns: dict: dictionary containing information pertaining the dataset. \"\"\" return { \"id\" : None , \"name\" : self . name , \"description\" : self . description , \"location\" : self . location , \"data_preparation_mlcube\" : self . cube . uid , \"input_data_hash\" : self . in_uid , \"generated_uid\" : self . generated_uid , \"split_seed\" : 0 , # Currently this is not used \"generated_metadata\" : self . get_temp_stats (), \"status\" : Status . PENDING . value , # not in the server \"state\" : \"OPERATION\" , \"separate_labels\" : self . labels_specified , # not in the server \"is_valid\" : True , \"user_metadata\" : {}, \"created_at\" : None , \"modified_at\" : None , \"owner\" : None , }","title":"todict()"},{"location":"reference/commands/dataset/create/#commands.dataset.create.DataPreparation.write","text":"Writes the registration into disk Parameters: Name Type Description Default filename str name of the file. Defaults to config.reg_file. required Source code in commands/dataset/create.py 243 244 245 246 247 248 249 250 def write ( self ) -> str : \"\"\"Writes the registration into disk Args: filename (str, optional): name of the file. Defaults to config.reg_file. \"\"\" dataset_dict = self . todict () dataset = Dataset ( dataset_dict ) dataset . write ()","title":"write()"},{"location":"reference/commands/dataset/dataset/","text":"associate ( data_uid = typer . Option ( Ellipsis , '--data_uid' , '-d' , help = 'Registered Dataset UID' ), benchmark_uid = typer . Option ( Ellipsis , '-benchmark_uid' , '-b' , help = 'Benchmark UID' ), approval = typer . Option ( False , '-y' , help = 'Skip approval step' ), force_test = typer . Option ( False , '--force-test' , help = 'Execute the test even if results already exist' )) Associate a registered dataset with a specific benchmark. The dataset and benchmark must share the same data preparation cube. Source code in commands/dataset/dataset.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 @app . command ( \"associate\" ) @clean_except def associate ( data_uid : str = typer . Option ( ... , \"--data_uid\" , \"-d\" , help = \"Registered Dataset UID\" ), benchmark_uid : int = typer . Option ( ... , \"-benchmark_uid\" , \"-b\" , help = \"Benchmark UID\" ), approval : bool = typer . Option ( False , \"-y\" , help = \"Skip approval step\" ), force_test : bool = typer . Option ( False , \"--force-test\" , help = \"Execute the test even if results already exist\" , ), ): \"\"\"Associate a registered dataset with a specific benchmark. The dataset and benchmark must share the same data preparation cube. \"\"\" ui = config . ui AssociateDataset . run ( data_uid , benchmark_uid , approved = approval , force_test = force_test ) ui . print ( \"\u2705 Done!\" ) ui . print ( f \"Next step: Once approved, run the benchmark with 'medperf run -b { benchmark_uid } -d { data_uid } '\" ) create ( benchmark_uid = typer . Option ( None , '--benchmark' , '-b' , help = 'UID of the desired benchmark' ), data_prep_uid = typer . Option ( None , '--data_prep' , '-p' , help = 'UID of the desired preparation cube' ), data_path = typer . Option ( Ellipsis , '--data_path' , '-d' , help = 'Location of the data to be prepared' ), labels_path = typer . Option ( Ellipsis , '--labels_path' , '-l' , help = 'Labels file location' ), name = typer . Option ( Ellipsis , '--name' , help = 'Name of the dataset' ), description = typer . Option ( Ellipsis , '--description' , help = 'Description of the dataset' ), location = typer . Option ( Ellipsis , '--location' , help = 'Location or Institution the data belongs to' )) Runs the Data preparation step for a specified benchmark and raw dataset Source code in commands/dataset/dataset.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 @app . command ( \"create\" ) @clean_except def create ( benchmark_uid : int = typer . Option ( None , \"--benchmark\" , \"-b\" , help = \"UID of the desired benchmark\" ), data_prep_uid : int = typer . Option ( None , \"--data_prep\" , \"-p\" , help = \"UID of the desired preparation cube\" ), data_path : str = typer . Option ( ... , \"--data_path\" , \"-d\" , help = \"Location of the data to be prepared\" ), labels_path : str = typer . Option ( ... , \"--labels_path\" , \"-l\" , help = \"Labels file location\" ), name : str = typer . Option ( ... , \"--name\" , help = \"Name of the dataset\" ), description : str = typer . Option ( ... , \"--description\" , help = \"Description of the dataset\" ), location : str = typer . Option ( ... , \"--location\" , help = \"Location or Institution the data belongs to\" ), ): \"\"\"Runs the Data preparation step for a specified benchmark and raw dataset \"\"\" ui = config . ui data_uid = DataPreparation . run ( benchmark_uid , data_prep_uid , data_path , labels_path , name = name , description = description , location = location , ) ui . print ( \"\u2705 Done!\" ) ui . print ( f \"Next step: register the dataset with 'medperf dataset submit -d { data_uid } '\" ) datasets ( all = typer . Option ( False , help = 'Get all datasets from the platform' )) Lists all datasets from the user by default. Use all to get all datasets in the platform Source code in commands/dataset/dataset.py 13 14 15 16 17 18 19 20 21 @app . command ( \"ls\" ) @clean_except def datasets ( all : bool = typer . Option ( False , help = \"Get all datasets from the platform\" ) ): \"\"\"Lists all datasets from the user by default. Use all to get all datasets in the platform \"\"\" DatasetsList . run ( all ) register ( data_uid = typer . Option ( Ellipsis , '--data_uid' , '-d' , help = 'Unregistered Dataset UID' ), approval = typer . Option ( False , '-y' , help = 'Skip approval step' )) Submits an unregistered Dataset instance to the backend Source code in commands/dataset/dataset.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 @app . command ( \"submit\" ) @clean_except def register ( data_uid : str = typer . Option ( ... , \"--data_uid\" , \"-d\" , help = \"Unregistered Dataset UID\" ), approval : bool = typer . Option ( False , \"-y\" , help = \"Skip approval step\" ), ): \"\"\"Submits an unregistered Dataset instance to the backend \"\"\" ui = config . ui DatasetRegistration . run ( data_uid , approved = approval ) ui . print ( \"\u2705 Done!\" ) ui . print ( f \"Next step: associate the dataset with 'medperf dataset associate -b <BENCHMARK_UID> -d { data_uid } '\" )","title":"dataset"},{"location":"reference/commands/dataset/dataset/#commands.dataset.dataset.associate","text":"Associate a registered dataset with a specific benchmark. The dataset and benchmark must share the same data preparation cube. Source code in commands/dataset/dataset.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 @app . command ( \"associate\" ) @clean_except def associate ( data_uid : str = typer . Option ( ... , \"--data_uid\" , \"-d\" , help = \"Registered Dataset UID\" ), benchmark_uid : int = typer . Option ( ... , \"-benchmark_uid\" , \"-b\" , help = \"Benchmark UID\" ), approval : bool = typer . Option ( False , \"-y\" , help = \"Skip approval step\" ), force_test : bool = typer . Option ( False , \"--force-test\" , help = \"Execute the test even if results already exist\" , ), ): \"\"\"Associate a registered dataset with a specific benchmark. The dataset and benchmark must share the same data preparation cube. \"\"\" ui = config . ui AssociateDataset . run ( data_uid , benchmark_uid , approved = approval , force_test = force_test ) ui . print ( \"\u2705 Done!\" ) ui . print ( f \"Next step: Once approved, run the benchmark with 'medperf run -b { benchmark_uid } -d { data_uid } '\" )","title":"associate()"},{"location":"reference/commands/dataset/dataset/#commands.dataset.dataset.create","text":"Runs the Data preparation step for a specified benchmark and raw dataset Source code in commands/dataset/dataset.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 @app . command ( \"create\" ) @clean_except def create ( benchmark_uid : int = typer . Option ( None , \"--benchmark\" , \"-b\" , help = \"UID of the desired benchmark\" ), data_prep_uid : int = typer . Option ( None , \"--data_prep\" , \"-p\" , help = \"UID of the desired preparation cube\" ), data_path : str = typer . Option ( ... , \"--data_path\" , \"-d\" , help = \"Location of the data to be prepared\" ), labels_path : str = typer . Option ( ... , \"--labels_path\" , \"-l\" , help = \"Labels file location\" ), name : str = typer . Option ( ... , \"--name\" , help = \"Name of the dataset\" ), description : str = typer . Option ( ... , \"--description\" , help = \"Description of the dataset\" ), location : str = typer . Option ( ... , \"--location\" , help = \"Location or Institution the data belongs to\" ), ): \"\"\"Runs the Data preparation step for a specified benchmark and raw dataset \"\"\" ui = config . ui data_uid = DataPreparation . run ( benchmark_uid , data_prep_uid , data_path , labels_path , name = name , description = description , location = location , ) ui . print ( \"\u2705 Done!\" ) ui . print ( f \"Next step: register the dataset with 'medperf dataset submit -d { data_uid } '\" )","title":"create()"},{"location":"reference/commands/dataset/dataset/#commands.dataset.dataset.datasets","text":"Lists all datasets from the user by default. Use all to get all datasets in the platform Source code in commands/dataset/dataset.py 13 14 15 16 17 18 19 20 21 @app . command ( \"ls\" ) @clean_except def datasets ( all : bool = typer . Option ( False , help = \"Get all datasets from the platform\" ) ): \"\"\"Lists all datasets from the user by default. Use all to get all datasets in the platform \"\"\" DatasetsList . run ( all )","title":"datasets()"},{"location":"reference/commands/dataset/dataset/#commands.dataset.dataset.register","text":"Submits an unregistered Dataset instance to the backend Source code in commands/dataset/dataset.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 @app . command ( \"submit\" ) @clean_except def register ( data_uid : str = typer . Option ( ... , \"--data_uid\" , \"-d\" , help = \"Unregistered Dataset UID\" ), approval : bool = typer . Option ( False , \"-y\" , help = \"Skip approval step\" ), ): \"\"\"Submits an unregistered Dataset instance to the backend \"\"\" ui = config . ui DatasetRegistration . run ( data_uid , approved = approval ) ui . print ( \"\u2705 Done!\" ) ui . print ( f \"Next step: associate the dataset with 'medperf dataset associate -b <BENCHMARK_UID> -d { data_uid } '\" )","title":"register()"},{"location":"reference/commands/dataset/list/","text":"DatasetsList Source code in commands/dataset/list.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 class DatasetsList : @staticmethod def run ( all : bool = False ): \"\"\"List all local and remote users created by user. Use \"all\" to list all remote datasets in the platform Args: comms (Comms): Communications instance ui (UI): UI instance all (bool, optional): List all datasets in the platform. Defaults to False. \"\"\" comms = config . comms ui = config . ui # Get local and remote datasets local_dsets = Dataset . all () if all : remote_dsets = comms . get_datasets () else : remote_dsets = comms . get_user_datasets () local_uids = set ([ dset . generated_uid for dset in local_dsets ]) remote_uids = set ([ dset [ \"generated_uid\" ] for dset in remote_dsets ]) # Build data table headers = [ \"UID\" , \"Server UID\" , \"Name\" , \"Data Preparation Cube UID\" , \"Registered\" , \"Local\" , ] # Get local dsets information local_dsets_data = [ [ dset . generated_uid , dset . uid , dset . name , dset . preparation_cube_uid , dset . generated_uid in remote_uids , True , ] for dset in local_dsets ] # Get remote dsets information filtered by local remote_dsets_data = [ [ dset [ \"generated_uid\" ], dset [ \"id\" ], dset [ \"name\" ], \"-\" , True , False ,] for dset in remote_dsets if dset [ \"generated_uid\" ] not in local_uids ] # Combine dsets dsets_data = local_dsets_data + remote_dsets_data tab = tabulate ( dsets_data , headers = headers ) ui . print ( tab ) run ( all = False ) staticmethod List all local and remote users created by user. Use \"all\" to list all remote datasets in the platform Parameters: Name Type Description Default comms Comms Communications instance required ui UI UI instance required all bool List all datasets in the platform. Defaults to False. False Source code in commands/dataset/list.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 @staticmethod def run ( all : bool = False ): \"\"\"List all local and remote users created by user. Use \"all\" to list all remote datasets in the platform Args: comms (Comms): Communications instance ui (UI): UI instance all (bool, optional): List all datasets in the platform. Defaults to False. \"\"\" comms = config . comms ui = config . ui # Get local and remote datasets local_dsets = Dataset . all () if all : remote_dsets = comms . get_datasets () else : remote_dsets = comms . get_user_datasets () local_uids = set ([ dset . generated_uid for dset in local_dsets ]) remote_uids = set ([ dset [ \"generated_uid\" ] for dset in remote_dsets ]) # Build data table headers = [ \"UID\" , \"Server UID\" , \"Name\" , \"Data Preparation Cube UID\" , \"Registered\" , \"Local\" , ] # Get local dsets information local_dsets_data = [ [ dset . generated_uid , dset . uid , dset . name , dset . preparation_cube_uid , dset . generated_uid in remote_uids , True , ] for dset in local_dsets ] # Get remote dsets information filtered by local remote_dsets_data = [ [ dset [ \"generated_uid\" ], dset [ \"id\" ], dset [ \"name\" ], \"-\" , True , False ,] for dset in remote_dsets if dset [ \"generated_uid\" ] not in local_uids ] # Combine dsets dsets_data = local_dsets_data + remote_dsets_data tab = tabulate ( dsets_data , headers = headers ) ui . print ( tab )","title":"list"},{"location":"reference/commands/dataset/list/#commands.dataset.list.DatasetsList","text":"Source code in commands/dataset/list.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 class DatasetsList : @staticmethod def run ( all : bool = False ): \"\"\"List all local and remote users created by user. Use \"all\" to list all remote datasets in the platform Args: comms (Comms): Communications instance ui (UI): UI instance all (bool, optional): List all datasets in the platform. Defaults to False. \"\"\" comms = config . comms ui = config . ui # Get local and remote datasets local_dsets = Dataset . all () if all : remote_dsets = comms . get_datasets () else : remote_dsets = comms . get_user_datasets () local_uids = set ([ dset . generated_uid for dset in local_dsets ]) remote_uids = set ([ dset [ \"generated_uid\" ] for dset in remote_dsets ]) # Build data table headers = [ \"UID\" , \"Server UID\" , \"Name\" , \"Data Preparation Cube UID\" , \"Registered\" , \"Local\" , ] # Get local dsets information local_dsets_data = [ [ dset . generated_uid , dset . uid , dset . name , dset . preparation_cube_uid , dset . generated_uid in remote_uids , True , ] for dset in local_dsets ] # Get remote dsets information filtered by local remote_dsets_data = [ [ dset [ \"generated_uid\" ], dset [ \"id\" ], dset [ \"name\" ], \"-\" , True , False ,] for dset in remote_dsets if dset [ \"generated_uid\" ] not in local_uids ] # Combine dsets dsets_data = local_dsets_data + remote_dsets_data tab = tabulate ( dsets_data , headers = headers ) ui . print ( tab )","title":"DatasetsList"},{"location":"reference/commands/dataset/list/#commands.dataset.list.DatasetsList.run","text":"List all local and remote users created by user. Use \"all\" to list all remote datasets in the platform Parameters: Name Type Description Default comms Comms Communications instance required ui UI UI instance required all bool List all datasets in the platform. Defaults to False. False Source code in commands/dataset/list.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 @staticmethod def run ( all : bool = False ): \"\"\"List all local and remote users created by user. Use \"all\" to list all remote datasets in the platform Args: comms (Comms): Communications instance ui (UI): UI instance all (bool, optional): List all datasets in the platform. Defaults to False. \"\"\" comms = config . comms ui = config . ui # Get local and remote datasets local_dsets = Dataset . all () if all : remote_dsets = comms . get_datasets () else : remote_dsets = comms . get_user_datasets () local_uids = set ([ dset . generated_uid for dset in local_dsets ]) remote_uids = set ([ dset [ \"generated_uid\" ] for dset in remote_dsets ]) # Build data table headers = [ \"UID\" , \"Server UID\" , \"Name\" , \"Data Preparation Cube UID\" , \"Registered\" , \"Local\" , ] # Get local dsets information local_dsets_data = [ [ dset . generated_uid , dset . uid , dset . name , dset . preparation_cube_uid , dset . generated_uid in remote_uids , True , ] for dset in local_dsets ] # Get remote dsets information filtered by local remote_dsets_data = [ [ dset [ \"generated_uid\" ], dset [ \"id\" ], dset [ \"name\" ], \"-\" , True , False ,] for dset in remote_dsets if dset [ \"generated_uid\" ] not in local_uids ] # Combine dsets dsets_data = local_dsets_data + remote_dsets_data tab = tabulate ( dsets_data , headers = headers ) ui . print ( tab )","title":"run()"},{"location":"reference/commands/dataset/submit/","text":"DatasetRegistration Source code in commands/dataset/submit.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class DatasetRegistration : @staticmethod def run ( data_uid : str , approved = False ): \"\"\"Registers a database to the backend. Args: data_uid (str): UID Hint of the unregistered dataset \"\"\" comms = config . comms ui = config . ui dset = Dataset . from_generated_uid ( data_uid ) if dset . uid : # TODO: should get_dataset and update locally. solves existing issue? pretty_error ( \"This dataset has already been registered.\" , add_instructions = False ) remote_dsets = comms . get_user_datasets () remote_dset = [ remote_dset for remote_dset in remote_dsets if remote_dset [ \"generated_uid\" ] == dset . generated_uid ] if len ( remote_dset ) == 1 : dset = Dataset ( remote_dset [ 0 ]) dset . write () ui . print ( f \"Remote dataset { dset . name } detected. Updating local dataset.\" ) return dict_pretty_print ( dset . todict ()) msg = \"Do you approve the registration of the presented data to the MLCommons comms? [Y/n] \" approved = approved or approval_prompt ( msg ) dset . status = Status ( \"APPROVED\" ) if approved else Status ( \"REJECTED\" ) if approved : ui . print ( \"Uploading...\" ) updated_dset_dict = dset . upload () updated_dset = Dataset ( updated_dset_dict ) updated_dset . write () else : pretty_error ( \"Registration request cancelled.\" , add_instructions = False ) run ( data_uid , approved = False ) staticmethod Registers a database to the backend. Parameters: Name Type Description Default data_uid str UID Hint of the unregistered dataset required Source code in commands/dataset/submit.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 @staticmethod def run ( data_uid : str , approved = False ): \"\"\"Registers a database to the backend. Args: data_uid (str): UID Hint of the unregistered dataset \"\"\" comms = config . comms ui = config . ui dset = Dataset . from_generated_uid ( data_uid ) if dset . uid : # TODO: should get_dataset and update locally. solves existing issue? pretty_error ( \"This dataset has already been registered.\" , add_instructions = False ) remote_dsets = comms . get_user_datasets () remote_dset = [ remote_dset for remote_dset in remote_dsets if remote_dset [ \"generated_uid\" ] == dset . generated_uid ] if len ( remote_dset ) == 1 : dset = Dataset ( remote_dset [ 0 ]) dset . write () ui . print ( f \"Remote dataset { dset . name } detected. Updating local dataset.\" ) return dict_pretty_print ( dset . todict ()) msg = \"Do you approve the registration of the presented data to the MLCommons comms? [Y/n] \" approved = approved or approval_prompt ( msg ) dset . status = Status ( \"APPROVED\" ) if approved else Status ( \"REJECTED\" ) if approved : ui . print ( \"Uploading...\" ) updated_dset_dict = dset . upload () updated_dset = Dataset ( updated_dset_dict ) updated_dset . write () else : pretty_error ( \"Registration request cancelled.\" , add_instructions = False )","title":"submit"},{"location":"reference/commands/dataset/submit/#commands.dataset.submit.DatasetRegistration","text":"Source code in commands/dataset/submit.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class DatasetRegistration : @staticmethod def run ( data_uid : str , approved = False ): \"\"\"Registers a database to the backend. Args: data_uid (str): UID Hint of the unregistered dataset \"\"\" comms = config . comms ui = config . ui dset = Dataset . from_generated_uid ( data_uid ) if dset . uid : # TODO: should get_dataset and update locally. solves existing issue? pretty_error ( \"This dataset has already been registered.\" , add_instructions = False ) remote_dsets = comms . get_user_datasets () remote_dset = [ remote_dset for remote_dset in remote_dsets if remote_dset [ \"generated_uid\" ] == dset . generated_uid ] if len ( remote_dset ) == 1 : dset = Dataset ( remote_dset [ 0 ]) dset . write () ui . print ( f \"Remote dataset { dset . name } detected. Updating local dataset.\" ) return dict_pretty_print ( dset . todict ()) msg = \"Do you approve the registration of the presented data to the MLCommons comms? [Y/n] \" approved = approved or approval_prompt ( msg ) dset . status = Status ( \"APPROVED\" ) if approved else Status ( \"REJECTED\" ) if approved : ui . print ( \"Uploading...\" ) updated_dset_dict = dset . upload () updated_dset = Dataset ( updated_dset_dict ) updated_dset . write () else : pretty_error ( \"Registration request cancelled.\" , add_instructions = False )","title":"DatasetRegistration"},{"location":"reference/commands/dataset/submit/#commands.dataset.submit.DatasetRegistration.run","text":"Registers a database to the backend. Parameters: Name Type Description Default data_uid str UID Hint of the unregistered dataset required Source code in commands/dataset/submit.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 @staticmethod def run ( data_uid : str , approved = False ): \"\"\"Registers a database to the backend. Args: data_uid (str): UID Hint of the unregistered dataset \"\"\" comms = config . comms ui = config . ui dset = Dataset . from_generated_uid ( data_uid ) if dset . uid : # TODO: should get_dataset and update locally. solves existing issue? pretty_error ( \"This dataset has already been registered.\" , add_instructions = False ) remote_dsets = comms . get_user_datasets () remote_dset = [ remote_dset for remote_dset in remote_dsets if remote_dset [ \"generated_uid\" ] == dset . generated_uid ] if len ( remote_dset ) == 1 : dset = Dataset ( remote_dset [ 0 ]) dset . write () ui . print ( f \"Remote dataset { dset . name } detected. Updating local dataset.\" ) return dict_pretty_print ( dset . todict ()) msg = \"Do you approve the registration of the presented data to the MLCommons comms? [Y/n] \" approved = approved or approval_prompt ( msg ) dset . status = Status ( \"APPROVED\" ) if approved else Status ( \"REJECTED\" ) if approved : ui . print ( \"Uploading...\" ) updated_dset_dict = dset . upload () updated_dset = Dataset ( updated_dset_dict ) updated_dset . write () else : pretty_error ( \"Registration request cancelled.\" , add_instructions = False )","title":"run()"},{"location":"reference/commands/mlcube/associate/","text":"AssociateCube Source code in commands/mlcube/associate.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class AssociateCube : @classmethod def run ( cls , cube_uid : str , benchmark_uid : int , approved = False , force_test = False , ): \"\"\"Associates a cube with a given benchmark Args: cube_uid (str): UID of model MLCube benchmark_uid (int): UID of benchmark approved (bool): Skip validation step. Defualts to False \"\"\" comms = config . comms ui = config . ui cube = Cube . get ( cube_uid ) benchmark = Benchmark . get ( benchmark_uid ) _ , _ , _ , result = CompatibilityTestExecution . run ( benchmark_uid , model = cube_uid , force_test = force_test ) ui . print ( \"These are the results generated by the compatibility test. \" ) ui . print ( \"This will be sent along the association request.\" ) ui . print ( \"They will not be part of the benchmark.\" ) dict_pretty_print ( result . results ) msg = \"Please confirm that you would like to associate \" msg += f \"the MLCube ' { cube . name } ' with the benchmark ' { benchmark . name } ' [Y/n]\" approved = approved or approval_prompt ( msg ) if approved : ui . print ( \"Generating mlcube benchmark association\" ) metadata = { \"test_result\" : result . results } comms . associate_cube ( cube_uid , benchmark_uid , metadata ) else : pretty_error ( \"MLCube association operation cancelled\" , add_instructions = False ) run ( cube_uid , benchmark_uid , approved = False , force_test = False ) classmethod Associates a cube with a given benchmark Parameters: Name Type Description Default cube_uid str UID of model MLCube required benchmark_uid int UID of benchmark required approved bool Skip validation step. Defualts to False False Source code in commands/mlcube/associate.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 @classmethod def run ( cls , cube_uid : str , benchmark_uid : int , approved = False , force_test = False , ): \"\"\"Associates a cube with a given benchmark Args: cube_uid (str): UID of model MLCube benchmark_uid (int): UID of benchmark approved (bool): Skip validation step. Defualts to False \"\"\" comms = config . comms ui = config . ui cube = Cube . get ( cube_uid ) benchmark = Benchmark . get ( benchmark_uid ) _ , _ , _ , result = CompatibilityTestExecution . run ( benchmark_uid , model = cube_uid , force_test = force_test ) ui . print ( \"These are the results generated by the compatibility test. \" ) ui . print ( \"This will be sent along the association request.\" ) ui . print ( \"They will not be part of the benchmark.\" ) dict_pretty_print ( result . results ) msg = \"Please confirm that you would like to associate \" msg += f \"the MLCube ' { cube . name } ' with the benchmark ' { benchmark . name } ' [Y/n]\" approved = approved or approval_prompt ( msg ) if approved : ui . print ( \"Generating mlcube benchmark association\" ) metadata = { \"test_result\" : result . results } comms . associate_cube ( cube_uid , benchmark_uid , metadata ) else : pretty_error ( \"MLCube association operation cancelled\" , add_instructions = False )","title":"associate"},{"location":"reference/commands/mlcube/associate/#commands.mlcube.associate.AssociateCube","text":"Source code in commands/mlcube/associate.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class AssociateCube : @classmethod def run ( cls , cube_uid : str , benchmark_uid : int , approved = False , force_test = False , ): \"\"\"Associates a cube with a given benchmark Args: cube_uid (str): UID of model MLCube benchmark_uid (int): UID of benchmark approved (bool): Skip validation step. Defualts to False \"\"\" comms = config . comms ui = config . ui cube = Cube . get ( cube_uid ) benchmark = Benchmark . get ( benchmark_uid ) _ , _ , _ , result = CompatibilityTestExecution . run ( benchmark_uid , model = cube_uid , force_test = force_test ) ui . print ( \"These are the results generated by the compatibility test. \" ) ui . print ( \"This will be sent along the association request.\" ) ui . print ( \"They will not be part of the benchmark.\" ) dict_pretty_print ( result . results ) msg = \"Please confirm that you would like to associate \" msg += f \"the MLCube ' { cube . name } ' with the benchmark ' { benchmark . name } ' [Y/n]\" approved = approved or approval_prompt ( msg ) if approved : ui . print ( \"Generating mlcube benchmark association\" ) metadata = { \"test_result\" : result . results } comms . associate_cube ( cube_uid , benchmark_uid , metadata ) else : pretty_error ( \"MLCube association operation cancelled\" , add_instructions = False )","title":"AssociateCube"},{"location":"reference/commands/mlcube/associate/#commands.mlcube.associate.AssociateCube.run","text":"Associates a cube with a given benchmark Parameters: Name Type Description Default cube_uid str UID of model MLCube required benchmark_uid int UID of benchmark required approved bool Skip validation step. Defualts to False False Source code in commands/mlcube/associate.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 @classmethod def run ( cls , cube_uid : str , benchmark_uid : int , approved = False , force_test = False , ): \"\"\"Associates a cube with a given benchmark Args: cube_uid (str): UID of model MLCube benchmark_uid (int): UID of benchmark approved (bool): Skip validation step. Defualts to False \"\"\" comms = config . comms ui = config . ui cube = Cube . get ( cube_uid ) benchmark = Benchmark . get ( benchmark_uid ) _ , _ , _ , result = CompatibilityTestExecution . run ( benchmark_uid , model = cube_uid , force_test = force_test ) ui . print ( \"These are the results generated by the compatibility test. \" ) ui . print ( \"This will be sent along the association request.\" ) ui . print ( \"They will not be part of the benchmark.\" ) dict_pretty_print ( result . results ) msg = \"Please confirm that you would like to associate \" msg += f \"the MLCube ' { cube . name } ' with the benchmark ' { benchmark . name } ' [Y/n]\" approved = approved or approval_prompt ( msg ) if approved : ui . print ( \"Generating mlcube benchmark association\" ) metadata = { \"test_result\" : result . results } comms . associate_cube ( cube_uid , benchmark_uid , metadata ) else : pretty_error ( \"MLCube association operation cancelled\" , add_instructions = False )","title":"run()"},{"location":"reference/commands/mlcube/list/","text":"CubesList Source code in commands/mlcube/list.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class CubesList : @staticmethod def run ( all : bool = False ): \"\"\"Lists all mlcubes created by the user by default. Use \"all\" to display all mlcubes in the platform Args: all (bool, optional): Wether to get all mlcubes. Defaults to False \"\"\" comms = config . comms ui = config . ui if all : cubes = comms . get_cubes () else : cubes = comms . get_user_cubes () headers = [ \"MLCube UID\" , \"Name\" , \"State\" ] cubes_data = [[ cube [ \"id\" ], cube [ \"name\" ], cube [ \"state\" ]] for cube in cubes ] tab = tabulate ( cubes_data , headers = headers ) ui . print ( tab ) run ( all = False ) staticmethod Lists all mlcubes created by the user by default. Use \"all\" to display all mlcubes in the platform Parameters: Name Type Description Default all bool Wether to get all mlcubes. Defaults to False False Source code in commands/mlcube/list.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 @staticmethod def run ( all : bool = False ): \"\"\"Lists all mlcubes created by the user by default. Use \"all\" to display all mlcubes in the platform Args: all (bool, optional): Wether to get all mlcubes. Defaults to False \"\"\" comms = config . comms ui = config . ui if all : cubes = comms . get_cubes () else : cubes = comms . get_user_cubes () headers = [ \"MLCube UID\" , \"Name\" , \"State\" ] cubes_data = [[ cube [ \"id\" ], cube [ \"name\" ], cube [ \"state\" ]] for cube in cubes ] tab = tabulate ( cubes_data , headers = headers ) ui . print ( tab )","title":"list"},{"location":"reference/commands/mlcube/list/#commands.mlcube.list.CubesList","text":"Source code in commands/mlcube/list.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class CubesList : @staticmethod def run ( all : bool = False ): \"\"\"Lists all mlcubes created by the user by default. Use \"all\" to display all mlcubes in the platform Args: all (bool, optional): Wether to get all mlcubes. Defaults to False \"\"\" comms = config . comms ui = config . ui if all : cubes = comms . get_cubes () else : cubes = comms . get_user_cubes () headers = [ \"MLCube UID\" , \"Name\" , \"State\" ] cubes_data = [[ cube [ \"id\" ], cube [ \"name\" ], cube [ \"state\" ]] for cube in cubes ] tab = tabulate ( cubes_data , headers = headers ) ui . print ( tab )","title":"CubesList"},{"location":"reference/commands/mlcube/list/#commands.mlcube.list.CubesList.run","text":"Lists all mlcubes created by the user by default. Use \"all\" to display all mlcubes in the platform Parameters: Name Type Description Default all bool Wether to get all mlcubes. Defaults to False False Source code in commands/mlcube/list.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 @staticmethod def run ( all : bool = False ): \"\"\"Lists all mlcubes created by the user by default. Use \"all\" to display all mlcubes in the platform Args: all (bool, optional): Wether to get all mlcubes. Defaults to False \"\"\" comms = config . comms ui = config . ui if all : cubes = comms . get_cubes () else : cubes = comms . get_user_cubes () headers = [ \"MLCube UID\" , \"Name\" , \"State\" ] cubes_data = [[ cube [ \"id\" ], cube [ \"name\" ], cube [ \"state\" ]] for cube in cubes ] tab = tabulate ( cubes_data , headers = headers ) ui . print ( tab )","title":"run()"},{"location":"reference/commands/mlcube/mlcube/","text":"associate ( benchmark_uid = typer . Option ( Ellipsis , '--benchmark' , '-b' , help = 'Benchmark UID' ), model_uid = typer . Option ( Ellipsis , '--model_uid' , '-m' , help = 'Model UID' ), approval = typer . Option ( False , '-y' , help = 'Skip approval step' ), force_test = typer . Option ( False , '--force-test' , help = 'Execute the test even if results already exist' )) Associates an MLCube to a benchmark Source code in commands/mlcube/mlcube.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 @app . command ( \"associate\" ) @clean_except def associate ( benchmark_uid : int = typer . Option ( ... , \"--benchmark\" , \"-b\" , help = \"Benchmark UID\" ), model_uid : int = typer . Option ( ... , \"--model_uid\" , \"-m\" , help = \"Model UID\" ), approval : bool = typer . Option ( False , \"-y\" , help = \"Skip approval step\" ), force_test : bool = typer . Option ( False , \"--force-test\" , help = \"Execute the test even if results already exist\" , ), ): \"\"\"Associates an MLCube to a benchmark\"\"\" AssociateCube . run ( model_uid , benchmark_uid , approved = approval , force_test = force_test ) config . ui . print ( \"\u2705 Done!\" ) list ( all = typer . Option ( False , help = 'Display all mlcubes' )) List mlcubes registered by the user by default. Use \"all\" to display all mlcubes in the platform Source code in commands/mlcube/mlcube.py 13 14 15 16 17 18 19 @app . command ( \"ls\" ) @clean_except def list ( all : bool = typer . Option ( False , help = \"Display all mlcubes\" )): \"\"\"List mlcubes registered by the user by default. Use \"all\" to display all mlcubes in the platform \"\"\" CubesList . run ( all ) submit ( name = typer . Option ( Ellipsis , '--name' , '-n' , help = 'Name of the mlcube' ), mlcube_file = typer . Option ( Ellipsis , '--mlcube-file' , '-m' , help = 'URL to mlcube file' ), mlcube_hash = typer . Option ( '' , '--mlcube-hash' , help = 'SHA1 of mlcube file' ), params_file = typer . Option ( '' , '--parameters-file' , '-p' , help = 'URL to parameters file' ), parameters_hash = typer . Option ( '' , '--parameters-hash' , help = 'SHA1 of parameters file' ), additional_file = typer . Option ( '' , '--additional-file' , '-a' , help = 'URL to additional files tarball' ), additional_hash = typer . Option ( '' , '--additional-hash' , help = 'SHA1 of additional file' ), image_file = typer . Option ( '' , '--image-file' , '-i' , help = 'URL to image file. Expected image to be compressed inside a tarball' ), image_hash = typer . Option ( '' , '--image-hash' , help = 'SHA1 of image file' )) Submits a new cube to the platform Source code in commands/mlcube/mlcube.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 @app . command ( \"submit\" ) @clean_except def submit ( name : str = typer . Option ( ... , \"--name\" , \"-n\" , help = \"Name of the mlcube\" ), mlcube_file : str = typer . Option ( ... , \"--mlcube-file\" , \"-m\" , help = \"URL to mlcube file\" ), mlcube_hash : str = typer . Option ( \"\" , \"--mlcube-hash\" , help = \"SHA1 of mlcube file\" ), params_file : str = typer . Option ( \"\" , \"--parameters-file\" , \"-p\" , help = \"URL to parameters file\" ), parameters_hash : str = typer . Option ( \"\" , \"--parameters-hash\" , help = \"SHA1 of parameters file\" ), additional_file : str = typer . Option ( \"\" , \"--additional-file\" , \"-a\" , help = \"URL to additional files tarball\" ), additional_hash : str = typer . Option ( \"\" , \"--additional-hash\" , help = \"SHA1 of additional file\" ), image_file : str = typer . Option ( \"\" , \"--image-file\" , \"-i\" , help = \"URL to image file. Expected image to be compressed inside a tarball\" , ), image_hash : str = typer . Option ( \"\" , \"--image-hash\" , help = \"SHA1 of image file\" ), ): \"\"\"Submits a new cube to the platform\"\"\" mlcube_info = { \"name\" : name , \"mlcube_file\" : mlcube_file , \"mlcube_hash\" : mlcube_hash , \"params_file\" : params_file , \"parameters_hash\" : parameters_hash , \"image_tarball_url\" : image_file , \"image_tarball_hash\" : image_hash , \"additional_files_tarball_url\" : additional_file , \"additional_files_tarball_hash\" : additional_hash , } SubmitCube . run ( mlcube_info ) cleanup () config . ui . print ( \"\u2705 Done!\" )","title":"mlcube"},{"location":"reference/commands/mlcube/mlcube/#commands.mlcube.mlcube.associate","text":"Associates an MLCube to a benchmark Source code in commands/mlcube/mlcube.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 @app . command ( \"associate\" ) @clean_except def associate ( benchmark_uid : int = typer . Option ( ... , \"--benchmark\" , \"-b\" , help = \"Benchmark UID\" ), model_uid : int = typer . Option ( ... , \"--model_uid\" , \"-m\" , help = \"Model UID\" ), approval : bool = typer . Option ( False , \"-y\" , help = \"Skip approval step\" ), force_test : bool = typer . Option ( False , \"--force-test\" , help = \"Execute the test even if results already exist\" , ), ): \"\"\"Associates an MLCube to a benchmark\"\"\" AssociateCube . run ( model_uid , benchmark_uid , approved = approval , force_test = force_test ) config . ui . print ( \"\u2705 Done!\" )","title":"associate()"},{"location":"reference/commands/mlcube/mlcube/#commands.mlcube.mlcube.list","text":"List mlcubes registered by the user by default. Use \"all\" to display all mlcubes in the platform Source code in commands/mlcube/mlcube.py 13 14 15 16 17 18 19 @app . command ( \"ls\" ) @clean_except def list ( all : bool = typer . Option ( False , help = \"Display all mlcubes\" )): \"\"\"List mlcubes registered by the user by default. Use \"all\" to display all mlcubes in the platform \"\"\" CubesList . run ( all )","title":"list()"},{"location":"reference/commands/mlcube/mlcube/#commands.mlcube.mlcube.submit","text":"Submits a new cube to the platform Source code in commands/mlcube/mlcube.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 @app . command ( \"submit\" ) @clean_except def submit ( name : str = typer . Option ( ... , \"--name\" , \"-n\" , help = \"Name of the mlcube\" ), mlcube_file : str = typer . Option ( ... , \"--mlcube-file\" , \"-m\" , help = \"URL to mlcube file\" ), mlcube_hash : str = typer . Option ( \"\" , \"--mlcube-hash\" , help = \"SHA1 of mlcube file\" ), params_file : str = typer . Option ( \"\" , \"--parameters-file\" , \"-p\" , help = \"URL to parameters file\" ), parameters_hash : str = typer . Option ( \"\" , \"--parameters-hash\" , help = \"SHA1 of parameters file\" ), additional_file : str = typer . Option ( \"\" , \"--additional-file\" , \"-a\" , help = \"URL to additional files tarball\" ), additional_hash : str = typer . Option ( \"\" , \"--additional-hash\" , help = \"SHA1 of additional file\" ), image_file : str = typer . Option ( \"\" , \"--image-file\" , \"-i\" , help = \"URL to image file. Expected image to be compressed inside a tarball\" , ), image_hash : str = typer . Option ( \"\" , \"--image-hash\" , help = \"SHA1 of image file\" ), ): \"\"\"Submits a new cube to the platform\"\"\" mlcube_info = { \"name\" : name , \"mlcube_file\" : mlcube_file , \"mlcube_hash\" : mlcube_hash , \"params_file\" : params_file , \"parameters_hash\" : parameters_hash , \"image_tarball_url\" : image_file , \"image_tarball_hash\" : image_hash , \"additional_files_tarball_url\" : additional_file , \"additional_files_tarball_hash\" : additional_hash , } SubmitCube . run ( mlcube_info ) cleanup () config . ui . print ( \"\u2705 Done!\" )","title":"submit()"},{"location":"reference/commands/mlcube/submit/","text":"SubmitCube Source code in commands/mlcube/submit.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 class SubmitCube : @classmethod def run ( cls , submit_info : dict ): \"\"\"Submits a new cube to the medperf platform Args: submit_info (dict): Dictionary containing the cube information. expected keys: name, mlcube_file, mlcube_hash params_file, params_hash additional_files_tarball_url, additional_files_tarball_hash, image_tarball_url, image_tarball_hash, \"\"\" ui = config . ui submission = cls ( submit_info ) if not submission . is_valid (): pretty_error ( \"MLCube submission is invalid\" ) with ui . interactive (): ui . text = \"Validating MLCube can be downloaded\" submission . download () ui . text = \"Submitting MLCube to MedPerf\" updated_cube_dict = submission . upload () submission . to_permanent_path ( updated_cube_dict [ \"id\" ]) submission . write ( updated_cube_dict ) def __init__ ( self , submit_info : dict ): self . comms = config . comms self . ui = config . ui self . name = submit_info [ \"name\" ] self . mlcube_file = submit_info [ \"mlcube_file\" ] self . mlcube_hash = submit_info [ \"mlcube_hash\" ] self . params_file = submit_info [ \"params_file\" ] self . parameters_hash = submit_info [ \"parameters_hash\" ] self . additional_file = submit_info [ \"additional_files_tarball_url\" ] self . additional_hash = submit_info [ \"additional_files_tarball_hash\" ] self . image_file = submit_info [ \"image_tarball_url\" ] self . image_tarball_hash = submit_info [ \"image_tarball_hash\" ] def is_valid ( self ): name_valid_length = 0 < len ( self . name ) < 20 mlcube_file_is_valid = validators . url ( self . mlcube_file ) and self . mlcube_file . endswith ( \".yaml\" ) params_file_is_valid = self . params_file == \"\" or ( validators . url ( self . params_file ) and self . params_file . endswith ( \".yaml\" ) ) add_file_is_valid = self . additional_file == \"\" or validators . url ( self . additional_file ) image_file_is_valid = self . image_file == \"\" or validators . url ( self . image_file ) valid = True if not name_valid_length : valid = False self . name = None self . ui . print_error ( \"Name is invalid\" ) if not mlcube_file_is_valid : valid = False self . mlcube_file = None self . ui . print_error ( \"MLCube file is invalid\" ) if not params_file_is_valid : valid = False self . params_file = None self . ui . print_error ( \"Parameters file is invalid\" ) if not add_file_is_valid : valid = False self . additional_file = None self . ui . print_error ( \"Additional file is invalid\" ) if not image_file_is_valid : valid = False self . image_file = None self . ui . print_error ( \"Image file is invalid\" ) return valid def download ( self ): cube = Cube ( self . todict ()) cube . download () self . additional_hash = cube . additional_hash self . image_tarball_hash = cube . image_tarball_hash self . mlcube_hash = cube . mlcube_hash self . parameters_hash = cube . parameters_hash if not cube . is_valid (): pretty_error ( \"MLCube hash check failed. Submission aborted.\" ) def todict ( self ): dict = { \"name\" : self . name , \"git_mlcube_url\" : self . mlcube_file , \"mlcube_hash\" : self . mlcube_hash , \"git_parameters_url\" : self . params_file , \"parameters_hash\" : self . parameters_hash , \"image_tarball_url\" : self . image_file , \"image_tarball_hash\" : self . image_tarball_hash , \"additional_files_tarball_url\" : self . additional_file , \"additional_files_tarball_hash\" : self . additional_hash , \"state\" : \"OPERATION\" , \"is_valid\" : True , \"id\" : config . cube_submission_id , \"owner\" : None , \"metadata\" : {}, \"user_metadata\" : {}, \"created_at\" : None , \"modified_at\" : None , } return dict def upload ( self ): body = self . todict () updated_body = Cube ( body ) . upload () return updated_body def to_permanent_path ( self , cube_uid ): \"\"\"Renames the temporary cube submission to a permanent one using the uid of the registered cube \"\"\" cubes_storage = storage_path ( config . cubes_storage ) old_cube_loc = os . path . join ( cubes_storage , config . cube_submission_id ) new_cube_loc = os . path . join ( cubes_storage , str ( cube_uid )) if os . path . exists ( new_cube_loc ): shutil . rmtree ( new_cube_loc ) os . rename ( old_cube_loc , new_cube_loc ) def write ( self , updated_cube_dict ): cube = Cube ( updated_cube_dict ) cube . write () run ( submit_info ) classmethod Submits a new cube to the medperf platform Parameters: Name Type Description Default submit_info dict Dictionary containing the cube information. expected keys: name, mlcube_file, mlcube_hash params_file, params_hash additional_files_tarball_url, additional_files_tarball_hash, image_tarball_url, image_tarball_hash, required Source code in commands/mlcube/submit.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 @classmethod def run ( cls , submit_info : dict ): \"\"\"Submits a new cube to the medperf platform Args: submit_info (dict): Dictionary containing the cube information. expected keys: name, mlcube_file, mlcube_hash params_file, params_hash additional_files_tarball_url, additional_files_tarball_hash, image_tarball_url, image_tarball_hash, \"\"\" ui = config . ui submission = cls ( submit_info ) if not submission . is_valid (): pretty_error ( \"MLCube submission is invalid\" ) with ui . interactive (): ui . text = \"Validating MLCube can be downloaded\" submission . download () ui . text = \"Submitting MLCube to MedPerf\" updated_cube_dict = submission . upload () submission . to_permanent_path ( updated_cube_dict [ \"id\" ]) submission . write ( updated_cube_dict ) to_permanent_path ( cube_uid ) Renames the temporary cube submission to a permanent one using the uid of the registered cube Source code in commands/mlcube/submit.py 128 129 130 131 132 133 134 135 136 137 def to_permanent_path ( self , cube_uid ): \"\"\"Renames the temporary cube submission to a permanent one using the uid of the registered cube \"\"\" cubes_storage = storage_path ( config . cubes_storage ) old_cube_loc = os . path . join ( cubes_storage , config . cube_submission_id ) new_cube_loc = os . path . join ( cubes_storage , str ( cube_uid )) if os . path . exists ( new_cube_loc ): shutil . rmtree ( new_cube_loc ) os . rename ( old_cube_loc , new_cube_loc )","title":"submit"},{"location":"reference/commands/mlcube/submit/#commands.mlcube.submit.SubmitCube","text":"Source code in commands/mlcube/submit.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 class SubmitCube : @classmethod def run ( cls , submit_info : dict ): \"\"\"Submits a new cube to the medperf platform Args: submit_info (dict): Dictionary containing the cube information. expected keys: name, mlcube_file, mlcube_hash params_file, params_hash additional_files_tarball_url, additional_files_tarball_hash, image_tarball_url, image_tarball_hash, \"\"\" ui = config . ui submission = cls ( submit_info ) if not submission . is_valid (): pretty_error ( \"MLCube submission is invalid\" ) with ui . interactive (): ui . text = \"Validating MLCube can be downloaded\" submission . download () ui . text = \"Submitting MLCube to MedPerf\" updated_cube_dict = submission . upload () submission . to_permanent_path ( updated_cube_dict [ \"id\" ]) submission . write ( updated_cube_dict ) def __init__ ( self , submit_info : dict ): self . comms = config . comms self . ui = config . ui self . name = submit_info [ \"name\" ] self . mlcube_file = submit_info [ \"mlcube_file\" ] self . mlcube_hash = submit_info [ \"mlcube_hash\" ] self . params_file = submit_info [ \"params_file\" ] self . parameters_hash = submit_info [ \"parameters_hash\" ] self . additional_file = submit_info [ \"additional_files_tarball_url\" ] self . additional_hash = submit_info [ \"additional_files_tarball_hash\" ] self . image_file = submit_info [ \"image_tarball_url\" ] self . image_tarball_hash = submit_info [ \"image_tarball_hash\" ] def is_valid ( self ): name_valid_length = 0 < len ( self . name ) < 20 mlcube_file_is_valid = validators . url ( self . mlcube_file ) and self . mlcube_file . endswith ( \".yaml\" ) params_file_is_valid = self . params_file == \"\" or ( validators . url ( self . params_file ) and self . params_file . endswith ( \".yaml\" ) ) add_file_is_valid = self . additional_file == \"\" or validators . url ( self . additional_file ) image_file_is_valid = self . image_file == \"\" or validators . url ( self . image_file ) valid = True if not name_valid_length : valid = False self . name = None self . ui . print_error ( \"Name is invalid\" ) if not mlcube_file_is_valid : valid = False self . mlcube_file = None self . ui . print_error ( \"MLCube file is invalid\" ) if not params_file_is_valid : valid = False self . params_file = None self . ui . print_error ( \"Parameters file is invalid\" ) if not add_file_is_valid : valid = False self . additional_file = None self . ui . print_error ( \"Additional file is invalid\" ) if not image_file_is_valid : valid = False self . image_file = None self . ui . print_error ( \"Image file is invalid\" ) return valid def download ( self ): cube = Cube ( self . todict ()) cube . download () self . additional_hash = cube . additional_hash self . image_tarball_hash = cube . image_tarball_hash self . mlcube_hash = cube . mlcube_hash self . parameters_hash = cube . parameters_hash if not cube . is_valid (): pretty_error ( \"MLCube hash check failed. Submission aborted.\" ) def todict ( self ): dict = { \"name\" : self . name , \"git_mlcube_url\" : self . mlcube_file , \"mlcube_hash\" : self . mlcube_hash , \"git_parameters_url\" : self . params_file , \"parameters_hash\" : self . parameters_hash , \"image_tarball_url\" : self . image_file , \"image_tarball_hash\" : self . image_tarball_hash , \"additional_files_tarball_url\" : self . additional_file , \"additional_files_tarball_hash\" : self . additional_hash , \"state\" : \"OPERATION\" , \"is_valid\" : True , \"id\" : config . cube_submission_id , \"owner\" : None , \"metadata\" : {}, \"user_metadata\" : {}, \"created_at\" : None , \"modified_at\" : None , } return dict def upload ( self ): body = self . todict () updated_body = Cube ( body ) . upload () return updated_body def to_permanent_path ( self , cube_uid ): \"\"\"Renames the temporary cube submission to a permanent one using the uid of the registered cube \"\"\" cubes_storage = storage_path ( config . cubes_storage ) old_cube_loc = os . path . join ( cubes_storage , config . cube_submission_id ) new_cube_loc = os . path . join ( cubes_storage , str ( cube_uid )) if os . path . exists ( new_cube_loc ): shutil . rmtree ( new_cube_loc ) os . rename ( old_cube_loc , new_cube_loc ) def write ( self , updated_cube_dict ): cube = Cube ( updated_cube_dict ) cube . write ()","title":"SubmitCube"},{"location":"reference/commands/mlcube/submit/#commands.mlcube.submit.SubmitCube.run","text":"Submits a new cube to the medperf platform Parameters: Name Type Description Default submit_info dict Dictionary containing the cube information. expected keys: name, mlcube_file, mlcube_hash params_file, params_hash additional_files_tarball_url, additional_files_tarball_hash, image_tarball_url, image_tarball_hash, required Source code in commands/mlcube/submit.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 @classmethod def run ( cls , submit_info : dict ): \"\"\"Submits a new cube to the medperf platform Args: submit_info (dict): Dictionary containing the cube information. expected keys: name, mlcube_file, mlcube_hash params_file, params_hash additional_files_tarball_url, additional_files_tarball_hash, image_tarball_url, image_tarball_hash, \"\"\" ui = config . ui submission = cls ( submit_info ) if not submission . is_valid (): pretty_error ( \"MLCube submission is invalid\" ) with ui . interactive (): ui . text = \"Validating MLCube can be downloaded\" submission . download () ui . text = \"Submitting MLCube to MedPerf\" updated_cube_dict = submission . upload () submission . to_permanent_path ( updated_cube_dict [ \"id\" ]) submission . write ( updated_cube_dict )","title":"run()"},{"location":"reference/commands/mlcube/submit/#commands.mlcube.submit.SubmitCube.to_permanent_path","text":"Renames the temporary cube submission to a permanent one using the uid of the registered cube Source code in commands/mlcube/submit.py 128 129 130 131 132 133 134 135 136 137 def to_permanent_path ( self , cube_uid ): \"\"\"Renames the temporary cube submission to a permanent one using the uid of the registered cube \"\"\" cubes_storage = storage_path ( config . cubes_storage ) old_cube_loc = os . path . join ( cubes_storage , config . cube_submission_id ) new_cube_loc = os . path . join ( cubes_storage , str ( cube_uid )) if os . path . exists ( new_cube_loc ): shutil . rmtree ( new_cube_loc ) os . rename ( old_cube_loc , new_cube_loc )","title":"to_permanent_path()"},{"location":"reference/commands/result/create/","text":"BenchmarkExecution Source code in commands/result/create.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 class BenchmarkExecution : @classmethod def run ( cls , benchmark_uid : int , data_uid : str , model_uid : int , run_test = False , ignore_errors = False , ): \"\"\"Benchmark execution flow. Args: benchmark_uid (int): UID of the desired benchmark data_uid (str): Registered Dataset UID model_uid (int): UID of model to execute \"\"\" execution = cls ( benchmark_uid , data_uid , model_uid , run_test , ignore_errors ) execution . prepare () execution . validate () with execution . ui . interactive (): execution . get_cubes () execution . run_cubes () execution . write () execution . remove_temp_results () def __init__ ( self , benchmark_uid : int , data_uid : int , model_uid : int , run_test = False , ignore_errors = False , ): self . benchmark_uid = benchmark_uid self . data_uid = data_uid self . model_uid = model_uid self . comms = config . comms self . ui = config . ui self . evaluator = None self . model_cube = None self . run_test = run_test self . ignore_errors = ignore_errors self . metadata = { \"partial\" : False } def prepare ( self ): init_storage () # If not running the test, redownload the benchmark self . benchmark = Benchmark . get ( self . benchmark_uid ) self . ui . print ( f \"Benchmark Execution: { self . benchmark . name } \" ) self . dataset = Dataset . from_generated_uid ( self . data_uid ) if not self . run_test : self . out_path = results_path ( self . benchmark_uid , self . model_uid , self . dataset . uid ) else : self . out_path = results_path ( self . benchmark_uid , self . model_uid , self . dataset . generated_uid ) def validate ( self ): dset_prep_cube = str ( self . dataset . preparation_cube_uid ) bmark_prep_cube = str ( self . benchmark . data_preparation ) if self . dataset . uid is None and not self . run_test : msg = \"The provided dataset is not registered.\" pretty_error ( msg ) if dset_prep_cube != bmark_prep_cube : msg = \"The provided dataset is not compatible with the specified benchmark.\" pretty_error ( msg ) in_assoc_cubes = self . model_uid in self . benchmark . models if not self . run_test and not in_assoc_cubes : pretty_error ( \"The provided model is not part of the specified benchmark.\" ) def get_cubes ( self ): evaluator_uid = self . benchmark . evaluator self . evaluator = self . __get_cube ( evaluator_uid , \"Evaluator\" ) self . model_cube = self . __get_cube ( self . model_uid , \"Model\" ) def __get_cube ( self , uid : int , name : str ) -> Cube : self . ui . text = f \"Retrieving { name } cube\" cube = Cube . get ( uid ) self . ui . print ( f \"> { name } cube download complete\" ) check_cube_validity ( cube ) return cube def run_cubes ( self ): infer_timeout = config . infer_timeout evaluate_timeout = config . evaluate_timeout self . ui . text = \"Running model inference on dataset\" model_uid = str ( self . model_cube . uid ) data_uid = str ( self . dataset . generated_uid ) preds_path = os . path . join ( config . predictions_storage , model_uid , data_uid ) preds_path = storage_path ( preds_path ) data_path = self . dataset . data_path out_path = os . path . join ( self . out_path , config . results_filename ) labels_path = self . dataset . labels_path try : self . model_cube . run ( task = \"infer\" , timeout = infer_timeout , data_path = data_path , output_path = preds_path , string_params = { \"Ptasks.infer.parameters.input.data_path.opts\" : \"ro\" }, ) self . ui . print ( \"> Model execution complete\" ) except RuntimeError as e : if not self . ignore_errors : logging . error ( f \"Model MLCube Execution failed: { e } \" ) cleanup ([ preds_path ]) pretty_error ( \"Benchmark execution failed\" ) else : self . metadata [ \"partial\" ] = True logging . warning ( f \"Model MLCube Execution failed: { e } \" ) try : self . ui . text = \"Evaluating results\" self . evaluator . run ( task = \"evaluate\" , timeout = evaluate_timeout , predictions = preds_path , labels = labels_path , output_path = out_path , string_params = { \"Ptasks.evaluate.parameters.input.predictions.opts\" : \"ro\" , \"Ptasks.evaluate.parameters.input.labels.opts\" : \"ro\" , }, ) except RuntimeError as e : if not self . ignore_errors : logging . error ( f \"Metrics MLCube Execution failed: { e } \" ) cleanup ([ preds_path , out_path ]) pretty_error ( \"Benchmark execution failed\" ) else : self . metadata [ \"partial\" ] = True logging . warning ( f \"Metrics MLCube Execution failed: { e } \" ) def todict ( self ): data_uid = self . dataset . generated_uid if self . run_test else self . dataset . uid return { \"id\" : None , \"name\" : f \" { self . benchmark_uid } _ { self . model_uid } _ { data_uid } \" , \"owner\" : None , \"benchmark\" : self . benchmark_uid , \"model\" : self . model_uid , \"dataset\" : data_uid , \"results\" : self . get_temp_results (), \"metadata\" : self . metadata , \"approval_status\" : Status . PENDING . value , \"approved_at\" : None , \"created_at\" : None , \"modified_at\" : None , } def get_temp_results ( self ): path = os . path . join ( self . out_path , config . results_filename ) with open ( path , \"r\" ) as f : results = yaml . safe_load ( f ) return results def remove_temp_results ( self ): path = os . path . join ( self . out_path , config . results_filename ) os . remove ( path ) def write ( self ): results_info = self . todict () result = Result ( results_info ) result . write () run ( benchmark_uid , data_uid , model_uid , run_test = False , ignore_errors = False ) classmethod Benchmark execution flow. Parameters: Name Type Description Default benchmark_uid int UID of the desired benchmark required data_uid str Registered Dataset UID required model_uid int UID of model to execute required Source code in commands/result/create.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 @classmethod def run ( cls , benchmark_uid : int , data_uid : str , model_uid : int , run_test = False , ignore_errors = False , ): \"\"\"Benchmark execution flow. Args: benchmark_uid (int): UID of the desired benchmark data_uid (str): Registered Dataset UID model_uid (int): UID of model to execute \"\"\" execution = cls ( benchmark_uid , data_uid , model_uid , run_test , ignore_errors ) execution . prepare () execution . validate () with execution . ui . interactive (): execution . get_cubes () execution . run_cubes () execution . write () execution . remove_temp_results ()","title":"create"},{"location":"reference/commands/result/create/#commands.result.create.BenchmarkExecution","text":"Source code in commands/result/create.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 class BenchmarkExecution : @classmethod def run ( cls , benchmark_uid : int , data_uid : str , model_uid : int , run_test = False , ignore_errors = False , ): \"\"\"Benchmark execution flow. Args: benchmark_uid (int): UID of the desired benchmark data_uid (str): Registered Dataset UID model_uid (int): UID of model to execute \"\"\" execution = cls ( benchmark_uid , data_uid , model_uid , run_test , ignore_errors ) execution . prepare () execution . validate () with execution . ui . interactive (): execution . get_cubes () execution . run_cubes () execution . write () execution . remove_temp_results () def __init__ ( self , benchmark_uid : int , data_uid : int , model_uid : int , run_test = False , ignore_errors = False , ): self . benchmark_uid = benchmark_uid self . data_uid = data_uid self . model_uid = model_uid self . comms = config . comms self . ui = config . ui self . evaluator = None self . model_cube = None self . run_test = run_test self . ignore_errors = ignore_errors self . metadata = { \"partial\" : False } def prepare ( self ): init_storage () # If not running the test, redownload the benchmark self . benchmark = Benchmark . get ( self . benchmark_uid ) self . ui . print ( f \"Benchmark Execution: { self . benchmark . name } \" ) self . dataset = Dataset . from_generated_uid ( self . data_uid ) if not self . run_test : self . out_path = results_path ( self . benchmark_uid , self . model_uid , self . dataset . uid ) else : self . out_path = results_path ( self . benchmark_uid , self . model_uid , self . dataset . generated_uid ) def validate ( self ): dset_prep_cube = str ( self . dataset . preparation_cube_uid ) bmark_prep_cube = str ( self . benchmark . data_preparation ) if self . dataset . uid is None and not self . run_test : msg = \"The provided dataset is not registered.\" pretty_error ( msg ) if dset_prep_cube != bmark_prep_cube : msg = \"The provided dataset is not compatible with the specified benchmark.\" pretty_error ( msg ) in_assoc_cubes = self . model_uid in self . benchmark . models if not self . run_test and not in_assoc_cubes : pretty_error ( \"The provided model is not part of the specified benchmark.\" ) def get_cubes ( self ): evaluator_uid = self . benchmark . evaluator self . evaluator = self . __get_cube ( evaluator_uid , \"Evaluator\" ) self . model_cube = self . __get_cube ( self . model_uid , \"Model\" ) def __get_cube ( self , uid : int , name : str ) -> Cube : self . ui . text = f \"Retrieving { name } cube\" cube = Cube . get ( uid ) self . ui . print ( f \"> { name } cube download complete\" ) check_cube_validity ( cube ) return cube def run_cubes ( self ): infer_timeout = config . infer_timeout evaluate_timeout = config . evaluate_timeout self . ui . text = \"Running model inference on dataset\" model_uid = str ( self . model_cube . uid ) data_uid = str ( self . dataset . generated_uid ) preds_path = os . path . join ( config . predictions_storage , model_uid , data_uid ) preds_path = storage_path ( preds_path ) data_path = self . dataset . data_path out_path = os . path . join ( self . out_path , config . results_filename ) labels_path = self . dataset . labels_path try : self . model_cube . run ( task = \"infer\" , timeout = infer_timeout , data_path = data_path , output_path = preds_path , string_params = { \"Ptasks.infer.parameters.input.data_path.opts\" : \"ro\" }, ) self . ui . print ( \"> Model execution complete\" ) except RuntimeError as e : if not self . ignore_errors : logging . error ( f \"Model MLCube Execution failed: { e } \" ) cleanup ([ preds_path ]) pretty_error ( \"Benchmark execution failed\" ) else : self . metadata [ \"partial\" ] = True logging . warning ( f \"Model MLCube Execution failed: { e } \" ) try : self . ui . text = \"Evaluating results\" self . evaluator . run ( task = \"evaluate\" , timeout = evaluate_timeout , predictions = preds_path , labels = labels_path , output_path = out_path , string_params = { \"Ptasks.evaluate.parameters.input.predictions.opts\" : \"ro\" , \"Ptasks.evaluate.parameters.input.labels.opts\" : \"ro\" , }, ) except RuntimeError as e : if not self . ignore_errors : logging . error ( f \"Metrics MLCube Execution failed: { e } \" ) cleanup ([ preds_path , out_path ]) pretty_error ( \"Benchmark execution failed\" ) else : self . metadata [ \"partial\" ] = True logging . warning ( f \"Metrics MLCube Execution failed: { e } \" ) def todict ( self ): data_uid = self . dataset . generated_uid if self . run_test else self . dataset . uid return { \"id\" : None , \"name\" : f \" { self . benchmark_uid } _ { self . model_uid } _ { data_uid } \" , \"owner\" : None , \"benchmark\" : self . benchmark_uid , \"model\" : self . model_uid , \"dataset\" : data_uid , \"results\" : self . get_temp_results (), \"metadata\" : self . metadata , \"approval_status\" : Status . PENDING . value , \"approved_at\" : None , \"created_at\" : None , \"modified_at\" : None , } def get_temp_results ( self ): path = os . path . join ( self . out_path , config . results_filename ) with open ( path , \"r\" ) as f : results = yaml . safe_load ( f ) return results def remove_temp_results ( self ): path = os . path . join ( self . out_path , config . results_filename ) os . remove ( path ) def write ( self ): results_info = self . todict () result = Result ( results_info ) result . write ()","title":"BenchmarkExecution"},{"location":"reference/commands/result/create/#commands.result.create.BenchmarkExecution.run","text":"Benchmark execution flow. Parameters: Name Type Description Default benchmark_uid int UID of the desired benchmark required data_uid str Registered Dataset UID required model_uid int UID of model to execute required Source code in commands/result/create.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 @classmethod def run ( cls , benchmark_uid : int , data_uid : str , model_uid : int , run_test = False , ignore_errors = False , ): \"\"\"Benchmark execution flow. Args: benchmark_uid (int): UID of the desired benchmark data_uid (str): Registered Dataset UID model_uid (int): UID of model to execute \"\"\" execution = cls ( benchmark_uid , data_uid , model_uid , run_test , ignore_errors ) execution . prepare () execution . validate () with execution . ui . interactive (): execution . get_cubes () execution . run_cubes () execution . write () execution . remove_temp_results ()","title":"run()"},{"location":"reference/commands/result/list/","text":"ResultsList Source code in commands/result/list.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 class ResultsList : @staticmethod def run (): \"\"\"Lists all local datasets \"\"\" comms = config . comms ui = config . ui results = Result . all () headers = [ \"Benchmark UID\" , \"Model UID\" , \"Data UID\" , \"Submitted\" , \"Local\" ] # Get local results data results_data = [ [ result . benchmark_uid , result . model_uid , result . dataset_uid , result . uid is not None , True , ] for result in results ] local_uids = [ result . uid for result in results ] # Get remote results data remote_results = comms . get_user_results () remote_results_data = [ [ result [ \"benchmark\" ], result [ \"model\" ], result [ \"dataset\" ], True , False ] for result in remote_results if result [ \"id\" ] not in local_uids ] results_data += remote_results_data tab = tabulate ( results_data , headers = headers ) ui . print ( tab ) run () staticmethod Lists all local datasets Source code in commands/result/list.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 @staticmethod def run (): \"\"\"Lists all local datasets \"\"\" comms = config . comms ui = config . ui results = Result . all () headers = [ \"Benchmark UID\" , \"Model UID\" , \"Data UID\" , \"Submitted\" , \"Local\" ] # Get local results data results_data = [ [ result . benchmark_uid , result . model_uid , result . dataset_uid , result . uid is not None , True , ] for result in results ] local_uids = [ result . uid for result in results ] # Get remote results data remote_results = comms . get_user_results () remote_results_data = [ [ result [ \"benchmark\" ], result [ \"model\" ], result [ \"dataset\" ], True , False ] for result in remote_results if result [ \"id\" ] not in local_uids ] results_data += remote_results_data tab = tabulate ( results_data , headers = headers ) ui . print ( tab )","title":"list"},{"location":"reference/commands/result/list/#commands.result.list.ResultsList","text":"Source code in commands/result/list.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 class ResultsList : @staticmethod def run (): \"\"\"Lists all local datasets \"\"\" comms = config . comms ui = config . ui results = Result . all () headers = [ \"Benchmark UID\" , \"Model UID\" , \"Data UID\" , \"Submitted\" , \"Local\" ] # Get local results data results_data = [ [ result . benchmark_uid , result . model_uid , result . dataset_uid , result . uid is not None , True , ] for result in results ] local_uids = [ result . uid for result in results ] # Get remote results data remote_results = comms . get_user_results () remote_results_data = [ [ result [ \"benchmark\" ], result [ \"model\" ], result [ \"dataset\" ], True , False ] for result in remote_results if result [ \"id\" ] not in local_uids ] results_data += remote_results_data tab = tabulate ( results_data , headers = headers ) ui . print ( tab )","title":"ResultsList"},{"location":"reference/commands/result/list/#commands.result.list.ResultsList.run","text":"Lists all local datasets Source code in commands/result/list.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 @staticmethod def run (): \"\"\"Lists all local datasets \"\"\" comms = config . comms ui = config . ui results = Result . all () headers = [ \"Benchmark UID\" , \"Model UID\" , \"Data UID\" , \"Submitted\" , \"Local\" ] # Get local results data results_data = [ [ result . benchmark_uid , result . model_uid , result . dataset_uid , result . uid is not None , True , ] for result in results ] local_uids = [ result . uid for result in results ] # Get remote results data remote_results = comms . get_user_results () remote_results_data = [ [ result [ \"benchmark\" ], result [ \"model\" ], result [ \"dataset\" ], True , False ] for result in remote_results if result [ \"id\" ] not in local_uids ] results_data += remote_results_data tab = tabulate ( results_data , headers = headers ) ui . print ( tab )","title":"run()"},{"location":"reference/commands/result/result/","text":"create ( benchmark_uid = typer . Option ( Ellipsis , '--benchmark' , '-b' , help = 'UID of the desired benchmark' ), data_uid = typer . Option ( Ellipsis , '--data_uid' , '-d' , help = 'Registered Dataset UID' ), model_uid = typer . Option ( Ellipsis , '--model_uid' , '-m' , help = 'UID of model to execute' ), ignore_errors = typer . Option ( False , '--ignore-errors' , help = 'Ignore failing cubes, allowing for submitting partial results' )) Runs the benchmark execution step for a given benchmark, prepared dataset and model Source code in commands/result/result.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 @app . command ( \"create\" ) @clean_except def create ( benchmark_uid : int = typer . Option ( ... , \"--benchmark\" , \"-b\" , help = \"UID of the desired benchmark\" ), data_uid : str = typer . Option ( ... , \"--data_uid\" , \"-d\" , help = \"Registered Dataset UID\" ), model_uid : int = typer . Option ( ... , \"--model_uid\" , \"-m\" , help = \"UID of model to execute\" ), ignore_errors : bool = typer . Option ( False , \"--ignore-errors\" , help = \"Ignore failing cubes, allowing for submitting partial results\" , ), ): \"\"\"Runs the benchmark execution step for a given benchmark, prepared dataset and model \"\"\" BenchmarkExecution . run ( benchmark_uid , data_uid , model_uid , ignore_errors = ignore_errors ) config . ui . print ( \"\u2705 Done!\" ) list () List results stored locally and remotely from the user Source code in commands/result/result.py 57 58 59 60 61 @app . command ( \"ls\" ) @clean_except def list (): \"\"\"List results stored locally and remotely from the user\"\"\" ResultsList . run () submit ( benchmark_uid = typer . Option ( Ellipsis , '--benchmark' , '-b' , help = 'UID of the executed benchmark' ), data_uid = typer . Option ( Ellipsis , '--data_uid' , '-d' , help = 'UID of the dataset used for results' ), model_uid = typer . Option ( Ellipsis , '--model_uid' , '-m' , help = 'UID of the executed model' ), approval = typer . Option ( False , '-y' , help = 'Skip approval step' )) Submits already obtained results to the server Source code in commands/result/result.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 @app . command ( \"submit\" ) @clean_except def submit ( benchmark_uid : int = typer . Option ( ... , \"--benchmark\" , \"-b\" , help = \"UID of the executed benchmark\" ), data_uid : str = typer . Option ( ... , \"--data_uid\" , \"-d\" , help = \"UID of the dataset used for results\" ), model_uid : int = typer . Option ( ... , \"--model_uid\" , \"-m\" , help = \"UID of the executed model\" ), approval : bool = typer . Option ( False , \"-y\" , help = \"Skip approval step\" ), ): \"\"\"Submits already obtained results to the server\"\"\" ResultSubmission . run ( benchmark_uid , data_uid , model_uid , approved = approval ) config . ui . print ( \"\u2705 Done!\" )","title":"result"},{"location":"reference/commands/result/result/#commands.result.result.create","text":"Runs the benchmark execution step for a given benchmark, prepared dataset and model Source code in commands/result/result.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 @app . command ( \"create\" ) @clean_except def create ( benchmark_uid : int = typer . Option ( ... , \"--benchmark\" , \"-b\" , help = \"UID of the desired benchmark\" ), data_uid : str = typer . Option ( ... , \"--data_uid\" , \"-d\" , help = \"Registered Dataset UID\" ), model_uid : int = typer . Option ( ... , \"--model_uid\" , \"-m\" , help = \"UID of model to execute\" ), ignore_errors : bool = typer . Option ( False , \"--ignore-errors\" , help = \"Ignore failing cubes, allowing for submitting partial results\" , ), ): \"\"\"Runs the benchmark execution step for a given benchmark, prepared dataset and model \"\"\" BenchmarkExecution . run ( benchmark_uid , data_uid , model_uid , ignore_errors = ignore_errors ) config . ui . print ( \"\u2705 Done!\" )","title":"create()"},{"location":"reference/commands/result/result/#commands.result.result.list","text":"List results stored locally and remotely from the user Source code in commands/result/result.py 57 58 59 60 61 @app . command ( \"ls\" ) @clean_except def list (): \"\"\"List results stored locally and remotely from the user\"\"\" ResultsList . run ()","title":"list()"},{"location":"reference/commands/result/result/#commands.result.result.submit","text":"Submits already obtained results to the server Source code in commands/result/result.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 @app . command ( \"submit\" ) @clean_except def submit ( benchmark_uid : int = typer . Option ( ... , \"--benchmark\" , \"-b\" , help = \"UID of the executed benchmark\" ), data_uid : str = typer . Option ( ... , \"--data_uid\" , \"-d\" , help = \"UID of the dataset used for results\" ), model_uid : int = typer . Option ( ... , \"--model_uid\" , \"-m\" , help = \"UID of the executed model\" ), approval : bool = typer . Option ( False , \"-y\" , help = \"Skip approval step\" ), ): \"\"\"Submits already obtained results to the server\"\"\" ResultSubmission . run ( benchmark_uid , data_uid , model_uid , approved = approval ) config . ui . print ( \"\u2705 Done!\" )","title":"submit()"},{"location":"reference/commands/result/submit/","text":"","title":"submit"},{"location":"reference/comms/factory/","text":"","title":"factory"},{"location":"reference/comms/interface/","text":"Comms Bases: ABC Source code in comms/interface.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 class Comms ( ABC ): @abstractmethod def __init__ ( self , source : str , ui : UI , token : str = None ): \"\"\"Create an instance of a communication object. Args: source (str): location of the communication source. Where messages are going to be sent. ui (UI): Implementation of the UI interface. token (str, Optional): authentication token to be used throughout communication. Defaults to None. \"\"\" @abstractmethod def login ( self , ui : UI ): \"\"\"Authenticate the comms instance for further interactions Args: ui (UI): instance of an implementation of the UI interface. \"\"\" @abstractmethod def change_password ( self , pwd : str , ui : UI ) -> bool : \"\"\"Sets a new password for the current user. Args: pwd (str): New password to be set ui (UI): Instance of an implementation Returns: bool: Whether changing the password was successful or not \"\"\" @abstractmethod def authenticate ( self ): \"\"\"Retrieve a token stored locally for authentication \"\"\" @abstractmethod def benchmark_association ( self , benchmark_uid : int ) -> Role : \"\"\"Retrieves the benchmark association Args: benchmark_uid (int): UID of the benchmark Returns: Role: the association type between current user and benchmark \"\"\" @abstractmethod def authorized_by_role ( self , benchmark_uid : int , role : str ) -> bool : \"\"\"Indicates wether the current user is authorized to access a benchmark based on desired role Args: benchmark_uid (int): UID of the benchmark role (str): Desired role to check for authorization Returns: bool: Wether the user has the specified role for that benchmark \"\"\" @abstractmethod def get_benchmarks ( self ) -> List [ dict ]: \"\"\"Retrieves all benchmarks in the platform. Returns: List[dict]: all benchmarks information. \"\"\" @abstractmethod def get_benchmark ( self , benchmark_uid : int ) -> dict : \"\"\"Retrieves the benchmark specification file from the server Args: benchmark_uid (int): uid for the desired benchmark Returns: dict: benchmark specification \"\"\" @abstractmethod def get_benchmark_models ( self , benchmark_uid : int ) -> List [ int ]: \"\"\"Retrieves all the models associated with a benchmark. reference model not included Args: benchmark_uid (int): UID of the desired benchmark Returns: list[int]: List of model UIDS \"\"\" @abstractmethod def get_benchmark_demo_dataset ( self , demo_data_url : str ) -> str : \"\"\"Downloads the benchmark demo dataset and stores it in the user's machine Args: demo_data_url (str): location of demo data for download Returns: str: path where the downloaded demo dataset can be found \"\"\" @abstractmethod def get_user_benchmarks ( self ) -> List [ dict ]: \"\"\"Retrieves all benchmarks created by the user Returns: List[dict]: Benchmarks data \"\"\" @abstractmethod def get_cubes ( self ) -> List [ dict ]: \"\"\"Retrieves all MLCubes in the platform Returns: List[dict]: List containing the data of all MLCubes \"\"\" @abstractmethod def get_cube_metadata ( self , cube_uid : int ) -> dict : \"\"\"Retrieves metadata about the specified cube Args: cube_uid (int): UID of the desired cube. Returns: dict: Dictionary containing url and hashes for the cube files \"\"\" @abstractmethod def get_cube ( self , url : str , cube_uid : int ) -> str : \"\"\"Downloads and writes an mlcube.yaml file from the server Args: url (str): URL where the mlcube.yaml file can be downloaded. cube_uid (int): Cube UID. Returns: str: location where the mlcube.yaml file is stored locally. \"\"\" @abstractmethod def get_user_cubes ( self ) -> List [ dict ]: \"\"\"Retrieves metadata from all cubes registered by the user Returns: List[dict]: List of dictionaries containing the mlcubes registration information \"\"\" @abstractmethod def get_cube_params ( self , url : str , cube_uid : int ) -> str : \"\"\"Retrieves the cube parameters.yaml file from the server Args: url (str): URL where the parameters.yaml file can be downloaded. cube_uid (int): Cube UID. Returns: str: Location where the parameters.yaml file is stored locally. \"\"\" @abstractmethod def get_cube_additional ( self , url : str , cube_uid : int ) -> str : \"\"\"Retrieves and stores the additional_files.tar.gz file from the server Args: url (str): URL where the additional_files.tar.gz file can be downloaded. cube_uid (int): Cube UID. Returns: str: Location where the additional_files.tar.gz file is stored locally. \"\"\" @abstractmethod def upload_benchmark ( self , benchmark_dict : dict ) -> int : \"\"\"Uploads a new benchmark to the server. Args: benchmark_dict (dict): benchmark_data to be uploaded Returns: int: UID of newly created benchmark \"\"\" @abstractmethod def get_cube_image ( self , url : str , cube_uid : int ) -> str : \"\"\"Retrieves and stores the image file from the server Args: url (str): URL where the image file can be downloaded. cube_uid (int): Cube UID. Returns: str: Location where the image file is stored locally. \"\"\" @abstractmethod def upload_mlcube ( self , mlcube_body : dict ) -> int : \"\"\"Uploads an MLCube instance to the platform Args: mlcube_body (dict): Dictionary containing all the relevant data for creating mlcubes Returns: int: id of the created mlcube instance on the platform \"\"\" @abstractmethod def get_datasets ( self ) -> List [ dict ]: \"\"\"Retrieves all datasets in the platform Returns: List[dict]: List of data from all datasets \"\"\" @abstractmethod def get_dataset ( self , dset_uid : str ) -> dict : \"\"\"Retrieves a specific dataset Args: dset_uid (str): Dataset UID Returns: dict: Dataset metadata \"\"\" @abstractmethod def get_user_datasets ( self ) -> dict : \"\"\"Retrieves all datasets registered by the user Returns: dict: dictionary with the contents of each dataset registration query \"\"\" @abstractmethod def upload_dataset ( self , reg_dict : dict ) -> int : \"\"\"Uploads registration data to the server, under the sha name of the file. Args: reg_dict (dict): Dictionary containing registration information. Returns: int: id of the created dataset registration. \"\"\" @abstractmethod def get_result ( self , result_uid : str ) -> dict : \"\"\"Retrieves a specific result data Args: result_uid (str): Result UID Returns: dict: Result metadata \"\"\" @abstractmethod def get_user_results ( self ) -> dict : \"\"\"Retrieves all results registered by the user Returns: dict: dictionary with the contents of each dataset registration query \"\"\" @abstractmethod def upload_results ( self , results_dict : dict ) -> int : \"\"\"Uploads results to the server. Args: results_dict (dict): Dictionary containing results information. Returns: int: id of the generated results entry \"\"\" @abstractmethod def associate_dset ( self , data_uid : int , benchmark_uid : int , metadata : dict = {}): \"\"\"Create a Dataset Benchmark association Args: data_uid (int): Registered dataset UID benchmark_uid (int): Benchmark UID metadata (dict, optional): Additional metadata. Defaults to {}. \"\"\" @abstractmethod def associate_cube ( self , cube_uid : str , benchmark_uid : int , metadata : dict = {}): \"\"\"Create an MLCube-Benchmark association Args: cube_uid (str): MLCube UID benchmark_uid (int): Benchmark UID metadata (dict, optional): Additional metadata. Defaults to {}. \"\"\" @abstractmethod def set_dataset_association_approval ( self , dataset_uid : str , benchmark_uid : str , status : str ): \"\"\"Approves a dataset association Args: dataset_uid (str): Dataset UID benchmark_uid (str): Benchmark UID status (str): Approval status to set for the association \"\"\" @abstractmethod def set_mlcube_association_approval ( self , mlcube_uid : str , benchmark_uid : str , status : str ): \"\"\"Approves an mlcube association Args: mlcube_uid (str): Dataset UID benchmark_uid (str): Benchmark UID status (str): Approval status to set for the association \"\"\" @abstractmethod def get_datasets_associations ( self ) -> List [ dict ]: \"\"\"Get all dataset associations related to the current user Returns: List[dict]: List containing all associations information \"\"\" @abstractmethod def get_cubes_associations ( self ) -> List [ dict ]: \"\"\"Get all cube associations related to the current user Returns: List[dict]: List containing all associations information \"\"\" __init__ ( source , ui , token = None ) abstractmethod Create an instance of a communication object. Parameters: Name Type Description Default source str location of the communication source. Where messages are going to be sent. required ui UI Implementation of the UI interface. required token str , Optional authentication token to be used throughout communication. Defaults to None. None Source code in comms/interface.py 9 10 11 12 13 14 15 16 17 @abstractmethod def __init__ ( self , source : str , ui : UI , token : str = None ): \"\"\"Create an instance of a communication object. Args: source (str): location of the communication source. Where messages are going to be sent. ui (UI): Implementation of the UI interface. token (str, Optional): authentication token to be used throughout communication. Defaults to None. \"\"\" associate_cube ( cube_uid , benchmark_uid , metadata = {}) abstractmethod Create an MLCube-Benchmark association Parameters: Name Type Description Default cube_uid str MLCube UID required benchmark_uid int Benchmark UID required metadata dict Additional metadata. Defaults to {}. {} Source code in comms/interface.py 291 292 293 294 295 296 297 298 299 @abstractmethod def associate_cube ( self , cube_uid : str , benchmark_uid : int , metadata : dict = {}): \"\"\"Create an MLCube-Benchmark association Args: cube_uid (str): MLCube UID benchmark_uid (int): Benchmark UID metadata (dict, optional): Additional metadata. Defaults to {}. \"\"\" associate_dset ( data_uid , benchmark_uid , metadata = {}) abstractmethod Create a Dataset Benchmark association Parameters: Name Type Description Default data_uid int Registered dataset UID required benchmark_uid int Benchmark UID required metadata dict Additional metadata. Defaults to {}. {} Source code in comms/interface.py 281 282 283 284 285 286 287 288 289 @abstractmethod def associate_dset ( self , data_uid : int , benchmark_uid : int , metadata : dict = {}): \"\"\"Create a Dataset Benchmark association Args: data_uid (int): Registered dataset UID benchmark_uid (int): Benchmark UID metadata (dict, optional): Additional metadata. Defaults to {}. \"\"\" authenticate () abstractmethod Retrieve a token stored locally for authentication Source code in comms/interface.py 38 39 40 41 @abstractmethod def authenticate ( self ): \"\"\"Retrieve a token stored locally for authentication \"\"\" authorized_by_role ( benchmark_uid , role ) abstractmethod Indicates wether the current user is authorized to access a benchmark based on desired role Parameters: Name Type Description Default benchmark_uid int UID of the benchmark required role str Desired role to check for authorization required Returns: Name Type Description bool bool Wether the user has the specified role for that benchmark Source code in comms/interface.py 54 55 56 57 58 59 60 61 62 63 64 65 @abstractmethod def authorized_by_role ( self , benchmark_uid : int , role : str ) -> bool : \"\"\"Indicates wether the current user is authorized to access a benchmark based on desired role Args: benchmark_uid (int): UID of the benchmark role (str): Desired role to check for authorization Returns: bool: Wether the user has the specified role for that benchmark \"\"\" benchmark_association ( benchmark_uid ) abstractmethod Retrieves the benchmark association Parameters: Name Type Description Default benchmark_uid int UID of the benchmark required Returns: Name Type Description Role Role the association type between current user and benchmark Source code in comms/interface.py 43 44 45 46 47 48 49 50 51 52 @abstractmethod def benchmark_association ( self , benchmark_uid : int ) -> Role : \"\"\"Retrieves the benchmark association Args: benchmark_uid (int): UID of the benchmark Returns: Role: the association type between current user and benchmark \"\"\" change_password ( pwd , ui ) abstractmethod Sets a new password for the current user. Parameters: Name Type Description Default pwd str New password to be set required ui UI Instance of an implementation required Returns: Name Type Description bool bool Whether changing the password was successful or not Source code in comms/interface.py 27 28 29 30 31 32 33 34 35 36 @abstractmethod def change_password ( self , pwd : str , ui : UI ) -> bool : \"\"\"Sets a new password for the current user. Args: pwd (str): New password to be set ui (UI): Instance of an implementation Returns: bool: Whether changing the password was successful or not \"\"\" get_benchmark ( benchmark_uid ) abstractmethod Retrieves the benchmark specification file from the server Parameters: Name Type Description Default benchmark_uid int uid for the desired benchmark required Returns: Name Type Description dict dict benchmark specification Source code in comms/interface.py 75 76 77 78 79 80 81 82 83 84 @abstractmethod def get_benchmark ( self , benchmark_uid : int ) -> dict : \"\"\"Retrieves the benchmark specification file from the server Args: benchmark_uid (int): uid for the desired benchmark Returns: dict: benchmark specification \"\"\" get_benchmark_demo_dataset ( demo_data_url ) abstractmethod Downloads the benchmark demo dataset and stores it in the user's machine Parameters: Name Type Description Default demo_data_url str location of demo data for download required Returns: Name Type Description str str path where the downloaded demo dataset can be found Source code in comms/interface.py 97 98 99 100 101 102 103 104 105 106 @abstractmethod def get_benchmark_demo_dataset ( self , demo_data_url : str ) -> str : \"\"\"Downloads the benchmark demo dataset and stores it in the user's machine Args: demo_data_url (str): location of demo data for download Returns: str: path where the downloaded demo dataset can be found \"\"\" get_benchmark_models ( benchmark_uid ) abstractmethod Retrieves all the models associated with a benchmark. reference model not included Parameters: Name Type Description Default benchmark_uid int UID of the desired benchmark required Returns: Type Description List [ int ] list[int]: List of model UIDS Source code in comms/interface.py 86 87 88 89 90 91 92 93 94 95 @abstractmethod def get_benchmark_models ( self , benchmark_uid : int ) -> List [ int ]: \"\"\"Retrieves all the models associated with a benchmark. reference model not included Args: benchmark_uid (int): UID of the desired benchmark Returns: list[int]: List of model UIDS \"\"\" get_benchmarks () abstractmethod Retrieves all benchmarks in the platform. Returns: Type Description List [ dict ] List[dict]: all benchmarks information. Source code in comms/interface.py 67 68 69 70 71 72 73 @abstractmethod def get_benchmarks ( self ) -> List [ dict ]: \"\"\"Retrieves all benchmarks in the platform. Returns: List[dict]: all benchmarks information. \"\"\" get_cube ( url , cube_uid ) abstractmethod Downloads and writes an mlcube.yaml file from the server Parameters: Name Type Description Default url str URL where the mlcube.yaml file can be downloaded. required cube_uid int Cube UID. required Returns: Name Type Description str str location where the mlcube.yaml file is stored locally. Source code in comms/interface.py 135 136 137 138 139 140 141 142 143 144 145 @abstractmethod def get_cube ( self , url : str , cube_uid : int ) -> str : \"\"\"Downloads and writes an mlcube.yaml file from the server Args: url (str): URL where the mlcube.yaml file can be downloaded. cube_uid (int): Cube UID. Returns: str: location where the mlcube.yaml file is stored locally. \"\"\" get_cube_additional ( url , cube_uid ) abstractmethod Retrieves and stores the additional_files.tar.gz file from the server Parameters: Name Type Description Default url str URL where the additional_files.tar.gz file can be downloaded. required cube_uid int Cube UID. required Returns: Name Type Description str str Location where the additional_files.tar.gz file is stored locally. Source code in comms/interface.py 167 168 169 170 171 172 173 174 175 176 177 @abstractmethod def get_cube_additional ( self , url : str , cube_uid : int ) -> str : \"\"\"Retrieves and stores the additional_files.tar.gz file from the server Args: url (str): URL where the additional_files.tar.gz file can be downloaded. cube_uid (int): Cube UID. Returns: str: Location where the additional_files.tar.gz file is stored locally. \"\"\" get_cube_image ( url , cube_uid ) abstractmethod Retrieves and stores the image file from the server Parameters: Name Type Description Default url str URL where the image file can be downloaded. required cube_uid int Cube UID. required Returns: Name Type Description str str Location where the image file is stored locally. Source code in comms/interface.py 190 191 192 193 194 195 196 197 198 199 200 @abstractmethod def get_cube_image ( self , url : str , cube_uid : int ) -> str : \"\"\"Retrieves and stores the image file from the server Args: url (str): URL where the image file can be downloaded. cube_uid (int): Cube UID. Returns: str: Location where the image file is stored locally. \"\"\" get_cube_metadata ( cube_uid ) abstractmethod Retrieves metadata about the specified cube Parameters: Name Type Description Default cube_uid int UID of the desired cube. required Returns: Name Type Description dict dict Dictionary containing url and hashes for the cube files Source code in comms/interface.py 124 125 126 127 128 129 130 131 132 133 @abstractmethod def get_cube_metadata ( self , cube_uid : int ) -> dict : \"\"\"Retrieves metadata about the specified cube Args: cube_uid (int): UID of the desired cube. Returns: dict: Dictionary containing url and hashes for the cube files \"\"\" get_cube_params ( url , cube_uid ) abstractmethod Retrieves the cube parameters.yaml file from the server Parameters: Name Type Description Default url str URL where the parameters.yaml file can be downloaded. required cube_uid int Cube UID. required Returns: Name Type Description str str Location where the parameters.yaml file is stored locally. Source code in comms/interface.py 155 156 157 158 159 160 161 162 163 164 165 @abstractmethod def get_cube_params ( self , url : str , cube_uid : int ) -> str : \"\"\"Retrieves the cube parameters.yaml file from the server Args: url (str): URL where the parameters.yaml file can be downloaded. cube_uid (int): Cube UID. Returns: str: Location where the parameters.yaml file is stored locally. \"\"\" get_cubes () abstractmethod Retrieves all MLCubes in the platform Returns: Type Description List [ dict ] List[dict]: List containing the data of all MLCubes Source code in comms/interface.py 116 117 118 119 120 121 122 @abstractmethod def get_cubes ( self ) -> List [ dict ]: \"\"\"Retrieves all MLCubes in the platform Returns: List[dict]: List containing the data of all MLCubes \"\"\" get_cubes_associations () abstractmethod Get all cube associations related to the current user Returns: Type Description List [ dict ] List[dict]: List containing all associations information Source code in comms/interface.py 333 334 335 336 337 338 339 @abstractmethod def get_cubes_associations ( self ) -> List [ dict ]: \"\"\"Get all cube associations related to the current user Returns: List[dict]: List containing all associations information \"\"\" get_dataset ( dset_uid ) abstractmethod Retrieves a specific dataset Parameters: Name Type Description Default dset_uid str Dataset UID required Returns: Name Type Description dict dict Dataset metadata Source code in comms/interface.py 221 222 223 224 225 226 227 228 229 230 @abstractmethod def get_dataset ( self , dset_uid : str ) -> dict : \"\"\"Retrieves a specific dataset Args: dset_uid (str): Dataset UID Returns: dict: Dataset metadata \"\"\" get_datasets () abstractmethod Retrieves all datasets in the platform Returns: Type Description List [ dict ] List[dict]: List of data from all datasets Source code in comms/interface.py 213 214 215 216 217 218 219 @abstractmethod def get_datasets ( self ) -> List [ dict ]: \"\"\"Retrieves all datasets in the platform Returns: List[dict]: List of data from all datasets \"\"\" get_datasets_associations () abstractmethod Get all dataset associations related to the current user Returns: Type Description List [ dict ] List[dict]: List containing all associations information Source code in comms/interface.py 325 326 327 328 329 330 331 @abstractmethod def get_datasets_associations ( self ) -> List [ dict ]: \"\"\"Get all dataset associations related to the current user Returns: List[dict]: List containing all associations information \"\"\" get_result ( result_uid ) abstractmethod Retrieves a specific result data Parameters: Name Type Description Default result_uid str Result UID required Returns: Name Type Description dict dict Result metadata Source code in comms/interface.py 251 252 253 254 255 256 257 258 259 260 @abstractmethod def get_result ( self , result_uid : str ) -> dict : \"\"\"Retrieves a specific result data Args: result_uid (str): Result UID Returns: dict: Result metadata \"\"\" get_user_benchmarks () abstractmethod Retrieves all benchmarks created by the user Returns: Type Description List [ dict ] List[dict]: Benchmarks data Source code in comms/interface.py 108 109 110 111 112 113 114 @abstractmethod def get_user_benchmarks ( self ) -> List [ dict ]: \"\"\"Retrieves all benchmarks created by the user Returns: List[dict]: Benchmarks data \"\"\" get_user_cubes () abstractmethod Retrieves metadata from all cubes registered by the user Returns: Type Description List [ dict ] List[dict]: List of dictionaries containing the mlcubes registration information Source code in comms/interface.py 147 148 149 150 151 152 153 @abstractmethod def get_user_cubes ( self ) -> List [ dict ]: \"\"\"Retrieves metadata from all cubes registered by the user Returns: List[dict]: List of dictionaries containing the mlcubes registration information \"\"\" get_user_datasets () abstractmethod Retrieves all datasets registered by the user Returns: Name Type Description dict dict dictionary with the contents of each dataset registration query Source code in comms/interface.py 232 233 234 235 236 237 238 @abstractmethod def get_user_datasets ( self ) -> dict : \"\"\"Retrieves all datasets registered by the user Returns: dict: dictionary with the contents of each dataset registration query \"\"\" get_user_results () abstractmethod Retrieves all results registered by the user Returns: Name Type Description dict dict dictionary with the contents of each dataset registration query Source code in comms/interface.py 262 263 264 265 266 267 268 @abstractmethod def get_user_results ( self ) -> dict : \"\"\"Retrieves all results registered by the user Returns: dict: dictionary with the contents of each dataset registration query \"\"\" login ( ui ) abstractmethod Authenticate the comms instance for further interactions Parameters: Name Type Description Default ui UI instance of an implementation of the UI interface. required Source code in comms/interface.py 19 20 21 22 23 24 25 @abstractmethod def login ( self , ui : UI ): \"\"\"Authenticate the comms instance for further interactions Args: ui (UI): instance of an implementation of the UI interface. \"\"\" set_dataset_association_approval ( dataset_uid , benchmark_uid , status ) abstractmethod Approves a dataset association Parameters: Name Type Description Default dataset_uid str Dataset UID required benchmark_uid str Benchmark UID required status str Approval status to set for the association required Source code in comms/interface.py 301 302 303 304 305 306 307 308 309 310 311 @abstractmethod def set_dataset_association_approval ( self , dataset_uid : str , benchmark_uid : str , status : str ): \"\"\"Approves a dataset association Args: dataset_uid (str): Dataset UID benchmark_uid (str): Benchmark UID status (str): Approval status to set for the association \"\"\" set_mlcube_association_approval ( mlcube_uid , benchmark_uid , status ) abstractmethod Approves an mlcube association Parameters: Name Type Description Default mlcube_uid str Dataset UID required benchmark_uid str Benchmark UID required status str Approval status to set for the association required Source code in comms/interface.py 313 314 315 316 317 318 319 320 321 322 323 @abstractmethod def set_mlcube_association_approval ( self , mlcube_uid : str , benchmark_uid : str , status : str ): \"\"\"Approves an mlcube association Args: mlcube_uid (str): Dataset UID benchmark_uid (str): Benchmark UID status (str): Approval status to set for the association \"\"\" upload_benchmark ( benchmark_dict ) abstractmethod Uploads a new benchmark to the server. Parameters: Name Type Description Default benchmark_dict dict benchmark_data to be uploaded required Returns: Name Type Description int int UID of newly created benchmark Source code in comms/interface.py 179 180 181 182 183 184 185 186 187 188 @abstractmethod def upload_benchmark ( self , benchmark_dict : dict ) -> int : \"\"\"Uploads a new benchmark to the server. Args: benchmark_dict (dict): benchmark_data to be uploaded Returns: int: UID of newly created benchmark \"\"\" upload_dataset ( reg_dict ) abstractmethod Uploads registration data to the server, under the sha name of the file. Parameters: Name Type Description Default reg_dict dict Dictionary containing registration information. required Returns: Name Type Description int int id of the created dataset registration. Source code in comms/interface.py 240 241 242 243 244 245 246 247 248 249 @abstractmethod def upload_dataset ( self , reg_dict : dict ) -> int : \"\"\"Uploads registration data to the server, under the sha name of the file. Args: reg_dict (dict): Dictionary containing registration information. Returns: int: id of the created dataset registration. \"\"\" upload_mlcube ( mlcube_body ) abstractmethod Uploads an MLCube instance to the platform Parameters: Name Type Description Default mlcube_body dict Dictionary containing all the relevant data for creating mlcubes required Returns: Name Type Description int int id of the created mlcube instance on the platform Source code in comms/interface.py 202 203 204 205 206 207 208 209 210 211 @abstractmethod def upload_mlcube ( self , mlcube_body : dict ) -> int : \"\"\"Uploads an MLCube instance to the platform Args: mlcube_body (dict): Dictionary containing all the relevant data for creating mlcubes Returns: int: id of the created mlcube instance on the platform \"\"\" upload_results ( results_dict ) abstractmethod Uploads results to the server. Parameters: Name Type Description Default results_dict dict Dictionary containing results information. required Returns: Name Type Description int int id of the generated results entry Source code in comms/interface.py 270 271 272 273 274 275 276 277 278 279 @abstractmethod def upload_results ( self , results_dict : dict ) -> int : \"\"\"Uploads results to the server. Args: results_dict (dict): Dictionary containing results information. Returns: int: id of the generated results entry \"\"\"","title":"interface"},{"location":"reference/comms/interface/#comms.interface.Comms","text":"Bases: ABC Source code in comms/interface.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 class Comms ( ABC ): @abstractmethod def __init__ ( self , source : str , ui : UI , token : str = None ): \"\"\"Create an instance of a communication object. Args: source (str): location of the communication source. Where messages are going to be sent. ui (UI): Implementation of the UI interface. token (str, Optional): authentication token to be used throughout communication. Defaults to None. \"\"\" @abstractmethod def login ( self , ui : UI ): \"\"\"Authenticate the comms instance for further interactions Args: ui (UI): instance of an implementation of the UI interface. \"\"\" @abstractmethod def change_password ( self , pwd : str , ui : UI ) -> bool : \"\"\"Sets a new password for the current user. Args: pwd (str): New password to be set ui (UI): Instance of an implementation Returns: bool: Whether changing the password was successful or not \"\"\" @abstractmethod def authenticate ( self ): \"\"\"Retrieve a token stored locally for authentication \"\"\" @abstractmethod def benchmark_association ( self , benchmark_uid : int ) -> Role : \"\"\"Retrieves the benchmark association Args: benchmark_uid (int): UID of the benchmark Returns: Role: the association type between current user and benchmark \"\"\" @abstractmethod def authorized_by_role ( self , benchmark_uid : int , role : str ) -> bool : \"\"\"Indicates wether the current user is authorized to access a benchmark based on desired role Args: benchmark_uid (int): UID of the benchmark role (str): Desired role to check for authorization Returns: bool: Wether the user has the specified role for that benchmark \"\"\" @abstractmethod def get_benchmarks ( self ) -> List [ dict ]: \"\"\"Retrieves all benchmarks in the platform. Returns: List[dict]: all benchmarks information. \"\"\" @abstractmethod def get_benchmark ( self , benchmark_uid : int ) -> dict : \"\"\"Retrieves the benchmark specification file from the server Args: benchmark_uid (int): uid for the desired benchmark Returns: dict: benchmark specification \"\"\" @abstractmethod def get_benchmark_models ( self , benchmark_uid : int ) -> List [ int ]: \"\"\"Retrieves all the models associated with a benchmark. reference model not included Args: benchmark_uid (int): UID of the desired benchmark Returns: list[int]: List of model UIDS \"\"\" @abstractmethod def get_benchmark_demo_dataset ( self , demo_data_url : str ) -> str : \"\"\"Downloads the benchmark demo dataset and stores it in the user's machine Args: demo_data_url (str): location of demo data for download Returns: str: path where the downloaded demo dataset can be found \"\"\" @abstractmethod def get_user_benchmarks ( self ) -> List [ dict ]: \"\"\"Retrieves all benchmarks created by the user Returns: List[dict]: Benchmarks data \"\"\" @abstractmethod def get_cubes ( self ) -> List [ dict ]: \"\"\"Retrieves all MLCubes in the platform Returns: List[dict]: List containing the data of all MLCubes \"\"\" @abstractmethod def get_cube_metadata ( self , cube_uid : int ) -> dict : \"\"\"Retrieves metadata about the specified cube Args: cube_uid (int): UID of the desired cube. Returns: dict: Dictionary containing url and hashes for the cube files \"\"\" @abstractmethod def get_cube ( self , url : str , cube_uid : int ) -> str : \"\"\"Downloads and writes an mlcube.yaml file from the server Args: url (str): URL where the mlcube.yaml file can be downloaded. cube_uid (int): Cube UID. Returns: str: location where the mlcube.yaml file is stored locally. \"\"\" @abstractmethod def get_user_cubes ( self ) -> List [ dict ]: \"\"\"Retrieves metadata from all cubes registered by the user Returns: List[dict]: List of dictionaries containing the mlcubes registration information \"\"\" @abstractmethod def get_cube_params ( self , url : str , cube_uid : int ) -> str : \"\"\"Retrieves the cube parameters.yaml file from the server Args: url (str): URL where the parameters.yaml file can be downloaded. cube_uid (int): Cube UID. Returns: str: Location where the parameters.yaml file is stored locally. \"\"\" @abstractmethod def get_cube_additional ( self , url : str , cube_uid : int ) -> str : \"\"\"Retrieves and stores the additional_files.tar.gz file from the server Args: url (str): URL where the additional_files.tar.gz file can be downloaded. cube_uid (int): Cube UID. Returns: str: Location where the additional_files.tar.gz file is stored locally. \"\"\" @abstractmethod def upload_benchmark ( self , benchmark_dict : dict ) -> int : \"\"\"Uploads a new benchmark to the server. Args: benchmark_dict (dict): benchmark_data to be uploaded Returns: int: UID of newly created benchmark \"\"\" @abstractmethod def get_cube_image ( self , url : str , cube_uid : int ) -> str : \"\"\"Retrieves and stores the image file from the server Args: url (str): URL where the image file can be downloaded. cube_uid (int): Cube UID. Returns: str: Location where the image file is stored locally. \"\"\" @abstractmethod def upload_mlcube ( self , mlcube_body : dict ) -> int : \"\"\"Uploads an MLCube instance to the platform Args: mlcube_body (dict): Dictionary containing all the relevant data for creating mlcubes Returns: int: id of the created mlcube instance on the platform \"\"\" @abstractmethod def get_datasets ( self ) -> List [ dict ]: \"\"\"Retrieves all datasets in the platform Returns: List[dict]: List of data from all datasets \"\"\" @abstractmethod def get_dataset ( self , dset_uid : str ) -> dict : \"\"\"Retrieves a specific dataset Args: dset_uid (str): Dataset UID Returns: dict: Dataset metadata \"\"\" @abstractmethod def get_user_datasets ( self ) -> dict : \"\"\"Retrieves all datasets registered by the user Returns: dict: dictionary with the contents of each dataset registration query \"\"\" @abstractmethod def upload_dataset ( self , reg_dict : dict ) -> int : \"\"\"Uploads registration data to the server, under the sha name of the file. Args: reg_dict (dict): Dictionary containing registration information. Returns: int: id of the created dataset registration. \"\"\" @abstractmethod def get_result ( self , result_uid : str ) -> dict : \"\"\"Retrieves a specific result data Args: result_uid (str): Result UID Returns: dict: Result metadata \"\"\" @abstractmethod def get_user_results ( self ) -> dict : \"\"\"Retrieves all results registered by the user Returns: dict: dictionary with the contents of each dataset registration query \"\"\" @abstractmethod def upload_results ( self , results_dict : dict ) -> int : \"\"\"Uploads results to the server. Args: results_dict (dict): Dictionary containing results information. Returns: int: id of the generated results entry \"\"\" @abstractmethod def associate_dset ( self , data_uid : int , benchmark_uid : int , metadata : dict = {}): \"\"\"Create a Dataset Benchmark association Args: data_uid (int): Registered dataset UID benchmark_uid (int): Benchmark UID metadata (dict, optional): Additional metadata. Defaults to {}. \"\"\" @abstractmethod def associate_cube ( self , cube_uid : str , benchmark_uid : int , metadata : dict = {}): \"\"\"Create an MLCube-Benchmark association Args: cube_uid (str): MLCube UID benchmark_uid (int): Benchmark UID metadata (dict, optional): Additional metadata. Defaults to {}. \"\"\" @abstractmethod def set_dataset_association_approval ( self , dataset_uid : str , benchmark_uid : str , status : str ): \"\"\"Approves a dataset association Args: dataset_uid (str): Dataset UID benchmark_uid (str): Benchmark UID status (str): Approval status to set for the association \"\"\" @abstractmethod def set_mlcube_association_approval ( self , mlcube_uid : str , benchmark_uid : str , status : str ): \"\"\"Approves an mlcube association Args: mlcube_uid (str): Dataset UID benchmark_uid (str): Benchmark UID status (str): Approval status to set for the association \"\"\" @abstractmethod def get_datasets_associations ( self ) -> List [ dict ]: \"\"\"Get all dataset associations related to the current user Returns: List[dict]: List containing all associations information \"\"\" @abstractmethod def get_cubes_associations ( self ) -> List [ dict ]: \"\"\"Get all cube associations related to the current user Returns: List[dict]: List containing all associations information \"\"\"","title":"Comms"},{"location":"reference/comms/interface/#comms.interface.Comms.__init__","text":"Create an instance of a communication object. Parameters: Name Type Description Default source str location of the communication source. Where messages are going to be sent. required ui UI Implementation of the UI interface. required token str , Optional authentication token to be used throughout communication. Defaults to None. None Source code in comms/interface.py 9 10 11 12 13 14 15 16 17 @abstractmethod def __init__ ( self , source : str , ui : UI , token : str = None ): \"\"\"Create an instance of a communication object. Args: source (str): location of the communication source. Where messages are going to be sent. ui (UI): Implementation of the UI interface. token (str, Optional): authentication token to be used throughout communication. Defaults to None. \"\"\"","title":"__init__()"},{"location":"reference/comms/interface/#comms.interface.Comms.associate_cube","text":"Create an MLCube-Benchmark association Parameters: Name Type Description Default cube_uid str MLCube UID required benchmark_uid int Benchmark UID required metadata dict Additional metadata. Defaults to {}. {} Source code in comms/interface.py 291 292 293 294 295 296 297 298 299 @abstractmethod def associate_cube ( self , cube_uid : str , benchmark_uid : int , metadata : dict = {}): \"\"\"Create an MLCube-Benchmark association Args: cube_uid (str): MLCube UID benchmark_uid (int): Benchmark UID metadata (dict, optional): Additional metadata. Defaults to {}. \"\"\"","title":"associate_cube()"},{"location":"reference/comms/interface/#comms.interface.Comms.associate_dset","text":"Create a Dataset Benchmark association Parameters: Name Type Description Default data_uid int Registered dataset UID required benchmark_uid int Benchmark UID required metadata dict Additional metadata. Defaults to {}. {} Source code in comms/interface.py 281 282 283 284 285 286 287 288 289 @abstractmethod def associate_dset ( self , data_uid : int , benchmark_uid : int , metadata : dict = {}): \"\"\"Create a Dataset Benchmark association Args: data_uid (int): Registered dataset UID benchmark_uid (int): Benchmark UID metadata (dict, optional): Additional metadata. Defaults to {}. \"\"\"","title":"associate_dset()"},{"location":"reference/comms/interface/#comms.interface.Comms.authenticate","text":"Retrieve a token stored locally for authentication Source code in comms/interface.py 38 39 40 41 @abstractmethod def authenticate ( self ): \"\"\"Retrieve a token stored locally for authentication \"\"\"","title":"authenticate()"},{"location":"reference/comms/interface/#comms.interface.Comms.authorized_by_role","text":"Indicates wether the current user is authorized to access a benchmark based on desired role Parameters: Name Type Description Default benchmark_uid int UID of the benchmark required role str Desired role to check for authorization required Returns: Name Type Description bool bool Wether the user has the specified role for that benchmark Source code in comms/interface.py 54 55 56 57 58 59 60 61 62 63 64 65 @abstractmethod def authorized_by_role ( self , benchmark_uid : int , role : str ) -> bool : \"\"\"Indicates wether the current user is authorized to access a benchmark based on desired role Args: benchmark_uid (int): UID of the benchmark role (str): Desired role to check for authorization Returns: bool: Wether the user has the specified role for that benchmark \"\"\"","title":"authorized_by_role()"},{"location":"reference/comms/interface/#comms.interface.Comms.benchmark_association","text":"Retrieves the benchmark association Parameters: Name Type Description Default benchmark_uid int UID of the benchmark required Returns: Name Type Description Role Role the association type between current user and benchmark Source code in comms/interface.py 43 44 45 46 47 48 49 50 51 52 @abstractmethod def benchmark_association ( self , benchmark_uid : int ) -> Role : \"\"\"Retrieves the benchmark association Args: benchmark_uid (int): UID of the benchmark Returns: Role: the association type between current user and benchmark \"\"\"","title":"benchmark_association()"},{"location":"reference/comms/interface/#comms.interface.Comms.change_password","text":"Sets a new password for the current user. Parameters: Name Type Description Default pwd str New password to be set required ui UI Instance of an implementation required Returns: Name Type Description bool bool Whether changing the password was successful or not Source code in comms/interface.py 27 28 29 30 31 32 33 34 35 36 @abstractmethod def change_password ( self , pwd : str , ui : UI ) -> bool : \"\"\"Sets a new password for the current user. Args: pwd (str): New password to be set ui (UI): Instance of an implementation Returns: bool: Whether changing the password was successful or not \"\"\"","title":"change_password()"},{"location":"reference/comms/interface/#comms.interface.Comms.get_benchmark","text":"Retrieves the benchmark specification file from the server Parameters: Name Type Description Default benchmark_uid int uid for the desired benchmark required Returns: Name Type Description dict dict benchmark specification Source code in comms/interface.py 75 76 77 78 79 80 81 82 83 84 @abstractmethod def get_benchmark ( self , benchmark_uid : int ) -> dict : \"\"\"Retrieves the benchmark specification file from the server Args: benchmark_uid (int): uid for the desired benchmark Returns: dict: benchmark specification \"\"\"","title":"get_benchmark()"},{"location":"reference/comms/interface/#comms.interface.Comms.get_benchmark_demo_dataset","text":"Downloads the benchmark demo dataset and stores it in the user's machine Parameters: Name Type Description Default demo_data_url str location of demo data for download required Returns: Name Type Description str str path where the downloaded demo dataset can be found Source code in comms/interface.py 97 98 99 100 101 102 103 104 105 106 @abstractmethod def get_benchmark_demo_dataset ( self , demo_data_url : str ) -> str : \"\"\"Downloads the benchmark demo dataset and stores it in the user's machine Args: demo_data_url (str): location of demo data for download Returns: str: path where the downloaded demo dataset can be found \"\"\"","title":"get_benchmark_demo_dataset()"},{"location":"reference/comms/interface/#comms.interface.Comms.get_benchmark_models","text":"Retrieves all the models associated with a benchmark. reference model not included Parameters: Name Type Description Default benchmark_uid int UID of the desired benchmark required Returns: Type Description List [ int ] list[int]: List of model UIDS Source code in comms/interface.py 86 87 88 89 90 91 92 93 94 95 @abstractmethod def get_benchmark_models ( self , benchmark_uid : int ) -> List [ int ]: \"\"\"Retrieves all the models associated with a benchmark. reference model not included Args: benchmark_uid (int): UID of the desired benchmark Returns: list[int]: List of model UIDS \"\"\"","title":"get_benchmark_models()"},{"location":"reference/comms/interface/#comms.interface.Comms.get_benchmarks","text":"Retrieves all benchmarks in the platform. Returns: Type Description List [ dict ] List[dict]: all benchmarks information. Source code in comms/interface.py 67 68 69 70 71 72 73 @abstractmethod def get_benchmarks ( self ) -> List [ dict ]: \"\"\"Retrieves all benchmarks in the platform. Returns: List[dict]: all benchmarks information. \"\"\"","title":"get_benchmarks()"},{"location":"reference/comms/interface/#comms.interface.Comms.get_cube","text":"Downloads and writes an mlcube.yaml file from the server Parameters: Name Type Description Default url str URL where the mlcube.yaml file can be downloaded. required cube_uid int Cube UID. required Returns: Name Type Description str str location where the mlcube.yaml file is stored locally. Source code in comms/interface.py 135 136 137 138 139 140 141 142 143 144 145 @abstractmethod def get_cube ( self , url : str , cube_uid : int ) -> str : \"\"\"Downloads and writes an mlcube.yaml file from the server Args: url (str): URL where the mlcube.yaml file can be downloaded. cube_uid (int): Cube UID. Returns: str: location where the mlcube.yaml file is stored locally. \"\"\"","title":"get_cube()"},{"location":"reference/comms/interface/#comms.interface.Comms.get_cube_additional","text":"Retrieves and stores the additional_files.tar.gz file from the server Parameters: Name Type Description Default url str URL where the additional_files.tar.gz file can be downloaded. required cube_uid int Cube UID. required Returns: Name Type Description str str Location where the additional_files.tar.gz file is stored locally. Source code in comms/interface.py 167 168 169 170 171 172 173 174 175 176 177 @abstractmethod def get_cube_additional ( self , url : str , cube_uid : int ) -> str : \"\"\"Retrieves and stores the additional_files.tar.gz file from the server Args: url (str): URL where the additional_files.tar.gz file can be downloaded. cube_uid (int): Cube UID. Returns: str: Location where the additional_files.tar.gz file is stored locally. \"\"\"","title":"get_cube_additional()"},{"location":"reference/comms/interface/#comms.interface.Comms.get_cube_image","text":"Retrieves and stores the image file from the server Parameters: Name Type Description Default url str URL where the image file can be downloaded. required cube_uid int Cube UID. required Returns: Name Type Description str str Location where the image file is stored locally. Source code in comms/interface.py 190 191 192 193 194 195 196 197 198 199 200 @abstractmethod def get_cube_image ( self , url : str , cube_uid : int ) -> str : \"\"\"Retrieves and stores the image file from the server Args: url (str): URL where the image file can be downloaded. cube_uid (int): Cube UID. Returns: str: Location where the image file is stored locally. \"\"\"","title":"get_cube_image()"},{"location":"reference/comms/interface/#comms.interface.Comms.get_cube_metadata","text":"Retrieves metadata about the specified cube Parameters: Name Type Description Default cube_uid int UID of the desired cube. required Returns: Name Type Description dict dict Dictionary containing url and hashes for the cube files Source code in comms/interface.py 124 125 126 127 128 129 130 131 132 133 @abstractmethod def get_cube_metadata ( self , cube_uid : int ) -> dict : \"\"\"Retrieves metadata about the specified cube Args: cube_uid (int): UID of the desired cube. Returns: dict: Dictionary containing url and hashes for the cube files \"\"\"","title":"get_cube_metadata()"},{"location":"reference/comms/interface/#comms.interface.Comms.get_cube_params","text":"Retrieves the cube parameters.yaml file from the server Parameters: Name Type Description Default url str URL where the parameters.yaml file can be downloaded. required cube_uid int Cube UID. required Returns: Name Type Description str str Location where the parameters.yaml file is stored locally. Source code in comms/interface.py 155 156 157 158 159 160 161 162 163 164 165 @abstractmethod def get_cube_params ( self , url : str , cube_uid : int ) -> str : \"\"\"Retrieves the cube parameters.yaml file from the server Args: url (str): URL where the parameters.yaml file can be downloaded. cube_uid (int): Cube UID. Returns: str: Location where the parameters.yaml file is stored locally. \"\"\"","title":"get_cube_params()"},{"location":"reference/comms/interface/#comms.interface.Comms.get_cubes","text":"Retrieves all MLCubes in the platform Returns: Type Description List [ dict ] List[dict]: List containing the data of all MLCubes Source code in comms/interface.py 116 117 118 119 120 121 122 @abstractmethod def get_cubes ( self ) -> List [ dict ]: \"\"\"Retrieves all MLCubes in the platform Returns: List[dict]: List containing the data of all MLCubes \"\"\"","title":"get_cubes()"},{"location":"reference/comms/interface/#comms.interface.Comms.get_cubes_associations","text":"Get all cube associations related to the current user Returns: Type Description List [ dict ] List[dict]: List containing all associations information Source code in comms/interface.py 333 334 335 336 337 338 339 @abstractmethod def get_cubes_associations ( self ) -> List [ dict ]: \"\"\"Get all cube associations related to the current user Returns: List[dict]: List containing all associations information \"\"\"","title":"get_cubes_associations()"},{"location":"reference/comms/interface/#comms.interface.Comms.get_dataset","text":"Retrieves a specific dataset Parameters: Name Type Description Default dset_uid str Dataset UID required Returns: Name Type Description dict dict Dataset metadata Source code in comms/interface.py 221 222 223 224 225 226 227 228 229 230 @abstractmethod def get_dataset ( self , dset_uid : str ) -> dict : \"\"\"Retrieves a specific dataset Args: dset_uid (str): Dataset UID Returns: dict: Dataset metadata \"\"\"","title":"get_dataset()"},{"location":"reference/comms/interface/#comms.interface.Comms.get_datasets","text":"Retrieves all datasets in the platform Returns: Type Description List [ dict ] List[dict]: List of data from all datasets Source code in comms/interface.py 213 214 215 216 217 218 219 @abstractmethod def get_datasets ( self ) -> List [ dict ]: \"\"\"Retrieves all datasets in the platform Returns: List[dict]: List of data from all datasets \"\"\"","title":"get_datasets()"},{"location":"reference/comms/interface/#comms.interface.Comms.get_datasets_associations","text":"Get all dataset associations related to the current user Returns: Type Description List [ dict ] List[dict]: List containing all associations information Source code in comms/interface.py 325 326 327 328 329 330 331 @abstractmethod def get_datasets_associations ( self ) -> List [ dict ]: \"\"\"Get all dataset associations related to the current user Returns: List[dict]: List containing all associations information \"\"\"","title":"get_datasets_associations()"},{"location":"reference/comms/interface/#comms.interface.Comms.get_result","text":"Retrieves a specific result data Parameters: Name Type Description Default result_uid str Result UID required Returns: Name Type Description dict dict Result metadata Source code in comms/interface.py 251 252 253 254 255 256 257 258 259 260 @abstractmethod def get_result ( self , result_uid : str ) -> dict : \"\"\"Retrieves a specific result data Args: result_uid (str): Result UID Returns: dict: Result metadata \"\"\"","title":"get_result()"},{"location":"reference/comms/interface/#comms.interface.Comms.get_user_benchmarks","text":"Retrieves all benchmarks created by the user Returns: Type Description List [ dict ] List[dict]: Benchmarks data Source code in comms/interface.py 108 109 110 111 112 113 114 @abstractmethod def get_user_benchmarks ( self ) -> List [ dict ]: \"\"\"Retrieves all benchmarks created by the user Returns: List[dict]: Benchmarks data \"\"\"","title":"get_user_benchmarks()"},{"location":"reference/comms/interface/#comms.interface.Comms.get_user_cubes","text":"Retrieves metadata from all cubes registered by the user Returns: Type Description List [ dict ] List[dict]: List of dictionaries containing the mlcubes registration information Source code in comms/interface.py 147 148 149 150 151 152 153 @abstractmethod def get_user_cubes ( self ) -> List [ dict ]: \"\"\"Retrieves metadata from all cubes registered by the user Returns: List[dict]: List of dictionaries containing the mlcubes registration information \"\"\"","title":"get_user_cubes()"},{"location":"reference/comms/interface/#comms.interface.Comms.get_user_datasets","text":"Retrieves all datasets registered by the user Returns: Name Type Description dict dict dictionary with the contents of each dataset registration query Source code in comms/interface.py 232 233 234 235 236 237 238 @abstractmethod def get_user_datasets ( self ) -> dict : \"\"\"Retrieves all datasets registered by the user Returns: dict: dictionary with the contents of each dataset registration query \"\"\"","title":"get_user_datasets()"},{"location":"reference/comms/interface/#comms.interface.Comms.get_user_results","text":"Retrieves all results registered by the user Returns: Name Type Description dict dict dictionary with the contents of each dataset registration query Source code in comms/interface.py 262 263 264 265 266 267 268 @abstractmethod def get_user_results ( self ) -> dict : \"\"\"Retrieves all results registered by the user Returns: dict: dictionary with the contents of each dataset registration query \"\"\"","title":"get_user_results()"},{"location":"reference/comms/interface/#comms.interface.Comms.login","text":"Authenticate the comms instance for further interactions Parameters: Name Type Description Default ui UI instance of an implementation of the UI interface. required Source code in comms/interface.py 19 20 21 22 23 24 25 @abstractmethod def login ( self , ui : UI ): \"\"\"Authenticate the comms instance for further interactions Args: ui (UI): instance of an implementation of the UI interface. \"\"\"","title":"login()"},{"location":"reference/comms/interface/#comms.interface.Comms.set_dataset_association_approval","text":"Approves a dataset association Parameters: Name Type Description Default dataset_uid str Dataset UID required benchmark_uid str Benchmark UID required status str Approval status to set for the association required Source code in comms/interface.py 301 302 303 304 305 306 307 308 309 310 311 @abstractmethod def set_dataset_association_approval ( self , dataset_uid : str , benchmark_uid : str , status : str ): \"\"\"Approves a dataset association Args: dataset_uid (str): Dataset UID benchmark_uid (str): Benchmark UID status (str): Approval status to set for the association \"\"\"","title":"set_dataset_association_approval()"},{"location":"reference/comms/interface/#comms.interface.Comms.set_mlcube_association_approval","text":"Approves an mlcube association Parameters: Name Type Description Default mlcube_uid str Dataset UID required benchmark_uid str Benchmark UID required status str Approval status to set for the association required Source code in comms/interface.py 313 314 315 316 317 318 319 320 321 322 323 @abstractmethod def set_mlcube_association_approval ( self , mlcube_uid : str , benchmark_uid : str , status : str ): \"\"\"Approves an mlcube association Args: mlcube_uid (str): Dataset UID benchmark_uid (str): Benchmark UID status (str): Approval status to set for the association \"\"\"","title":"set_mlcube_association_approval()"},{"location":"reference/comms/interface/#comms.interface.Comms.upload_benchmark","text":"Uploads a new benchmark to the server. Parameters: Name Type Description Default benchmark_dict dict benchmark_data to be uploaded required Returns: Name Type Description int int UID of newly created benchmark Source code in comms/interface.py 179 180 181 182 183 184 185 186 187 188 @abstractmethod def upload_benchmark ( self , benchmark_dict : dict ) -> int : \"\"\"Uploads a new benchmark to the server. Args: benchmark_dict (dict): benchmark_data to be uploaded Returns: int: UID of newly created benchmark \"\"\"","title":"upload_benchmark()"},{"location":"reference/comms/interface/#comms.interface.Comms.upload_dataset","text":"Uploads registration data to the server, under the sha name of the file. Parameters: Name Type Description Default reg_dict dict Dictionary containing registration information. required Returns: Name Type Description int int id of the created dataset registration. Source code in comms/interface.py 240 241 242 243 244 245 246 247 248 249 @abstractmethod def upload_dataset ( self , reg_dict : dict ) -> int : \"\"\"Uploads registration data to the server, under the sha name of the file. Args: reg_dict (dict): Dictionary containing registration information. Returns: int: id of the created dataset registration. \"\"\"","title":"upload_dataset()"},{"location":"reference/comms/interface/#comms.interface.Comms.upload_mlcube","text":"Uploads an MLCube instance to the platform Parameters: Name Type Description Default mlcube_body dict Dictionary containing all the relevant data for creating mlcubes required Returns: Name Type Description int int id of the created mlcube instance on the platform Source code in comms/interface.py 202 203 204 205 206 207 208 209 210 211 @abstractmethod def upload_mlcube ( self , mlcube_body : dict ) -> int : \"\"\"Uploads an MLCube instance to the platform Args: mlcube_body (dict): Dictionary containing all the relevant data for creating mlcubes Returns: int: id of the created mlcube instance on the platform \"\"\"","title":"upload_mlcube()"},{"location":"reference/comms/interface/#comms.interface.Comms.upload_results","text":"Uploads results to the server. Parameters: Name Type Description Default results_dict dict Dictionary containing results information. required Returns: Name Type Description int int id of the generated results entry Source code in comms/interface.py 270 271 272 273 274 275 276 277 278 279 @abstractmethod def upload_results ( self , results_dict : dict ) -> int : \"\"\"Uploads results to the server. Args: results_dict (dict): Dictionary containing results information. Returns: int: id of the generated results entry \"\"\"","title":"upload_results()"},{"location":"reference/comms/rest/","text":"REST Bases: Comms Source code in comms/rest.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 class REST ( Comms ): def __init__ ( self , source : str , token = None ): self . server_url = self . __parse_url ( source ) self . token = token self . cert = config . certificate if self . cert is None : # No certificate provided, default to normal verification self . cert = True def __parse_url ( self , url ): url_sections = url . split ( \"://\" ) # Remove protocol if passed if len ( url_sections ) > 1 : url = \"\" . join ( url_sections [ 1 :]) return f \"https:// { url } \" def login ( self , user : str , pwd : str ): \"\"\"Authenticates the user with the server. Required for most endpoints Args: ui (UI): Instance of an implementation of the UI interface user (str): Username pwd (str): Password \"\"\" body = { \"username\" : user , \"password\" : pwd } res = self . __req ( f \" { self . server_url } /auth-token/\" , requests . post , json = body ) if res . status_code != 200 : log_response_error ( res ) CommunicationAuthenticationError ( \"Unable to authenticate user with provided credentials\" ) else : self . token = res . json ()[ \"token\" ] def change_password ( self , pwd : str ) -> bool : \"\"\"Sets a new password for the current user. Args: pwd (str): New password to be set ui (UI): Instance of an implementation Returns: bool: Whether changing the password was successful or not \"\"\" body = { \"password\" : pwd } res = self . __auth_post ( f \" { self . server_url } /me/password/\" , json = body ) if res . status_code != 200 : log_response_error ( res ) raise CommunicationRequestError ( \"Unable to change the current password\" ) return True def authenticate ( self ): cred_path = storage_path ( config . credentials_path ) if os . path . exists ( cred_path ): with open ( cred_path ) as f : self . token = f . readline () else : raise CommunicationAuthenticationError ( \"Couldn't find credentials file. Did you run 'medperf login' before?\" ) def __auth_get ( self , url , ** kwargs ): return self . __auth_req ( url , requests . get , ** kwargs ) def __auth_post ( self , url , ** kwargs ): return self . __auth_req ( url , requests . post , ** kwargs ) def __auth_put ( self , url , ** kwargs ): return self . __auth_req ( url , requests . put , ** kwargs ) def __auth_req ( self , url , req_func , ** kwargs ): if self . token is None : self . authenticate () return self . __req ( url , req_func , headers = { \"Authorization\" : f \"Token { self . token } \" }, ** kwargs ) def __req ( self , url , req_func , ** kwargs ): logging . debug ( f \"Calling { req_func } : { url } \" ) if \"json\" in kwargs : logging . debug ( f \"Passing JSON contents: { kwargs [ 'json' ] } \" ) kwargs [ \"json\" ] = sanitize_json ( kwargs [ \"json\" ]) try : return req_func ( url , verify = self . cert , ** kwargs ) except requests . exceptions . SSLError as e : logging . error ( f \"Couldn't connect to { self . server_url } : { e } \" ) raise requests . exceptions . SSLError ( \"Couldn't connect to server through HTTPS. If running locally, \" \"remember to provide the server certificate through --certificate\" ) def __get_list ( self , url , num_elements = None , page_size = config . default_page_size , offset = 0 , binary_reduction = False , ): \"\"\"Retrieves a list of elements from a URL by iterating over pages until num_elements is obtained. If num_elements is None, then iterates until all elements have been retrieved. If binary_reduction is enabled, errors are assumed to be related to response size. In that case, the page_size is reduced by half until a successful response is obtained or until page_size can't be reduced anymore. Args: url (str): The url to retrieve elements from num_elements (int, optional): The desired number of elements to be retrieved. Defaults to None. page_size (int, optional): Starting page size. Defaults to config.default_page_size. start_limit (int, optional): The starting position for element retrieval. Defaults to 0. binary_reduction (bool, optional): Wether to handle errors by halfing the page size. Defaults to False. Returns: List[dict]: A list of dictionaries representing the retrieved elements. \"\"\" el_list = [] if num_elements is None : num_elements = float ( \"inf\" ) while len ( el_list ) < num_elements : paginated_url = f \" { url } ?limit= { page_size } &offset= { offset } \" res = self . __auth_get ( paginated_url ) if res . status_code != 200 : if not binary_reduction : log_response_error ( res ) raise CommunicationRetrievalError ( \"there was an error retrieving the current list.\" ) log_response_error ( res , warn = True ) if page_size <= 1 : raise CommunicationRetrievalError ( \"Could not retrieve list. Minimum page size achieved without success.\" ) page_size = page_size // 2 continue else : data = res . json () el_list += data [ \"results\" ] offset += len ( data [ \"results\" ]) if data [ \"next\" ] is None : break if type ( num_elements ) == int : return el_list [: num_elements ] return el_list def __set_approval_status ( self , url : str , status : str ) -> requests . Response : \"\"\"Sets the approval status of a resource Args: url (str): URL to the resource to update status (str): approval status to set Returns: requests.Response: Response object returned by the update \"\"\" data = { \"approval_status\" : status } res = self . __auth_put ( url , json = data ,) return res def benchmark_association ( self , benchmark_uid : int ) -> Role : \"\"\"Retrieves the benchmark association Args: benchmark_uid (int): UID of the benchmark Returns: Role: the association type between current user and benchmark \"\"\" benchmarks = self . __get_list ( f \" { self . server_url } /me/benchmarks\" ) bm_dict = { bm [ \"benchmark\" ]: bm for bm in benchmarks } rolename = None if benchmark_uid in bm_dict : rolename = bm_dict [ benchmark_uid ][ \"role\" ] return Role ( rolename ) def authorized_by_role ( self , benchmark_uid : int , role : str ) -> bool : \"\"\"Indicates wether the current user is authorized to access a benchmark based on desired role Args: benchmark_uid (int): UID of the benchmark role (str): Desired role to check for authorization Returns: bool: Wether the user has the specified role for that benchmark \"\"\" assoc_role = self . benchmark_association ( benchmark_uid ) return assoc_role . name == role def get_benchmarks ( self ) -> List [ dict ]: \"\"\"Retrieves all benchmarks in the platform. Returns: List[dict]: all benchmarks information. \"\"\" bmks = self . __get_list ( f \" { self . server_url } /benchmarks/\" ) return bmks def get_benchmark ( self , benchmark_uid : int ) -> dict : \"\"\"Retrieves the benchmark specification file from the server Args: benchmark_uid (int): uid for the desired benchmark Returns: dict: benchmark specification \"\"\" res = self . __auth_get ( f \" { self . server_url } /benchmarks/ { benchmark_uid } \" ) if res . status_code != 200 : log_response_error ( res ) raise CommunicationRetrievalError ( \"the specified benchmark doesn't exist\" ) return res . json () def get_benchmark_models ( self , benchmark_uid : int ) -> List [ int ]: \"\"\"Retrieves all the models associated with a benchmark. reference model not included Args: benchmark_uid (int): UID of the desired benchmark Returns: list[int]: List of model UIDS \"\"\" models = self . __get_list ( f \" { self . server_url } /benchmarks/ { benchmark_uid } /models\" ) model_uids = [ model [ \"id\" ] for model in models ] return model_uids def get_benchmark_demo_dataset ( self , demo_data_url : str , uid : str = generate_tmp_uid () ) -> str : \"\"\"Downloads the benchmark demo dataset and stores it in the user's machine Args: demo_data_url (str): location of demo data for download uid (str): UID to use for storing the demo dataset. Defaults to generate_tmp_uid(). Returns: str: path where the downloaded demo dataset can be found \"\"\" tmp_dir = storage_path ( config . demo_data_storage ) demo_data_path = os . path . join ( tmp_dir , uid ) tball_file = config . tarball_filename filepath = os . path . join ( demo_data_path , tball_file ) # Don't re-download if something already exists with same uid if os . path . exists ( filepath ): return filepath res = requests . get ( demo_data_url ) if res . status_code != 200 : log_response_error ( res ) raise CommunicationRetrievalError ( \"couldn't download the demo dataset\" ) os . makedirs ( demo_data_path , exist_ok = True ) open ( filepath , \"wb+\" ) . write ( res . content ) return filepath def get_user_benchmarks ( self ) -> List [ dict ]: \"\"\"Retrieves all benchmarks created by the user Returns: List[dict]: Benchmarks data \"\"\" bmks = self . __get_list ( f \" { self . server_url } /me/benchmarks/\" ) return bmks def get_cubes ( self ) -> List [ dict ]: \"\"\"Retrieves all MLCubes in the platform Returns: List[dict]: List containing the data of all MLCubes \"\"\" cubes = self . __get_list ( f \" { self . server_url } /mlcubes/\" ) return cubes def get_cube_metadata ( self , cube_uid : int ) -> dict : \"\"\"Retrieves metadata about the specified cube Args: cube_uid (int): UID of the desired cube. Returns: dict: Dictionary containing url and hashes for the cube files \"\"\" res = self . __auth_get ( f \" { self . server_url } /mlcubes/ { cube_uid } /\" ) if res . status_code != 200 : log_response_error ( res ) raise CommunicationRetrievalError ( \"the specified cube doesn't exist\" ) return res . json () def get_cube ( self , url : str , cube_uid : int ) -> str : \"\"\"Downloads and writes an mlcube.yaml file from the server Args: url (str): URL where the mlcube.yaml file can be downloaded. cube_uid (int): Cube UID. Returns: str: location where the mlcube.yaml file is stored locally. \"\"\" cube_file = config . cube_filename return self . __get_cube_file ( url , cube_uid , \"\" , cube_file ) def get_user_cubes ( self ) -> List [ dict ]: \"\"\"Retrieves metadata from all cubes registered by the user Returns: List[dict]: List of dictionaries containing the mlcubes registration information \"\"\" cubes = self . __get_list ( f \" { self . server_url } /me/mlcubes/\" ) return cubes def get_cube_params ( self , url : str , cube_uid : int ) -> str : \"\"\"Retrieves the cube parameters.yaml file from the server Args: url (str): URL where the parameters.yaml file can be downloaded. cube_uid (int): Cube UID. Returns: str: Location where the parameters.yaml file is stored locally. \"\"\" ws = config . workspace_path params_file = config . params_filename return self . __get_cube_file ( url , cube_uid , ws , params_file ) def get_cube_additional ( self , url : str , cube_uid : int ) -> str : \"\"\"Retrieves and stores the additional_files.tar.gz file from the server Args: url (str): URL where the additional_files.tar.gz file can be downloaded. cube_uid (int): Cube UID. Returns: str: Location where the additional_files.tar.gz file is stored locally. \"\"\" add_path = config . additional_path tball_file = config . tarball_filename return self . __get_cube_file ( url , cube_uid , add_path , tball_file ) def get_cube_image ( self , url : str , cube_uid : int ) -> str : \"\"\"Retrieves and stores the image file from the server Args: url (str): URL where the image file can be downloaded. cube_uid (int): Cube UID. Returns: str: Location where the image file is stored locally. \"\"\" image_path = config . image_path image_name = url . split ( \"/\" )[ - 1 ] return self . __get_cube_file ( url , cube_uid , image_path , image_name ) def __get_cube_file ( self , url : str , cube_uid : int , path : str , filename : str ): res = requests . get ( url ) if res . status_code != 200 : log_response_error ( res ) msg = \"There was a problem retrieving the specified file at \" + url raise CommunicationRetrievalError ( msg ) else : c_path = cube_path ( cube_uid ) path = os . path . join ( c_path , path ) if not os . path . isdir ( path ): os . makedirs ( path , exist_ok = True ) filepath = os . path . join ( path , filename ) open ( filepath , \"wb+\" ) . write ( res . content ) return filepath def upload_benchmark ( self , benchmark_dict : dict ) -> int : \"\"\"Uploads a new benchmark to the server. Args: benchmark_dict (dict): benchmark_data to be uploaded Returns: int: UID of newly created benchmark \"\"\" res = self . __auth_post ( f \" { self . server_url } /benchmarks/\" , json = benchmark_dict ) if res . status_code != 201 : log_response_error ( res ) raise CommunicationRetrievalError ( \"Could not upload benchmark\" ) return res . json () def upload_mlcube ( self , mlcube_body : dict ) -> int : \"\"\"Uploads an MLCube instance to the platform Args: mlcube_body (dict): Dictionary containing all the relevant data for creating mlcubes Returns: int: id of the created mlcube instance on the platform \"\"\" res = self . __auth_post ( f \" { self . server_url } /mlcubes/\" , json = mlcube_body ) if res . status_code != 201 : log_response_error ( res ) raise CommunicationRetrievalError ( \"Could not upload the mlcube\" ) return res . json () def get_datasets ( self ) -> List [ dict ]: \"\"\"Retrieves all datasets in the platform Returns: List[dict]: List of data from all datasets \"\"\" dsets = self . __get_list ( f \" { self . server_url } /datasets/\" ) return dsets def get_dataset ( self , dset_uid : str ) -> dict : \"\"\"Retrieves a specific dataset Args: dset_uid (str): Dataset UID Returns: dict: Dataset metadata \"\"\" res = self . __auth_get ( f \" { self . server_url } /datasets/ { dset_uid } /\" ) if res . status_code != 200 : log_response_error ( res ) raise CommunicationRetrievalError ( \"Could not retrieve the specified dataset from server\" ) return res . json () def get_user_datasets ( self ) -> dict : \"\"\"Retrieves all datasets registered by the user Returns: dict: dictionary with the contents of each dataset registration query \"\"\" dsets = self . __get_list ( f \" { self . server_url } /me/datasets/\" ) return dsets def upload_dataset ( self , reg_dict : dict ) -> int : \"\"\"Uploads registration data to the server, under the sha name of the file. Args: reg_dict (dict): Dictionary containing registration information. Returns: int: id of the created dataset registration. \"\"\" res = self . __auth_post ( f \" { self . server_url } /datasets/\" , json = reg_dict ) if res . status_code != 201 : log_response_error ( res ) raise CommunicationRequestError ( \"Could not upload the dataset\" ) return res . json () def get_result ( self , result_uid : str ) -> dict : \"\"\"Retrieves a specific result data Args: result_uid (str): Result UID Returns: dict: Result metadata \"\"\" res = self . __auth_get ( f \" { self . server_url } /results/ { result_uid } /\" ) if res . status_code != 200 : log_response_error ( res ) raise CommunicationRetrievalError ( \"Could not retrieve the specified result\" ) return res . json () def get_user_results ( self ) -> dict : \"\"\"Retrieves all results registered by the user Returns: dict: dictionary with the contents of each dataset registration query \"\"\" results = self . __get_list ( f \" { self . server_url } /me/results/\" ) return results def upload_results ( self , results_dict : dict ) -> int : \"\"\"Uploads results to the server. Args: results_dict (dict): Dictionary containing results information. Returns: int: id of the generated results entry \"\"\" res = self . __auth_post ( f \" { self . server_url } /results/\" , json = results_dict ) if res . status_code != 201 : log_response_error ( res ) raise CommunicationRequestError ( \"Could not upload the results\" ) return res . json () def associate_dset ( self , data_uid : int , benchmark_uid : int , metadata : dict = {}): \"\"\"Create a Dataset Benchmark association Args: data_uid (int): Registered dataset UID benchmark_uid (int): Benchmark UID metadata (dict, optional): Additional metadata. Defaults to {}. \"\"\" data = { \"dataset\" : data_uid , \"benchmark\" : benchmark_uid , \"approval_status\" : Status . PENDING . value , \"metadata\" : metadata , } res = self . __auth_post ( f \" { self . server_url } /datasets/benchmarks/\" , json = data ) if res . status_code != 201 : log_response_error ( res ) raise CommunicationRequestError ( \"Could not associate dataset to benchmark\" ) def associate_cube ( self , cube_uid : str , benchmark_uid : int , metadata : dict = {}): \"\"\"Create an MLCube-Benchmark association Args: cube_uid (str): MLCube UID benchmark_uid (int): Benchmark UID metadata (dict, optional): Additional metadata. Defaults to {}. \"\"\" data = { \"approval_status\" : Status . PENDING . value , \"model_mlcube\" : cube_uid , \"benchmark\" : benchmark_uid , \"metadata\" : metadata , } res = self . __auth_post ( f \" { self . server_url } /mlcubes/benchmarks/\" , json = data ) if res . status_code != 201 : log_response_error ( res ) raise CommunicationRequestError ( \"Could not associate mlcube to benchmark\" ) def set_dataset_association_approval ( self , benchmark_uid : str , dataset_uid : str , status : str ): \"\"\"Approves a dataset association Args: dataset_uid (str): Dataset UID benchmark_uid (str): Benchmark UID status (str): Approval status to set for the association \"\"\" url = f \" { self . server_url } /datasets/ { dataset_uid } /benchmarks/ { benchmark_uid } /\" res = self . __set_approval_status ( url , status ) if res . status_code != 200 : log_response_error ( res ) raise CommunicationRequestError ( f \"Could not approve association between dataset { dataset_uid } and benchmark { benchmark_uid } \" ) def set_mlcube_association_approval ( self , benchmark_uid : str , mlcube_uid : str , status : str ): \"\"\"Approves an mlcube association Args: mlcube_uid (str): Dataset UID benchmark_uid (str): Benchmark UID status (str): Approval status to set for the association \"\"\" url = f \" { self . server_url } /mlcubes/ { mlcube_uid } /benchmarks/ { benchmark_uid } /\" res = self . __set_approval_status ( url , status ) if res . status_code != 200 : log_response_error ( res ) raise CommunicationRequestError ( f \"Could not approve association between mlcube { mlcube_uid } and benchmark { benchmark_uid } \" ) def get_datasets_associations ( self ) -> List [ dict ]: \"\"\"Get all dataset associations related to the current user Returns: List[dict]: List containing all associations information \"\"\" assocs = self . __get_list ( f \" { self . server_url } /me/datasets/associations/\" ) return assocs def get_cubes_associations ( self ) -> List [ dict ]: \"\"\"Get all cube associations related to the current user Returns: List[dict]: List containing all associations information \"\"\" assocs = self . __get_list ( f \" { self . server_url } /me/mlcubes/associations/\" ) return assocs __get_list ( url , num_elements = None , page_size = config . default_page_size , offset = 0 , binary_reduction = False ) Retrieves a list of elements from a URL by iterating over pages until num_elements is obtained. If num_elements is None, then iterates until all elements have been retrieved. If binary_reduction is enabled, errors are assumed to be related to response size. In that case, the page_size is reduced by half until a successful response is obtained or until page_size can't be reduced anymore. Parameters: Name Type Description Default url str The url to retrieve elements from required num_elements int The desired number of elements to be retrieved. Defaults to None. None page_size int Starting page size. Defaults to config.default_page_size. config.default_page_size start_limit int The starting position for element retrieval. Defaults to 0. required binary_reduction bool Wether to handle errors by halfing the page size. Defaults to False. False Returns: Type Description List[dict]: A list of dictionaries representing the retrieved elements. Source code in comms/rest.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 def __get_list ( self , url , num_elements = None , page_size = config . default_page_size , offset = 0 , binary_reduction = False , ): \"\"\"Retrieves a list of elements from a URL by iterating over pages until num_elements is obtained. If num_elements is None, then iterates until all elements have been retrieved. If binary_reduction is enabled, errors are assumed to be related to response size. In that case, the page_size is reduced by half until a successful response is obtained or until page_size can't be reduced anymore. Args: url (str): The url to retrieve elements from num_elements (int, optional): The desired number of elements to be retrieved. Defaults to None. page_size (int, optional): Starting page size. Defaults to config.default_page_size. start_limit (int, optional): The starting position for element retrieval. Defaults to 0. binary_reduction (bool, optional): Wether to handle errors by halfing the page size. Defaults to False. Returns: List[dict]: A list of dictionaries representing the retrieved elements. \"\"\" el_list = [] if num_elements is None : num_elements = float ( \"inf\" ) while len ( el_list ) < num_elements : paginated_url = f \" { url } ?limit= { page_size } &offset= { offset } \" res = self . __auth_get ( paginated_url ) if res . status_code != 200 : if not binary_reduction : log_response_error ( res ) raise CommunicationRetrievalError ( \"there was an error retrieving the current list.\" ) log_response_error ( res , warn = True ) if page_size <= 1 : raise CommunicationRetrievalError ( \"Could not retrieve list. Minimum page size achieved without success.\" ) page_size = page_size // 2 continue else : data = res . json () el_list += data [ \"results\" ] offset += len ( data [ \"results\" ]) if data [ \"next\" ] is None : break if type ( num_elements ) == int : return el_list [: num_elements ] return el_list __set_approval_status ( url , status ) Sets the approval status of a resource Parameters: Name Type Description Default url str URL to the resource to update required status str approval status to set required Returns: Type Description requests . Response requests.Response: Response object returned by the update Source code in comms/rest.py 185 186 187 188 189 190 191 192 193 194 195 196 197 def __set_approval_status ( self , url : str , status : str ) -> requests . Response : \"\"\"Sets the approval status of a resource Args: url (str): URL to the resource to update status (str): approval status to set Returns: requests.Response: Response object returned by the update \"\"\" data = { \"approval_status\" : status } res = self . __auth_put ( url , json = data ,) return res associate_cube ( cube_uid , benchmark_uid , metadata = {}) Create an MLCube-Benchmark association Parameters: Name Type Description Default cube_uid str MLCube UID required benchmark_uid int Benchmark UID required metadata dict Additional metadata. Defaults to {}. {} Source code in comms/rest.py 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 def associate_cube ( self , cube_uid : str , benchmark_uid : int , metadata : dict = {}): \"\"\"Create an MLCube-Benchmark association Args: cube_uid (str): MLCube UID benchmark_uid (int): Benchmark UID metadata (dict, optional): Additional metadata. Defaults to {}. \"\"\" data = { \"approval_status\" : Status . PENDING . value , \"model_mlcube\" : cube_uid , \"benchmark\" : benchmark_uid , \"metadata\" : metadata , } res = self . __auth_post ( f \" { self . server_url } /mlcubes/benchmarks/\" , json = data ) if res . status_code != 201 : log_response_error ( res ) raise CommunicationRequestError ( \"Could not associate mlcube to benchmark\" ) associate_dset ( data_uid , benchmark_uid , metadata = {}) Create a Dataset Benchmark association Parameters: Name Type Description Default data_uid int Registered dataset UID required benchmark_uid int Benchmark UID required metadata dict Additional metadata. Defaults to {}. {} Source code in comms/rest.py 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 def associate_dset ( self , data_uid : int , benchmark_uid : int , metadata : dict = {}): \"\"\"Create a Dataset Benchmark association Args: data_uid (int): Registered dataset UID benchmark_uid (int): Benchmark UID metadata (dict, optional): Additional metadata. Defaults to {}. \"\"\" data = { \"dataset\" : data_uid , \"benchmark\" : benchmark_uid , \"approval_status\" : Status . PENDING . value , \"metadata\" : metadata , } res = self . __auth_post ( f \" { self . server_url } /datasets/benchmarks/\" , json = data ) if res . status_code != 201 : log_response_error ( res ) raise CommunicationRequestError ( \"Could not associate dataset to benchmark\" ) authorized_by_role ( benchmark_uid , role ) Indicates wether the current user is authorized to access a benchmark based on desired role Parameters: Name Type Description Default benchmark_uid int UID of the benchmark required role str Desired role to check for authorization required Returns: Name Type Description bool bool Wether the user has the specified role for that benchmark Source code in comms/rest.py 215 216 217 218 219 220 221 222 223 224 225 226 227 def authorized_by_role ( self , benchmark_uid : int , role : str ) -> bool : \"\"\"Indicates wether the current user is authorized to access a benchmark based on desired role Args: benchmark_uid (int): UID of the benchmark role (str): Desired role to check for authorization Returns: bool: Wether the user has the specified role for that benchmark \"\"\" assoc_role = self . benchmark_association ( benchmark_uid ) return assoc_role . name == role benchmark_association ( benchmark_uid ) Retrieves the benchmark association Parameters: Name Type Description Default benchmark_uid int UID of the benchmark required Returns: Name Type Description Role Role the association type between current user and benchmark Source code in comms/rest.py 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 def benchmark_association ( self , benchmark_uid : int ) -> Role : \"\"\"Retrieves the benchmark association Args: benchmark_uid (int): UID of the benchmark Returns: Role: the association type between current user and benchmark \"\"\" benchmarks = self . __get_list ( f \" { self . server_url } /me/benchmarks\" ) bm_dict = { bm [ \"benchmark\" ]: bm for bm in benchmarks } rolename = None if benchmark_uid in bm_dict : rolename = bm_dict [ benchmark_uid ][ \"role\" ] return Role ( rolename ) change_password ( pwd ) Sets a new password for the current user. Parameters: Name Type Description Default pwd str New password to be set required ui UI Instance of an implementation required Returns: Name Type Description bool bool Whether changing the password was successful or not Source code in comms/rest.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def change_password ( self , pwd : str ) -> bool : \"\"\"Sets a new password for the current user. Args: pwd (str): New password to be set ui (UI): Instance of an implementation Returns: bool: Whether changing the password was successful or not \"\"\" body = { \"password\" : pwd } res = self . __auth_post ( f \" { self . server_url } /me/password/\" , json = body ) if res . status_code != 200 : log_response_error ( res ) raise CommunicationRequestError ( \"Unable to change the current password\" ) return True get_benchmark ( benchmark_uid ) Retrieves the benchmark specification file from the server Parameters: Name Type Description Default benchmark_uid int uid for the desired benchmark required Returns: Name Type Description dict dict benchmark specification Source code in comms/rest.py 238 239 240 241 242 243 244 245 246 247 248 249 250 251 def get_benchmark ( self , benchmark_uid : int ) -> dict : \"\"\"Retrieves the benchmark specification file from the server Args: benchmark_uid (int): uid for the desired benchmark Returns: dict: benchmark specification \"\"\" res = self . __auth_get ( f \" { self . server_url } /benchmarks/ { benchmark_uid } \" ) if res . status_code != 200 : log_response_error ( res ) raise CommunicationRetrievalError ( \"the specified benchmark doesn't exist\" ) return res . json () get_benchmark_demo_dataset ( demo_data_url , uid = generate_tmp_uid ()) Downloads the benchmark demo dataset and stores it in the user's machine Parameters: Name Type Description Default demo_data_url str location of demo data for download required uid str UID to use for storing the demo dataset. Defaults to generate_tmp_uid(). generate_tmp_uid() Returns: Name Type Description str str path where the downloaded demo dataset can be found Source code in comms/rest.py 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 def get_benchmark_demo_dataset ( self , demo_data_url : str , uid : str = generate_tmp_uid () ) -> str : \"\"\"Downloads the benchmark demo dataset and stores it in the user's machine Args: demo_data_url (str): location of demo data for download uid (str): UID to use for storing the demo dataset. Defaults to generate_tmp_uid(). Returns: str: path where the downloaded demo dataset can be found \"\"\" tmp_dir = storage_path ( config . demo_data_storage ) demo_data_path = os . path . join ( tmp_dir , uid ) tball_file = config . tarball_filename filepath = os . path . join ( demo_data_path , tball_file ) # Don't re-download if something already exists with same uid if os . path . exists ( filepath ): return filepath res = requests . get ( demo_data_url ) if res . status_code != 200 : log_response_error ( res ) raise CommunicationRetrievalError ( \"couldn't download the demo dataset\" ) os . makedirs ( demo_data_path , exist_ok = True ) open ( filepath , \"wb+\" ) . write ( res . content ) return filepath get_benchmark_models ( benchmark_uid ) Retrieves all the models associated with a benchmark. reference model not included Parameters: Name Type Description Default benchmark_uid int UID of the desired benchmark required Returns: Type Description List [ int ] list[int]: List of model UIDS Source code in comms/rest.py 253 254 255 256 257 258 259 260 261 262 263 264 def get_benchmark_models ( self , benchmark_uid : int ) -> List [ int ]: \"\"\"Retrieves all the models associated with a benchmark. reference model not included Args: benchmark_uid (int): UID of the desired benchmark Returns: list[int]: List of model UIDS \"\"\" models = self . __get_list ( f \" { self . server_url } /benchmarks/ { benchmark_uid } /models\" ) model_uids = [ model [ \"id\" ] for model in models ] return model_uids get_benchmarks () Retrieves all benchmarks in the platform. Returns: Type Description List [ dict ] List[dict]: all benchmarks information. Source code in comms/rest.py 229 230 231 232 233 234 235 236 def get_benchmarks ( self ) -> List [ dict ]: \"\"\"Retrieves all benchmarks in the platform. Returns: List[dict]: all benchmarks information. \"\"\" bmks = self . __get_list ( f \" { self . server_url } /benchmarks/\" ) return bmks get_cube ( url , cube_uid ) Downloads and writes an mlcube.yaml file from the server Parameters: Name Type Description Default url str URL where the mlcube.yaml file can be downloaded. required cube_uid int Cube UID. required Returns: Name Type Description str str location where the mlcube.yaml file is stored locally. Source code in comms/rest.py 330 331 332 333 334 335 336 337 338 339 340 341 def get_cube ( self , url : str , cube_uid : int ) -> str : \"\"\"Downloads and writes an mlcube.yaml file from the server Args: url (str): URL where the mlcube.yaml file can be downloaded. cube_uid (int): Cube UID. Returns: str: location where the mlcube.yaml file is stored locally. \"\"\" cube_file = config . cube_filename return self . __get_cube_file ( url , cube_uid , \"\" , cube_file ) get_cube_additional ( url , cube_uid ) Retrieves and stores the additional_files.tar.gz file from the server Parameters: Name Type Description Default url str URL where the additional_files.tar.gz file can be downloaded. required cube_uid int Cube UID. required Returns: Name Type Description str str Location where the additional_files.tar.gz file is stored locally. Source code in comms/rest.py 366 367 368 369 370 371 372 373 374 375 376 377 378 def get_cube_additional ( self , url : str , cube_uid : int ) -> str : \"\"\"Retrieves and stores the additional_files.tar.gz file from the server Args: url (str): URL where the additional_files.tar.gz file can be downloaded. cube_uid (int): Cube UID. Returns: str: Location where the additional_files.tar.gz file is stored locally. \"\"\" add_path = config . additional_path tball_file = config . tarball_filename return self . __get_cube_file ( url , cube_uid , add_path , tball_file ) get_cube_image ( url , cube_uid ) Retrieves and stores the image file from the server Parameters: Name Type Description Default url str URL where the image file can be downloaded. required cube_uid int Cube UID. required Returns: Name Type Description str str Location where the image file is stored locally. Source code in comms/rest.py 380 381 382 383 384 385 386 387 388 389 390 391 392 def get_cube_image ( self , url : str , cube_uid : int ) -> str : \"\"\"Retrieves and stores the image file from the server Args: url (str): URL where the image file can be downloaded. cube_uid (int): Cube UID. Returns: str: Location where the image file is stored locally. \"\"\" image_path = config . image_path image_name = url . split ( \"/\" )[ - 1 ] return self . __get_cube_file ( url , cube_uid , image_path , image_name ) get_cube_metadata ( cube_uid ) Retrieves metadata about the specified cube Parameters: Name Type Description Default cube_uid int UID of the desired cube. required Returns: Name Type Description dict dict Dictionary containing url and hashes for the cube files Source code in comms/rest.py 315 316 317 318 319 320 321 322 323 324 325 326 327 328 def get_cube_metadata ( self , cube_uid : int ) -> dict : \"\"\"Retrieves metadata about the specified cube Args: cube_uid (int): UID of the desired cube. Returns: dict: Dictionary containing url and hashes for the cube files \"\"\" res = self . __auth_get ( f \" { self . server_url } /mlcubes/ { cube_uid } /\" ) if res . status_code != 200 : log_response_error ( res ) raise CommunicationRetrievalError ( \"the specified cube doesn't exist\" ) return res . json () get_cube_params ( url , cube_uid ) Retrieves the cube parameters.yaml file from the server Parameters: Name Type Description Default url str URL where the parameters.yaml file can be downloaded. required cube_uid int Cube UID. required Returns: Name Type Description str str Location where the parameters.yaml file is stored locally. Source code in comms/rest.py 352 353 354 355 356 357 358 359 360 361 362 363 364 def get_cube_params ( self , url : str , cube_uid : int ) -> str : \"\"\"Retrieves the cube parameters.yaml file from the server Args: url (str): URL where the parameters.yaml file can be downloaded. cube_uid (int): Cube UID. Returns: str: Location where the parameters.yaml file is stored locally. \"\"\" ws = config . workspace_path params_file = config . params_filename return self . __get_cube_file ( url , cube_uid , ws , params_file ) get_cubes () Retrieves all MLCubes in the platform Returns: Type Description List [ dict ] List[dict]: List containing the data of all MLCubes Source code in comms/rest.py 306 307 308 309 310 311 312 313 def get_cubes ( self ) -> List [ dict ]: \"\"\"Retrieves all MLCubes in the platform Returns: List[dict]: List containing the data of all MLCubes \"\"\" cubes = self . __get_list ( f \" { self . server_url } /mlcubes/\" ) return cubes get_cubes_associations () Get all cube associations related to the current user Returns: Type Description List [ dict ] List[dict]: List containing all associations information Source code in comms/rest.py 611 612 613 614 615 616 617 618 def get_cubes_associations ( self ) -> List [ dict ]: \"\"\"Get all cube associations related to the current user Returns: List[dict]: List containing all associations information \"\"\" assocs = self . __get_list ( f \" { self . server_url } /me/mlcubes/associations/\" ) return assocs get_dataset ( dset_uid ) Retrieves a specific dataset Parameters: Name Type Description Default dset_uid str Dataset UID required Returns: Name Type Description dict dict Dataset metadata Source code in comms/rest.py 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 def get_dataset ( self , dset_uid : str ) -> dict : \"\"\"Retrieves a specific dataset Args: dset_uid (str): Dataset UID Returns: dict: Dataset metadata \"\"\" res = self . __auth_get ( f \" { self . server_url } /datasets/ { dset_uid } /\" ) if res . status_code != 200 : log_response_error ( res ) raise CommunicationRetrievalError ( \"Could not retrieve the specified dataset from server\" ) return res . json () get_datasets () Retrieves all datasets in the platform Returns: Type Description List [ dict ] List[dict]: List of data from all datasets Source code in comms/rest.py 439 440 441 442 443 444 445 446 def get_datasets ( self ) -> List [ dict ]: \"\"\"Retrieves all datasets in the platform Returns: List[dict]: List of data from all datasets \"\"\" dsets = self . __get_list ( f \" { self . server_url } /datasets/\" ) return dsets get_datasets_associations () Get all dataset associations related to the current user Returns: Type Description List [ dict ] List[dict]: List containing all associations information Source code in comms/rest.py 602 603 604 605 606 607 608 609 def get_datasets_associations ( self ) -> List [ dict ]: \"\"\"Get all dataset associations related to the current user Returns: List[dict]: List containing all associations information \"\"\" assocs = self . __get_list ( f \" { self . server_url } /me/datasets/associations/\" ) return assocs get_result ( result_uid ) Retrieves a specific result data Parameters: Name Type Description Default result_uid str Result UID required Returns: Name Type Description dict dict Result metadata Source code in comms/rest.py 489 490 491 492 493 494 495 496 497 498 499 500 501 502 def get_result ( self , result_uid : str ) -> dict : \"\"\"Retrieves a specific result data Args: result_uid (str): Result UID Returns: dict: Result metadata \"\"\" res = self . __auth_get ( f \" { self . server_url } /results/ { result_uid } /\" ) if res . status_code != 200 : log_response_error ( res ) raise CommunicationRetrievalError ( \"Could not retrieve the specified result\" ) return res . json () get_user_benchmarks () Retrieves all benchmarks created by the user Returns: Type Description List [ dict ] List[dict]: Benchmarks data Source code in comms/rest.py 297 298 299 300 301 302 303 304 def get_user_benchmarks ( self ) -> List [ dict ]: \"\"\"Retrieves all benchmarks created by the user Returns: List[dict]: Benchmarks data \"\"\" bmks = self . __get_list ( f \" { self . server_url } /me/benchmarks/\" ) return bmks get_user_cubes () Retrieves metadata from all cubes registered by the user Returns: Type Description List [ dict ] List[dict]: List of dictionaries containing the mlcubes registration information Source code in comms/rest.py 343 344 345 346 347 348 349 350 def get_user_cubes ( self ) -> List [ dict ]: \"\"\"Retrieves metadata from all cubes registered by the user Returns: List[dict]: List of dictionaries containing the mlcubes registration information \"\"\" cubes = self . __get_list ( f \" { self . server_url } /me/mlcubes/\" ) return cubes get_user_datasets () Retrieves all datasets registered by the user Returns: Name Type Description dict dict dictionary with the contents of each dataset registration query Source code in comms/rest.py 465 466 467 468 469 470 471 472 def get_user_datasets ( self ) -> dict : \"\"\"Retrieves all datasets registered by the user Returns: dict: dictionary with the contents of each dataset registration query \"\"\" dsets = self . __get_list ( f \" { self . server_url } /me/datasets/\" ) return dsets get_user_results () Retrieves all results registered by the user Returns: Name Type Description dict dict dictionary with the contents of each dataset registration query Source code in comms/rest.py 504 505 506 507 508 509 510 511 def get_user_results ( self ) -> dict : \"\"\"Retrieves all results registered by the user Returns: dict: dictionary with the contents of each dataset registration query \"\"\" results = self . __get_list ( f \" { self . server_url } /me/results/\" ) return results login ( user , pwd ) Authenticates the user with the server. Required for most endpoints Parameters: Name Type Description Default ui UI Instance of an implementation of the UI interface required user str Username required pwd str Password required Source code in comms/rest.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def login ( self , user : str , pwd : str ): \"\"\"Authenticates the user with the server. Required for most endpoints Args: ui (UI): Instance of an implementation of the UI interface user (str): Username pwd (str): Password \"\"\" body = { \"username\" : user , \"password\" : pwd } res = self . __req ( f \" { self . server_url } /auth-token/\" , requests . post , json = body ) if res . status_code != 200 : log_response_error ( res ) CommunicationAuthenticationError ( \"Unable to authenticate user with provided credentials\" ) else : self . token = res . json ()[ \"token\" ] set_dataset_association_approval ( benchmark_uid , dataset_uid , status ) Approves a dataset association Parameters: Name Type Description Default dataset_uid str Dataset UID required benchmark_uid str Benchmark UID required status str Approval status to set for the association required Source code in comms/rest.py 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def set_dataset_association_approval ( self , benchmark_uid : str , dataset_uid : str , status : str ): \"\"\"Approves a dataset association Args: dataset_uid (str): Dataset UID benchmark_uid (str): Benchmark UID status (str): Approval status to set for the association \"\"\" url = f \" { self . server_url } /datasets/ { dataset_uid } /benchmarks/ { benchmark_uid } /\" res = self . __set_approval_status ( url , status ) if res . status_code != 200 : log_response_error ( res ) raise CommunicationRequestError ( f \"Could not approve association between dataset { dataset_uid } and benchmark { benchmark_uid } \" ) set_mlcube_association_approval ( benchmark_uid , mlcube_uid , status ) Approves an mlcube association Parameters: Name Type Description Default mlcube_uid str Dataset UID required benchmark_uid str Benchmark UID required status str Approval status to set for the association required Source code in comms/rest.py 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 def set_mlcube_association_approval ( self , benchmark_uid : str , mlcube_uid : str , status : str ): \"\"\"Approves an mlcube association Args: mlcube_uid (str): Dataset UID benchmark_uid (str): Benchmark UID status (str): Approval status to set for the association \"\"\" url = f \" { self . server_url } /mlcubes/ { mlcube_uid } /benchmarks/ { benchmark_uid } /\" res = self . __set_approval_status ( url , status ) if res . status_code != 200 : log_response_error ( res ) raise CommunicationRequestError ( f \"Could not approve association between mlcube { mlcube_uid } and benchmark { benchmark_uid } \" ) upload_benchmark ( benchmark_dict ) Uploads a new benchmark to the server. Parameters: Name Type Description Default benchmark_dict dict benchmark_data to be uploaded required Returns: Name Type Description int int UID of newly created benchmark Source code in comms/rest.py 409 410 411 412 413 414 415 416 417 418 419 420 421 422 def upload_benchmark ( self , benchmark_dict : dict ) -> int : \"\"\"Uploads a new benchmark to the server. Args: benchmark_dict (dict): benchmark_data to be uploaded Returns: int: UID of newly created benchmark \"\"\" res = self . __auth_post ( f \" { self . server_url } /benchmarks/\" , json = benchmark_dict ) if res . status_code != 201 : log_response_error ( res ) raise CommunicationRetrievalError ( \"Could not upload benchmark\" ) return res . json () upload_dataset ( reg_dict ) Uploads registration data to the server, under the sha name of the file. Parameters: Name Type Description Default reg_dict dict Dictionary containing registration information. required Returns: Name Type Description int int id of the created dataset registration. Source code in comms/rest.py 474 475 476 477 478 479 480 481 482 483 484 485 486 487 def upload_dataset ( self , reg_dict : dict ) -> int : \"\"\"Uploads registration data to the server, under the sha name of the file. Args: reg_dict (dict): Dictionary containing registration information. Returns: int: id of the created dataset registration. \"\"\" res = self . __auth_post ( f \" { self . server_url } /datasets/\" , json = reg_dict ) if res . status_code != 201 : log_response_error ( res ) raise CommunicationRequestError ( \"Could not upload the dataset\" ) return res . json () upload_mlcube ( mlcube_body ) Uploads an MLCube instance to the platform Parameters: Name Type Description Default mlcube_body dict Dictionary containing all the relevant data for creating mlcubes required Returns: Name Type Description int int id of the created mlcube instance on the platform Source code in comms/rest.py 424 425 426 427 428 429 430 431 432 433 434 435 436 437 def upload_mlcube ( self , mlcube_body : dict ) -> int : \"\"\"Uploads an MLCube instance to the platform Args: mlcube_body (dict): Dictionary containing all the relevant data for creating mlcubes Returns: int: id of the created mlcube instance on the platform \"\"\" res = self . __auth_post ( f \" { self . server_url } /mlcubes/\" , json = mlcube_body ) if res . status_code != 201 : log_response_error ( res ) raise CommunicationRetrievalError ( \"Could not upload the mlcube\" ) return res . json () upload_results ( results_dict ) Uploads results to the server. Parameters: Name Type Description Default results_dict dict Dictionary containing results information. required Returns: Name Type Description int int id of the generated results entry Source code in comms/rest.py 513 514 515 516 517 518 519 520 521 522 523 524 525 526 def upload_results ( self , results_dict : dict ) -> int : \"\"\"Uploads results to the server. Args: results_dict (dict): Dictionary containing results information. Returns: int: id of the generated results entry \"\"\" res = self . __auth_post ( f \" { self . server_url } /results/\" , json = results_dict ) if res . status_code != 201 : log_response_error ( res ) raise CommunicationRequestError ( \"Could not upload the results\" ) return res . json ()","title":"rest"},{"location":"reference/comms/rest/#comms.rest.REST","text":"Bases: Comms Source code in comms/rest.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 class REST ( Comms ): def __init__ ( self , source : str , token = None ): self . server_url = self . __parse_url ( source ) self . token = token self . cert = config . certificate if self . cert is None : # No certificate provided, default to normal verification self . cert = True def __parse_url ( self , url ): url_sections = url . split ( \"://\" ) # Remove protocol if passed if len ( url_sections ) > 1 : url = \"\" . join ( url_sections [ 1 :]) return f \"https:// { url } \" def login ( self , user : str , pwd : str ): \"\"\"Authenticates the user with the server. Required for most endpoints Args: ui (UI): Instance of an implementation of the UI interface user (str): Username pwd (str): Password \"\"\" body = { \"username\" : user , \"password\" : pwd } res = self . __req ( f \" { self . server_url } /auth-token/\" , requests . post , json = body ) if res . status_code != 200 : log_response_error ( res ) CommunicationAuthenticationError ( \"Unable to authenticate user with provided credentials\" ) else : self . token = res . json ()[ \"token\" ] def change_password ( self , pwd : str ) -> bool : \"\"\"Sets a new password for the current user. Args: pwd (str): New password to be set ui (UI): Instance of an implementation Returns: bool: Whether changing the password was successful or not \"\"\" body = { \"password\" : pwd } res = self . __auth_post ( f \" { self . server_url } /me/password/\" , json = body ) if res . status_code != 200 : log_response_error ( res ) raise CommunicationRequestError ( \"Unable to change the current password\" ) return True def authenticate ( self ): cred_path = storage_path ( config . credentials_path ) if os . path . exists ( cred_path ): with open ( cred_path ) as f : self . token = f . readline () else : raise CommunicationAuthenticationError ( \"Couldn't find credentials file. Did you run 'medperf login' before?\" ) def __auth_get ( self , url , ** kwargs ): return self . __auth_req ( url , requests . get , ** kwargs ) def __auth_post ( self , url , ** kwargs ): return self . __auth_req ( url , requests . post , ** kwargs ) def __auth_put ( self , url , ** kwargs ): return self . __auth_req ( url , requests . put , ** kwargs ) def __auth_req ( self , url , req_func , ** kwargs ): if self . token is None : self . authenticate () return self . __req ( url , req_func , headers = { \"Authorization\" : f \"Token { self . token } \" }, ** kwargs ) def __req ( self , url , req_func , ** kwargs ): logging . debug ( f \"Calling { req_func } : { url } \" ) if \"json\" in kwargs : logging . debug ( f \"Passing JSON contents: { kwargs [ 'json' ] } \" ) kwargs [ \"json\" ] = sanitize_json ( kwargs [ \"json\" ]) try : return req_func ( url , verify = self . cert , ** kwargs ) except requests . exceptions . SSLError as e : logging . error ( f \"Couldn't connect to { self . server_url } : { e } \" ) raise requests . exceptions . SSLError ( \"Couldn't connect to server through HTTPS. If running locally, \" \"remember to provide the server certificate through --certificate\" ) def __get_list ( self , url , num_elements = None , page_size = config . default_page_size , offset = 0 , binary_reduction = False , ): \"\"\"Retrieves a list of elements from a URL by iterating over pages until num_elements is obtained. If num_elements is None, then iterates until all elements have been retrieved. If binary_reduction is enabled, errors are assumed to be related to response size. In that case, the page_size is reduced by half until a successful response is obtained or until page_size can't be reduced anymore. Args: url (str): The url to retrieve elements from num_elements (int, optional): The desired number of elements to be retrieved. Defaults to None. page_size (int, optional): Starting page size. Defaults to config.default_page_size. start_limit (int, optional): The starting position for element retrieval. Defaults to 0. binary_reduction (bool, optional): Wether to handle errors by halfing the page size. Defaults to False. Returns: List[dict]: A list of dictionaries representing the retrieved elements. \"\"\" el_list = [] if num_elements is None : num_elements = float ( \"inf\" ) while len ( el_list ) < num_elements : paginated_url = f \" { url } ?limit= { page_size } &offset= { offset } \" res = self . __auth_get ( paginated_url ) if res . status_code != 200 : if not binary_reduction : log_response_error ( res ) raise CommunicationRetrievalError ( \"there was an error retrieving the current list.\" ) log_response_error ( res , warn = True ) if page_size <= 1 : raise CommunicationRetrievalError ( \"Could not retrieve list. Minimum page size achieved without success.\" ) page_size = page_size // 2 continue else : data = res . json () el_list += data [ \"results\" ] offset += len ( data [ \"results\" ]) if data [ \"next\" ] is None : break if type ( num_elements ) == int : return el_list [: num_elements ] return el_list def __set_approval_status ( self , url : str , status : str ) -> requests . Response : \"\"\"Sets the approval status of a resource Args: url (str): URL to the resource to update status (str): approval status to set Returns: requests.Response: Response object returned by the update \"\"\" data = { \"approval_status\" : status } res = self . __auth_put ( url , json = data ,) return res def benchmark_association ( self , benchmark_uid : int ) -> Role : \"\"\"Retrieves the benchmark association Args: benchmark_uid (int): UID of the benchmark Returns: Role: the association type between current user and benchmark \"\"\" benchmarks = self . __get_list ( f \" { self . server_url } /me/benchmarks\" ) bm_dict = { bm [ \"benchmark\" ]: bm for bm in benchmarks } rolename = None if benchmark_uid in bm_dict : rolename = bm_dict [ benchmark_uid ][ \"role\" ] return Role ( rolename ) def authorized_by_role ( self , benchmark_uid : int , role : str ) -> bool : \"\"\"Indicates wether the current user is authorized to access a benchmark based on desired role Args: benchmark_uid (int): UID of the benchmark role (str): Desired role to check for authorization Returns: bool: Wether the user has the specified role for that benchmark \"\"\" assoc_role = self . benchmark_association ( benchmark_uid ) return assoc_role . name == role def get_benchmarks ( self ) -> List [ dict ]: \"\"\"Retrieves all benchmarks in the platform. Returns: List[dict]: all benchmarks information. \"\"\" bmks = self . __get_list ( f \" { self . server_url } /benchmarks/\" ) return bmks def get_benchmark ( self , benchmark_uid : int ) -> dict : \"\"\"Retrieves the benchmark specification file from the server Args: benchmark_uid (int): uid for the desired benchmark Returns: dict: benchmark specification \"\"\" res = self . __auth_get ( f \" { self . server_url } /benchmarks/ { benchmark_uid } \" ) if res . status_code != 200 : log_response_error ( res ) raise CommunicationRetrievalError ( \"the specified benchmark doesn't exist\" ) return res . json () def get_benchmark_models ( self , benchmark_uid : int ) -> List [ int ]: \"\"\"Retrieves all the models associated with a benchmark. reference model not included Args: benchmark_uid (int): UID of the desired benchmark Returns: list[int]: List of model UIDS \"\"\" models = self . __get_list ( f \" { self . server_url } /benchmarks/ { benchmark_uid } /models\" ) model_uids = [ model [ \"id\" ] for model in models ] return model_uids def get_benchmark_demo_dataset ( self , demo_data_url : str , uid : str = generate_tmp_uid () ) -> str : \"\"\"Downloads the benchmark demo dataset and stores it in the user's machine Args: demo_data_url (str): location of demo data for download uid (str): UID to use for storing the demo dataset. Defaults to generate_tmp_uid(). Returns: str: path where the downloaded demo dataset can be found \"\"\" tmp_dir = storage_path ( config . demo_data_storage ) demo_data_path = os . path . join ( tmp_dir , uid ) tball_file = config . tarball_filename filepath = os . path . join ( demo_data_path , tball_file ) # Don't re-download if something already exists with same uid if os . path . exists ( filepath ): return filepath res = requests . get ( demo_data_url ) if res . status_code != 200 : log_response_error ( res ) raise CommunicationRetrievalError ( \"couldn't download the demo dataset\" ) os . makedirs ( demo_data_path , exist_ok = True ) open ( filepath , \"wb+\" ) . write ( res . content ) return filepath def get_user_benchmarks ( self ) -> List [ dict ]: \"\"\"Retrieves all benchmarks created by the user Returns: List[dict]: Benchmarks data \"\"\" bmks = self . __get_list ( f \" { self . server_url } /me/benchmarks/\" ) return bmks def get_cubes ( self ) -> List [ dict ]: \"\"\"Retrieves all MLCubes in the platform Returns: List[dict]: List containing the data of all MLCubes \"\"\" cubes = self . __get_list ( f \" { self . server_url } /mlcubes/\" ) return cubes def get_cube_metadata ( self , cube_uid : int ) -> dict : \"\"\"Retrieves metadata about the specified cube Args: cube_uid (int): UID of the desired cube. Returns: dict: Dictionary containing url and hashes for the cube files \"\"\" res = self . __auth_get ( f \" { self . server_url } /mlcubes/ { cube_uid } /\" ) if res . status_code != 200 : log_response_error ( res ) raise CommunicationRetrievalError ( \"the specified cube doesn't exist\" ) return res . json () def get_cube ( self , url : str , cube_uid : int ) -> str : \"\"\"Downloads and writes an mlcube.yaml file from the server Args: url (str): URL where the mlcube.yaml file can be downloaded. cube_uid (int): Cube UID. Returns: str: location where the mlcube.yaml file is stored locally. \"\"\" cube_file = config . cube_filename return self . __get_cube_file ( url , cube_uid , \"\" , cube_file ) def get_user_cubes ( self ) -> List [ dict ]: \"\"\"Retrieves metadata from all cubes registered by the user Returns: List[dict]: List of dictionaries containing the mlcubes registration information \"\"\" cubes = self . __get_list ( f \" { self . server_url } /me/mlcubes/\" ) return cubes def get_cube_params ( self , url : str , cube_uid : int ) -> str : \"\"\"Retrieves the cube parameters.yaml file from the server Args: url (str): URL where the parameters.yaml file can be downloaded. cube_uid (int): Cube UID. Returns: str: Location where the parameters.yaml file is stored locally. \"\"\" ws = config . workspace_path params_file = config . params_filename return self . __get_cube_file ( url , cube_uid , ws , params_file ) def get_cube_additional ( self , url : str , cube_uid : int ) -> str : \"\"\"Retrieves and stores the additional_files.tar.gz file from the server Args: url (str): URL where the additional_files.tar.gz file can be downloaded. cube_uid (int): Cube UID. Returns: str: Location where the additional_files.tar.gz file is stored locally. \"\"\" add_path = config . additional_path tball_file = config . tarball_filename return self . __get_cube_file ( url , cube_uid , add_path , tball_file ) def get_cube_image ( self , url : str , cube_uid : int ) -> str : \"\"\"Retrieves and stores the image file from the server Args: url (str): URL where the image file can be downloaded. cube_uid (int): Cube UID. Returns: str: Location where the image file is stored locally. \"\"\" image_path = config . image_path image_name = url . split ( \"/\" )[ - 1 ] return self . __get_cube_file ( url , cube_uid , image_path , image_name ) def __get_cube_file ( self , url : str , cube_uid : int , path : str , filename : str ): res = requests . get ( url ) if res . status_code != 200 : log_response_error ( res ) msg = \"There was a problem retrieving the specified file at \" + url raise CommunicationRetrievalError ( msg ) else : c_path = cube_path ( cube_uid ) path = os . path . join ( c_path , path ) if not os . path . isdir ( path ): os . makedirs ( path , exist_ok = True ) filepath = os . path . join ( path , filename ) open ( filepath , \"wb+\" ) . write ( res . content ) return filepath def upload_benchmark ( self , benchmark_dict : dict ) -> int : \"\"\"Uploads a new benchmark to the server. Args: benchmark_dict (dict): benchmark_data to be uploaded Returns: int: UID of newly created benchmark \"\"\" res = self . __auth_post ( f \" { self . server_url } /benchmarks/\" , json = benchmark_dict ) if res . status_code != 201 : log_response_error ( res ) raise CommunicationRetrievalError ( \"Could not upload benchmark\" ) return res . json () def upload_mlcube ( self , mlcube_body : dict ) -> int : \"\"\"Uploads an MLCube instance to the platform Args: mlcube_body (dict): Dictionary containing all the relevant data for creating mlcubes Returns: int: id of the created mlcube instance on the platform \"\"\" res = self . __auth_post ( f \" { self . server_url } /mlcubes/\" , json = mlcube_body ) if res . status_code != 201 : log_response_error ( res ) raise CommunicationRetrievalError ( \"Could not upload the mlcube\" ) return res . json () def get_datasets ( self ) -> List [ dict ]: \"\"\"Retrieves all datasets in the platform Returns: List[dict]: List of data from all datasets \"\"\" dsets = self . __get_list ( f \" { self . server_url } /datasets/\" ) return dsets def get_dataset ( self , dset_uid : str ) -> dict : \"\"\"Retrieves a specific dataset Args: dset_uid (str): Dataset UID Returns: dict: Dataset metadata \"\"\" res = self . __auth_get ( f \" { self . server_url } /datasets/ { dset_uid } /\" ) if res . status_code != 200 : log_response_error ( res ) raise CommunicationRetrievalError ( \"Could not retrieve the specified dataset from server\" ) return res . json () def get_user_datasets ( self ) -> dict : \"\"\"Retrieves all datasets registered by the user Returns: dict: dictionary with the contents of each dataset registration query \"\"\" dsets = self . __get_list ( f \" { self . server_url } /me/datasets/\" ) return dsets def upload_dataset ( self , reg_dict : dict ) -> int : \"\"\"Uploads registration data to the server, under the sha name of the file. Args: reg_dict (dict): Dictionary containing registration information. Returns: int: id of the created dataset registration. \"\"\" res = self . __auth_post ( f \" { self . server_url } /datasets/\" , json = reg_dict ) if res . status_code != 201 : log_response_error ( res ) raise CommunicationRequestError ( \"Could not upload the dataset\" ) return res . json () def get_result ( self , result_uid : str ) -> dict : \"\"\"Retrieves a specific result data Args: result_uid (str): Result UID Returns: dict: Result metadata \"\"\" res = self . __auth_get ( f \" { self . server_url } /results/ { result_uid } /\" ) if res . status_code != 200 : log_response_error ( res ) raise CommunicationRetrievalError ( \"Could not retrieve the specified result\" ) return res . json () def get_user_results ( self ) -> dict : \"\"\"Retrieves all results registered by the user Returns: dict: dictionary with the contents of each dataset registration query \"\"\" results = self . __get_list ( f \" { self . server_url } /me/results/\" ) return results def upload_results ( self , results_dict : dict ) -> int : \"\"\"Uploads results to the server. Args: results_dict (dict): Dictionary containing results information. Returns: int: id of the generated results entry \"\"\" res = self . __auth_post ( f \" { self . server_url } /results/\" , json = results_dict ) if res . status_code != 201 : log_response_error ( res ) raise CommunicationRequestError ( \"Could not upload the results\" ) return res . json () def associate_dset ( self , data_uid : int , benchmark_uid : int , metadata : dict = {}): \"\"\"Create a Dataset Benchmark association Args: data_uid (int): Registered dataset UID benchmark_uid (int): Benchmark UID metadata (dict, optional): Additional metadata. Defaults to {}. \"\"\" data = { \"dataset\" : data_uid , \"benchmark\" : benchmark_uid , \"approval_status\" : Status . PENDING . value , \"metadata\" : metadata , } res = self . __auth_post ( f \" { self . server_url } /datasets/benchmarks/\" , json = data ) if res . status_code != 201 : log_response_error ( res ) raise CommunicationRequestError ( \"Could not associate dataset to benchmark\" ) def associate_cube ( self , cube_uid : str , benchmark_uid : int , metadata : dict = {}): \"\"\"Create an MLCube-Benchmark association Args: cube_uid (str): MLCube UID benchmark_uid (int): Benchmark UID metadata (dict, optional): Additional metadata. Defaults to {}. \"\"\" data = { \"approval_status\" : Status . PENDING . value , \"model_mlcube\" : cube_uid , \"benchmark\" : benchmark_uid , \"metadata\" : metadata , } res = self . __auth_post ( f \" { self . server_url } /mlcubes/benchmarks/\" , json = data ) if res . status_code != 201 : log_response_error ( res ) raise CommunicationRequestError ( \"Could not associate mlcube to benchmark\" ) def set_dataset_association_approval ( self , benchmark_uid : str , dataset_uid : str , status : str ): \"\"\"Approves a dataset association Args: dataset_uid (str): Dataset UID benchmark_uid (str): Benchmark UID status (str): Approval status to set for the association \"\"\" url = f \" { self . server_url } /datasets/ { dataset_uid } /benchmarks/ { benchmark_uid } /\" res = self . __set_approval_status ( url , status ) if res . status_code != 200 : log_response_error ( res ) raise CommunicationRequestError ( f \"Could not approve association between dataset { dataset_uid } and benchmark { benchmark_uid } \" ) def set_mlcube_association_approval ( self , benchmark_uid : str , mlcube_uid : str , status : str ): \"\"\"Approves an mlcube association Args: mlcube_uid (str): Dataset UID benchmark_uid (str): Benchmark UID status (str): Approval status to set for the association \"\"\" url = f \" { self . server_url } /mlcubes/ { mlcube_uid } /benchmarks/ { benchmark_uid } /\" res = self . __set_approval_status ( url , status ) if res . status_code != 200 : log_response_error ( res ) raise CommunicationRequestError ( f \"Could not approve association between mlcube { mlcube_uid } and benchmark { benchmark_uid } \" ) def get_datasets_associations ( self ) -> List [ dict ]: \"\"\"Get all dataset associations related to the current user Returns: List[dict]: List containing all associations information \"\"\" assocs = self . __get_list ( f \" { self . server_url } /me/datasets/associations/\" ) return assocs def get_cubes_associations ( self ) -> List [ dict ]: \"\"\"Get all cube associations related to the current user Returns: List[dict]: List containing all associations information \"\"\" assocs = self . __get_list ( f \" { self . server_url } /me/mlcubes/associations/\" ) return assocs","title":"REST"},{"location":"reference/comms/rest/#comms.rest.REST.__get_list","text":"Retrieves a list of elements from a URL by iterating over pages until num_elements is obtained. If num_elements is None, then iterates until all elements have been retrieved. If binary_reduction is enabled, errors are assumed to be related to response size. In that case, the page_size is reduced by half until a successful response is obtained or until page_size can't be reduced anymore. Parameters: Name Type Description Default url str The url to retrieve elements from required num_elements int The desired number of elements to be retrieved. Defaults to None. None page_size int Starting page size. Defaults to config.default_page_size. config.default_page_size start_limit int The starting position for element retrieval. Defaults to 0. required binary_reduction bool Wether to handle errors by halfing the page size. Defaults to False. False Returns: Type Description List[dict]: A list of dictionaries representing the retrieved elements. Source code in comms/rest.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 def __get_list ( self , url , num_elements = None , page_size = config . default_page_size , offset = 0 , binary_reduction = False , ): \"\"\"Retrieves a list of elements from a URL by iterating over pages until num_elements is obtained. If num_elements is None, then iterates until all elements have been retrieved. If binary_reduction is enabled, errors are assumed to be related to response size. In that case, the page_size is reduced by half until a successful response is obtained or until page_size can't be reduced anymore. Args: url (str): The url to retrieve elements from num_elements (int, optional): The desired number of elements to be retrieved. Defaults to None. page_size (int, optional): Starting page size. Defaults to config.default_page_size. start_limit (int, optional): The starting position for element retrieval. Defaults to 0. binary_reduction (bool, optional): Wether to handle errors by halfing the page size. Defaults to False. Returns: List[dict]: A list of dictionaries representing the retrieved elements. \"\"\" el_list = [] if num_elements is None : num_elements = float ( \"inf\" ) while len ( el_list ) < num_elements : paginated_url = f \" { url } ?limit= { page_size } &offset= { offset } \" res = self . __auth_get ( paginated_url ) if res . status_code != 200 : if not binary_reduction : log_response_error ( res ) raise CommunicationRetrievalError ( \"there was an error retrieving the current list.\" ) log_response_error ( res , warn = True ) if page_size <= 1 : raise CommunicationRetrievalError ( \"Could not retrieve list. Minimum page size achieved without success.\" ) page_size = page_size // 2 continue else : data = res . json () el_list += data [ \"results\" ] offset += len ( data [ \"results\" ]) if data [ \"next\" ] is None : break if type ( num_elements ) == int : return el_list [: num_elements ] return el_list","title":"__get_list()"},{"location":"reference/comms/rest/#comms.rest.REST.__set_approval_status","text":"Sets the approval status of a resource Parameters: Name Type Description Default url str URL to the resource to update required status str approval status to set required Returns: Type Description requests . Response requests.Response: Response object returned by the update Source code in comms/rest.py 185 186 187 188 189 190 191 192 193 194 195 196 197 def __set_approval_status ( self , url : str , status : str ) -> requests . Response : \"\"\"Sets the approval status of a resource Args: url (str): URL to the resource to update status (str): approval status to set Returns: requests.Response: Response object returned by the update \"\"\" data = { \"approval_status\" : status } res = self . __auth_put ( url , json = data ,) return res","title":"__set_approval_status()"},{"location":"reference/comms/rest/#comms.rest.REST.associate_cube","text":"Create an MLCube-Benchmark association Parameters: Name Type Description Default cube_uid str MLCube UID required benchmark_uid int Benchmark UID required metadata dict Additional metadata. Defaults to {}. {} Source code in comms/rest.py 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 def associate_cube ( self , cube_uid : str , benchmark_uid : int , metadata : dict = {}): \"\"\"Create an MLCube-Benchmark association Args: cube_uid (str): MLCube UID benchmark_uid (int): Benchmark UID metadata (dict, optional): Additional metadata. Defaults to {}. \"\"\" data = { \"approval_status\" : Status . PENDING . value , \"model_mlcube\" : cube_uid , \"benchmark\" : benchmark_uid , \"metadata\" : metadata , } res = self . __auth_post ( f \" { self . server_url } /mlcubes/benchmarks/\" , json = data ) if res . status_code != 201 : log_response_error ( res ) raise CommunicationRequestError ( \"Could not associate mlcube to benchmark\" )","title":"associate_cube()"},{"location":"reference/comms/rest/#comms.rest.REST.associate_dset","text":"Create a Dataset Benchmark association Parameters: Name Type Description Default data_uid int Registered dataset UID required benchmark_uid int Benchmark UID required metadata dict Additional metadata. Defaults to {}. {} Source code in comms/rest.py 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 def associate_dset ( self , data_uid : int , benchmark_uid : int , metadata : dict = {}): \"\"\"Create a Dataset Benchmark association Args: data_uid (int): Registered dataset UID benchmark_uid (int): Benchmark UID metadata (dict, optional): Additional metadata. Defaults to {}. \"\"\" data = { \"dataset\" : data_uid , \"benchmark\" : benchmark_uid , \"approval_status\" : Status . PENDING . value , \"metadata\" : metadata , } res = self . __auth_post ( f \" { self . server_url } /datasets/benchmarks/\" , json = data ) if res . status_code != 201 : log_response_error ( res ) raise CommunicationRequestError ( \"Could not associate dataset to benchmark\" )","title":"associate_dset()"},{"location":"reference/comms/rest/#comms.rest.REST.authorized_by_role","text":"Indicates wether the current user is authorized to access a benchmark based on desired role Parameters: Name Type Description Default benchmark_uid int UID of the benchmark required role str Desired role to check for authorization required Returns: Name Type Description bool bool Wether the user has the specified role for that benchmark Source code in comms/rest.py 215 216 217 218 219 220 221 222 223 224 225 226 227 def authorized_by_role ( self , benchmark_uid : int , role : str ) -> bool : \"\"\"Indicates wether the current user is authorized to access a benchmark based on desired role Args: benchmark_uid (int): UID of the benchmark role (str): Desired role to check for authorization Returns: bool: Wether the user has the specified role for that benchmark \"\"\" assoc_role = self . benchmark_association ( benchmark_uid ) return assoc_role . name == role","title":"authorized_by_role()"},{"location":"reference/comms/rest/#comms.rest.REST.benchmark_association","text":"Retrieves the benchmark association Parameters: Name Type Description Default benchmark_uid int UID of the benchmark required Returns: Name Type Description Role Role the association type between current user and benchmark Source code in comms/rest.py 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 def benchmark_association ( self , benchmark_uid : int ) -> Role : \"\"\"Retrieves the benchmark association Args: benchmark_uid (int): UID of the benchmark Returns: Role: the association type between current user and benchmark \"\"\" benchmarks = self . __get_list ( f \" { self . server_url } /me/benchmarks\" ) bm_dict = { bm [ \"benchmark\" ]: bm for bm in benchmarks } rolename = None if benchmark_uid in bm_dict : rolename = bm_dict [ benchmark_uid ][ \"role\" ] return Role ( rolename )","title":"benchmark_association()"},{"location":"reference/comms/rest/#comms.rest.REST.change_password","text":"Sets a new password for the current user. Parameters: Name Type Description Default pwd str New password to be set required ui UI Instance of an implementation required Returns: Name Type Description bool bool Whether changing the password was successful or not Source code in comms/rest.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def change_password ( self , pwd : str ) -> bool : \"\"\"Sets a new password for the current user. Args: pwd (str): New password to be set ui (UI): Instance of an implementation Returns: bool: Whether changing the password was successful or not \"\"\" body = { \"password\" : pwd } res = self . __auth_post ( f \" { self . server_url } /me/password/\" , json = body ) if res . status_code != 200 : log_response_error ( res ) raise CommunicationRequestError ( \"Unable to change the current password\" ) return True","title":"change_password()"},{"location":"reference/comms/rest/#comms.rest.REST.get_benchmark","text":"Retrieves the benchmark specification file from the server Parameters: Name Type Description Default benchmark_uid int uid for the desired benchmark required Returns: Name Type Description dict dict benchmark specification Source code in comms/rest.py 238 239 240 241 242 243 244 245 246 247 248 249 250 251 def get_benchmark ( self , benchmark_uid : int ) -> dict : \"\"\"Retrieves the benchmark specification file from the server Args: benchmark_uid (int): uid for the desired benchmark Returns: dict: benchmark specification \"\"\" res = self . __auth_get ( f \" { self . server_url } /benchmarks/ { benchmark_uid } \" ) if res . status_code != 200 : log_response_error ( res ) raise CommunicationRetrievalError ( \"the specified benchmark doesn't exist\" ) return res . json ()","title":"get_benchmark()"},{"location":"reference/comms/rest/#comms.rest.REST.get_benchmark_demo_dataset","text":"Downloads the benchmark demo dataset and stores it in the user's machine Parameters: Name Type Description Default demo_data_url str location of demo data for download required uid str UID to use for storing the demo dataset. Defaults to generate_tmp_uid(). generate_tmp_uid() Returns: Name Type Description str str path where the downloaded demo dataset can be found Source code in comms/rest.py 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 def get_benchmark_demo_dataset ( self , demo_data_url : str , uid : str = generate_tmp_uid () ) -> str : \"\"\"Downloads the benchmark demo dataset and stores it in the user's machine Args: demo_data_url (str): location of demo data for download uid (str): UID to use for storing the demo dataset. Defaults to generate_tmp_uid(). Returns: str: path where the downloaded demo dataset can be found \"\"\" tmp_dir = storage_path ( config . demo_data_storage ) demo_data_path = os . path . join ( tmp_dir , uid ) tball_file = config . tarball_filename filepath = os . path . join ( demo_data_path , tball_file ) # Don't re-download if something already exists with same uid if os . path . exists ( filepath ): return filepath res = requests . get ( demo_data_url ) if res . status_code != 200 : log_response_error ( res ) raise CommunicationRetrievalError ( \"couldn't download the demo dataset\" ) os . makedirs ( demo_data_path , exist_ok = True ) open ( filepath , \"wb+\" ) . write ( res . content ) return filepath","title":"get_benchmark_demo_dataset()"},{"location":"reference/comms/rest/#comms.rest.REST.get_benchmark_models","text":"Retrieves all the models associated with a benchmark. reference model not included Parameters: Name Type Description Default benchmark_uid int UID of the desired benchmark required Returns: Type Description List [ int ] list[int]: List of model UIDS Source code in comms/rest.py 253 254 255 256 257 258 259 260 261 262 263 264 def get_benchmark_models ( self , benchmark_uid : int ) -> List [ int ]: \"\"\"Retrieves all the models associated with a benchmark. reference model not included Args: benchmark_uid (int): UID of the desired benchmark Returns: list[int]: List of model UIDS \"\"\" models = self . __get_list ( f \" { self . server_url } /benchmarks/ { benchmark_uid } /models\" ) model_uids = [ model [ \"id\" ] for model in models ] return model_uids","title":"get_benchmark_models()"},{"location":"reference/comms/rest/#comms.rest.REST.get_benchmarks","text":"Retrieves all benchmarks in the platform. Returns: Type Description List [ dict ] List[dict]: all benchmarks information. Source code in comms/rest.py 229 230 231 232 233 234 235 236 def get_benchmarks ( self ) -> List [ dict ]: \"\"\"Retrieves all benchmarks in the platform. Returns: List[dict]: all benchmarks information. \"\"\" bmks = self . __get_list ( f \" { self . server_url } /benchmarks/\" ) return bmks","title":"get_benchmarks()"},{"location":"reference/comms/rest/#comms.rest.REST.get_cube","text":"Downloads and writes an mlcube.yaml file from the server Parameters: Name Type Description Default url str URL where the mlcube.yaml file can be downloaded. required cube_uid int Cube UID. required Returns: Name Type Description str str location where the mlcube.yaml file is stored locally. Source code in comms/rest.py 330 331 332 333 334 335 336 337 338 339 340 341 def get_cube ( self , url : str , cube_uid : int ) -> str : \"\"\"Downloads and writes an mlcube.yaml file from the server Args: url (str): URL where the mlcube.yaml file can be downloaded. cube_uid (int): Cube UID. Returns: str: location where the mlcube.yaml file is stored locally. \"\"\" cube_file = config . cube_filename return self . __get_cube_file ( url , cube_uid , \"\" , cube_file )","title":"get_cube()"},{"location":"reference/comms/rest/#comms.rest.REST.get_cube_additional","text":"Retrieves and stores the additional_files.tar.gz file from the server Parameters: Name Type Description Default url str URL where the additional_files.tar.gz file can be downloaded. required cube_uid int Cube UID. required Returns: Name Type Description str str Location where the additional_files.tar.gz file is stored locally. Source code in comms/rest.py 366 367 368 369 370 371 372 373 374 375 376 377 378 def get_cube_additional ( self , url : str , cube_uid : int ) -> str : \"\"\"Retrieves and stores the additional_files.tar.gz file from the server Args: url (str): URL where the additional_files.tar.gz file can be downloaded. cube_uid (int): Cube UID. Returns: str: Location where the additional_files.tar.gz file is stored locally. \"\"\" add_path = config . additional_path tball_file = config . tarball_filename return self . __get_cube_file ( url , cube_uid , add_path , tball_file )","title":"get_cube_additional()"},{"location":"reference/comms/rest/#comms.rest.REST.get_cube_image","text":"Retrieves and stores the image file from the server Parameters: Name Type Description Default url str URL where the image file can be downloaded. required cube_uid int Cube UID. required Returns: Name Type Description str str Location where the image file is stored locally. Source code in comms/rest.py 380 381 382 383 384 385 386 387 388 389 390 391 392 def get_cube_image ( self , url : str , cube_uid : int ) -> str : \"\"\"Retrieves and stores the image file from the server Args: url (str): URL where the image file can be downloaded. cube_uid (int): Cube UID. Returns: str: Location where the image file is stored locally. \"\"\" image_path = config . image_path image_name = url . split ( \"/\" )[ - 1 ] return self . __get_cube_file ( url , cube_uid , image_path , image_name )","title":"get_cube_image()"},{"location":"reference/comms/rest/#comms.rest.REST.get_cube_metadata","text":"Retrieves metadata about the specified cube Parameters: Name Type Description Default cube_uid int UID of the desired cube. required Returns: Name Type Description dict dict Dictionary containing url and hashes for the cube files Source code in comms/rest.py 315 316 317 318 319 320 321 322 323 324 325 326 327 328 def get_cube_metadata ( self , cube_uid : int ) -> dict : \"\"\"Retrieves metadata about the specified cube Args: cube_uid (int): UID of the desired cube. Returns: dict: Dictionary containing url and hashes for the cube files \"\"\" res = self . __auth_get ( f \" { self . server_url } /mlcubes/ { cube_uid } /\" ) if res . status_code != 200 : log_response_error ( res ) raise CommunicationRetrievalError ( \"the specified cube doesn't exist\" ) return res . json ()","title":"get_cube_metadata()"},{"location":"reference/comms/rest/#comms.rest.REST.get_cube_params","text":"Retrieves the cube parameters.yaml file from the server Parameters: Name Type Description Default url str URL where the parameters.yaml file can be downloaded. required cube_uid int Cube UID. required Returns: Name Type Description str str Location where the parameters.yaml file is stored locally. Source code in comms/rest.py 352 353 354 355 356 357 358 359 360 361 362 363 364 def get_cube_params ( self , url : str , cube_uid : int ) -> str : \"\"\"Retrieves the cube parameters.yaml file from the server Args: url (str): URL where the parameters.yaml file can be downloaded. cube_uid (int): Cube UID. Returns: str: Location where the parameters.yaml file is stored locally. \"\"\" ws = config . workspace_path params_file = config . params_filename return self . __get_cube_file ( url , cube_uid , ws , params_file )","title":"get_cube_params()"},{"location":"reference/comms/rest/#comms.rest.REST.get_cubes","text":"Retrieves all MLCubes in the platform Returns: Type Description List [ dict ] List[dict]: List containing the data of all MLCubes Source code in comms/rest.py 306 307 308 309 310 311 312 313 def get_cubes ( self ) -> List [ dict ]: \"\"\"Retrieves all MLCubes in the platform Returns: List[dict]: List containing the data of all MLCubes \"\"\" cubes = self . __get_list ( f \" { self . server_url } /mlcubes/\" ) return cubes","title":"get_cubes()"},{"location":"reference/comms/rest/#comms.rest.REST.get_cubes_associations","text":"Get all cube associations related to the current user Returns: Type Description List [ dict ] List[dict]: List containing all associations information Source code in comms/rest.py 611 612 613 614 615 616 617 618 def get_cubes_associations ( self ) -> List [ dict ]: \"\"\"Get all cube associations related to the current user Returns: List[dict]: List containing all associations information \"\"\" assocs = self . __get_list ( f \" { self . server_url } /me/mlcubes/associations/\" ) return assocs","title":"get_cubes_associations()"},{"location":"reference/comms/rest/#comms.rest.REST.get_dataset","text":"Retrieves a specific dataset Parameters: Name Type Description Default dset_uid str Dataset UID required Returns: Name Type Description dict dict Dataset metadata Source code in comms/rest.py 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 def get_dataset ( self , dset_uid : str ) -> dict : \"\"\"Retrieves a specific dataset Args: dset_uid (str): Dataset UID Returns: dict: Dataset metadata \"\"\" res = self . __auth_get ( f \" { self . server_url } /datasets/ { dset_uid } /\" ) if res . status_code != 200 : log_response_error ( res ) raise CommunicationRetrievalError ( \"Could not retrieve the specified dataset from server\" ) return res . json ()","title":"get_dataset()"},{"location":"reference/comms/rest/#comms.rest.REST.get_datasets","text":"Retrieves all datasets in the platform Returns: Type Description List [ dict ] List[dict]: List of data from all datasets Source code in comms/rest.py 439 440 441 442 443 444 445 446 def get_datasets ( self ) -> List [ dict ]: \"\"\"Retrieves all datasets in the platform Returns: List[dict]: List of data from all datasets \"\"\" dsets = self . __get_list ( f \" { self . server_url } /datasets/\" ) return dsets","title":"get_datasets()"},{"location":"reference/comms/rest/#comms.rest.REST.get_datasets_associations","text":"Get all dataset associations related to the current user Returns: Type Description List [ dict ] List[dict]: List containing all associations information Source code in comms/rest.py 602 603 604 605 606 607 608 609 def get_datasets_associations ( self ) -> List [ dict ]: \"\"\"Get all dataset associations related to the current user Returns: List[dict]: List containing all associations information \"\"\" assocs = self . __get_list ( f \" { self . server_url } /me/datasets/associations/\" ) return assocs","title":"get_datasets_associations()"},{"location":"reference/comms/rest/#comms.rest.REST.get_result","text":"Retrieves a specific result data Parameters: Name Type Description Default result_uid str Result UID required Returns: Name Type Description dict dict Result metadata Source code in comms/rest.py 489 490 491 492 493 494 495 496 497 498 499 500 501 502 def get_result ( self , result_uid : str ) -> dict : \"\"\"Retrieves a specific result data Args: result_uid (str): Result UID Returns: dict: Result metadata \"\"\" res = self . __auth_get ( f \" { self . server_url } /results/ { result_uid } /\" ) if res . status_code != 200 : log_response_error ( res ) raise CommunicationRetrievalError ( \"Could not retrieve the specified result\" ) return res . json ()","title":"get_result()"},{"location":"reference/comms/rest/#comms.rest.REST.get_user_benchmarks","text":"Retrieves all benchmarks created by the user Returns: Type Description List [ dict ] List[dict]: Benchmarks data Source code in comms/rest.py 297 298 299 300 301 302 303 304 def get_user_benchmarks ( self ) -> List [ dict ]: \"\"\"Retrieves all benchmarks created by the user Returns: List[dict]: Benchmarks data \"\"\" bmks = self . __get_list ( f \" { self . server_url } /me/benchmarks/\" ) return bmks","title":"get_user_benchmarks()"},{"location":"reference/comms/rest/#comms.rest.REST.get_user_cubes","text":"Retrieves metadata from all cubes registered by the user Returns: Type Description List [ dict ] List[dict]: List of dictionaries containing the mlcubes registration information Source code in comms/rest.py 343 344 345 346 347 348 349 350 def get_user_cubes ( self ) -> List [ dict ]: \"\"\"Retrieves metadata from all cubes registered by the user Returns: List[dict]: List of dictionaries containing the mlcubes registration information \"\"\" cubes = self . __get_list ( f \" { self . server_url } /me/mlcubes/\" ) return cubes","title":"get_user_cubes()"},{"location":"reference/comms/rest/#comms.rest.REST.get_user_datasets","text":"Retrieves all datasets registered by the user Returns: Name Type Description dict dict dictionary with the contents of each dataset registration query Source code in comms/rest.py 465 466 467 468 469 470 471 472 def get_user_datasets ( self ) -> dict : \"\"\"Retrieves all datasets registered by the user Returns: dict: dictionary with the contents of each dataset registration query \"\"\" dsets = self . __get_list ( f \" { self . server_url } /me/datasets/\" ) return dsets","title":"get_user_datasets()"},{"location":"reference/comms/rest/#comms.rest.REST.get_user_results","text":"Retrieves all results registered by the user Returns: Name Type Description dict dict dictionary with the contents of each dataset registration query Source code in comms/rest.py 504 505 506 507 508 509 510 511 def get_user_results ( self ) -> dict : \"\"\"Retrieves all results registered by the user Returns: dict: dictionary with the contents of each dataset registration query \"\"\" results = self . __get_list ( f \" { self . server_url } /me/results/\" ) return results","title":"get_user_results()"},{"location":"reference/comms/rest/#comms.rest.REST.login","text":"Authenticates the user with the server. Required for most endpoints Parameters: Name Type Description Default ui UI Instance of an implementation of the UI interface required user str Username required pwd str Password required Source code in comms/rest.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def login ( self , user : str , pwd : str ): \"\"\"Authenticates the user with the server. Required for most endpoints Args: ui (UI): Instance of an implementation of the UI interface user (str): Username pwd (str): Password \"\"\" body = { \"username\" : user , \"password\" : pwd } res = self . __req ( f \" { self . server_url } /auth-token/\" , requests . post , json = body ) if res . status_code != 200 : log_response_error ( res ) CommunicationAuthenticationError ( \"Unable to authenticate user with provided credentials\" ) else : self . token = res . json ()[ \"token\" ]","title":"login()"},{"location":"reference/comms/rest/#comms.rest.REST.set_dataset_association_approval","text":"Approves a dataset association Parameters: Name Type Description Default dataset_uid str Dataset UID required benchmark_uid str Benchmark UID required status str Approval status to set for the association required Source code in comms/rest.py 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 def set_dataset_association_approval ( self , benchmark_uid : str , dataset_uid : str , status : str ): \"\"\"Approves a dataset association Args: dataset_uid (str): Dataset UID benchmark_uid (str): Benchmark UID status (str): Approval status to set for the association \"\"\" url = f \" { self . server_url } /datasets/ { dataset_uid } /benchmarks/ { benchmark_uid } /\" res = self . __set_approval_status ( url , status ) if res . status_code != 200 : log_response_error ( res ) raise CommunicationRequestError ( f \"Could not approve association between dataset { dataset_uid } and benchmark { benchmark_uid } \" )","title":"set_dataset_association_approval()"},{"location":"reference/comms/rest/#comms.rest.REST.set_mlcube_association_approval","text":"Approves an mlcube association Parameters: Name Type Description Default mlcube_uid str Dataset UID required benchmark_uid str Benchmark UID required status str Approval status to set for the association required Source code in comms/rest.py 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 def set_mlcube_association_approval ( self , benchmark_uid : str , mlcube_uid : str , status : str ): \"\"\"Approves an mlcube association Args: mlcube_uid (str): Dataset UID benchmark_uid (str): Benchmark UID status (str): Approval status to set for the association \"\"\" url = f \" { self . server_url } /mlcubes/ { mlcube_uid } /benchmarks/ { benchmark_uid } /\" res = self . __set_approval_status ( url , status ) if res . status_code != 200 : log_response_error ( res ) raise CommunicationRequestError ( f \"Could not approve association between mlcube { mlcube_uid } and benchmark { benchmark_uid } \" )","title":"set_mlcube_association_approval()"},{"location":"reference/comms/rest/#comms.rest.REST.upload_benchmark","text":"Uploads a new benchmark to the server. Parameters: Name Type Description Default benchmark_dict dict benchmark_data to be uploaded required Returns: Name Type Description int int UID of newly created benchmark Source code in comms/rest.py 409 410 411 412 413 414 415 416 417 418 419 420 421 422 def upload_benchmark ( self , benchmark_dict : dict ) -> int : \"\"\"Uploads a new benchmark to the server. Args: benchmark_dict (dict): benchmark_data to be uploaded Returns: int: UID of newly created benchmark \"\"\" res = self . __auth_post ( f \" { self . server_url } /benchmarks/\" , json = benchmark_dict ) if res . status_code != 201 : log_response_error ( res ) raise CommunicationRetrievalError ( \"Could not upload benchmark\" ) return res . json ()","title":"upload_benchmark()"},{"location":"reference/comms/rest/#comms.rest.REST.upload_dataset","text":"Uploads registration data to the server, under the sha name of the file. Parameters: Name Type Description Default reg_dict dict Dictionary containing registration information. required Returns: Name Type Description int int id of the created dataset registration. Source code in comms/rest.py 474 475 476 477 478 479 480 481 482 483 484 485 486 487 def upload_dataset ( self , reg_dict : dict ) -> int : \"\"\"Uploads registration data to the server, under the sha name of the file. Args: reg_dict (dict): Dictionary containing registration information. Returns: int: id of the created dataset registration. \"\"\" res = self . __auth_post ( f \" { self . server_url } /datasets/\" , json = reg_dict ) if res . status_code != 201 : log_response_error ( res ) raise CommunicationRequestError ( \"Could not upload the dataset\" ) return res . json ()","title":"upload_dataset()"},{"location":"reference/comms/rest/#comms.rest.REST.upload_mlcube","text":"Uploads an MLCube instance to the platform Parameters: Name Type Description Default mlcube_body dict Dictionary containing all the relevant data for creating mlcubes required Returns: Name Type Description int int id of the created mlcube instance on the platform Source code in comms/rest.py 424 425 426 427 428 429 430 431 432 433 434 435 436 437 def upload_mlcube ( self , mlcube_body : dict ) -> int : \"\"\"Uploads an MLCube instance to the platform Args: mlcube_body (dict): Dictionary containing all the relevant data for creating mlcubes Returns: int: id of the created mlcube instance on the platform \"\"\" res = self . __auth_post ( f \" { self . server_url } /mlcubes/\" , json = mlcube_body ) if res . status_code != 201 : log_response_error ( res ) raise CommunicationRetrievalError ( \"Could not upload the mlcube\" ) return res . json ()","title":"upload_mlcube()"},{"location":"reference/comms/rest/#comms.rest.REST.upload_results","text":"Uploads results to the server. Parameters: Name Type Description Default results_dict dict Dictionary containing results information. required Returns: Name Type Description int int id of the generated results entry Source code in comms/rest.py 513 514 515 516 517 518 519 520 521 522 523 524 525 526 def upload_results ( self , results_dict : dict ) -> int : \"\"\"Uploads results to the server. Args: results_dict (dict): Dictionary containing results information. Returns: int: id of the generated results entry \"\"\" res = self . __auth_post ( f \" { self . server_url } /results/\" , json = results_dict ) if res . status_code != 201 : log_response_error ( res ) raise CommunicationRequestError ( \"Could not upload the results\" ) return res . json ()","title":"upload_results()"},{"location":"reference/entities/benchmark/","text":"Benchmark Bases: Entity Class representing a Benchmark a benchmark is a bundle of assets that enables quantitative measurement of the performance of AI models for a specific clinical problem. A Benchmark instance contains information regarding how to prepare datasets for execution, as well as what models to run and how to evaluate them. Source code in entities/benchmark.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 class Benchmark ( Entity ): \"\"\" Class representing a Benchmark a benchmark is a bundle of assets that enables quantitative measurement of the performance of AI models for a specific clinical problem. A Benchmark instance contains information regarding how to prepare datasets for execution, as well as what models to run and how to evaluate them. \"\"\" def __init__ ( self , bmk_dict : dict ): \"\"\"Creates a new benchmark instance Args: uid (str): The benchmark UID benchmark_dict (dict): key-value representation of the benchmark. \"\"\" self . uid = bmk_dict [ \"id\" ] self . name = bmk_dict [ \"name\" ] self . description = bmk_dict [ \"description\" ] self . docs_url = bmk_dict [ \"docs_url\" ] self . created_at = bmk_dict [ \"created_at\" ] self . modified_at = bmk_dict [ \"modified_at\" ] self . approved_at = bmk_dict [ \"approved_at\" ] self . owner = bmk_dict [ \"owner\" ] self . demo_dataset_url = bmk_dict [ \"demo_dataset_tarball_url\" ] self . demo_dataset_hash = bmk_dict [ \"demo_dataset_tarball_hash\" ] self . demo_dataset_generated_uid = bmk_dict [ \"demo_dataset_generated_uid\" ] self . data_preparation = bmk_dict [ \"data_preparation_mlcube\" ] self . reference_model = bmk_dict [ \"reference_model_mlcube\" ] self . evaluator = bmk_dict [ \"data_evaluator_mlcube\" ] self . models = bmk_dict [ \"models\" ] self . state = bmk_dict [ \"state\" ] self . is_valid = bmk_dict [ \"is_valid\" ] self . is_active = bmk_dict [ \"is_active\" ] self . approval_status = Status ( bmk_dict [ \"approval_status\" ]) self . metadata = bmk_dict [ \"metadata\" ] self . user_metadata = bmk_dict [ \"user_metadata\" ] @classmethod def all ( cls ) -> List [ \"Benchmark\" ]: \"\"\"Gets and creates instances of all locally present benchmarks Returns: List[Benchmark]: a list of Benchmark instances. \"\"\" logging . info ( \"Retrieving all benchmarks\" ) bmks_storage = storage_path ( config . benchmarks_storage ) try : uids = next ( os . walk ( bmks_storage ))[ 1 ] except StopIteration : msg = \"Couldn't iterate over benchmarks directory\" logging . warning ( msg ) raise RuntimeError ( msg ) benchmarks = [ cls . get ( uid ) for uid in uids ] return benchmarks @classmethod def get ( cls , benchmark_uid : str ) -> \"Benchmark\" : \"\"\"Retrieves and creates a Benchmark instance from the server. If benchmark already exists in the platform then retrieve that version. Args: benchmark_uid (str): UID of the benchmark. comms (Comms): Instance of a communication interface. Returns: Benchmark: a Benchmark instance with the retrieved data. \"\"\" comms = config . comms # Try to download first try : benchmark_dict = comms . get_benchmark ( benchmark_uid ) ref_model = benchmark_dict [ \"reference_model_mlcube\" ] add_models = cls . get_models_uids ( benchmark_uid ) benchmark_dict [ \"models\" ] = [ ref_model ] + add_models except CommunicationRetrievalError : # Get local benchmarks logging . warning ( f \"Getting benchmark { benchmark_uid } from comms failed\" ) logging . info ( f \"Looking for benchmark { benchmark_uid } locally\" ) bmk_storage = storage_path ( config . benchmarks_storage ) local_bmks = os . listdir ( bmk_storage ) if str ( benchmark_uid ) in local_bmks : benchmark_dict = cls . __get_local_dict ( benchmark_uid ) else : raise InvalidArgumentError ( \"No benchmark with the given uid could be found\" ) benchmark = cls ( benchmark_dict ) benchmark . write () return benchmark @classmethod def __get_local_dict ( cls , benchmark_uid : str ) -> dict : \"\"\"Retrieves a local benchmark information Args: benchmark_uid (str): uid of the local benchmark Returns: dict: information of the benchmark \"\"\" logging . info ( f \"Retrieving benchmark { benchmark_uid } from local storage\" ) storage = storage_path ( config . benchmarks_storage ) bmk_storage = os . path . join ( storage , str ( benchmark_uid )) bmk_file = os . path . join ( bmk_storage , config . benchmarks_filename ) with open ( bmk_file , \"r\" ) as f : data = yaml . safe_load ( f ) return data @classmethod def tmp ( cls , data_preparator : str , model : str , evaluator : str , demo_url : str = None , demo_hash : str = None , ) -> \"Benchmark\" : \"\"\"Creates a temporary instance of the benchmark Args: data_preparator (str): UID of the data preparator cube to use. model (str): UID of the model cube to use. evaluator (str): UID of the evaluator cube to use. demo_url (str, optional): URL to obtain the demo dataset. Defaults to None. demo_hash (str, optional): Hash of the demo dataset tarball file. Defaults to None. Returns: Benchmark: a benchmark instance \"\"\" benchmark_uid = f \" { config . tmp_prefix }{ data_preparator } _ { model } _ { evaluator } \" benchmark_dict = { \"id\" : benchmark_uid , \"name\" : benchmark_uid , \"data_preparation_mlcube\" : data_preparator , \"reference_model_mlcube\" : model , \"data_evaluator_mlcube\" : evaluator , \"demo_dataset_tarball_url\" : demo_url , \"demo_dataset_tarball_hash\" : demo_hash , \"models\" : [ model ], # not in the server (OK) \"description\" : None , \"docs_url\" : None , \"created_at\" : None , \"modified_at\" : None , \"approved_at\" : None , \"owner\" : None , \"demo_dataset_generated_uid\" : None , \"state\" : \"DEVELOPMENT\" , \"is_valid\" : True , \"is_active\" : True , \"approval_status\" : Status . PENDING . value , \"metadata\" : {}, \"user_metadata\" : {}, } benchmark = cls ( benchmark_dict ) benchmark . write () return benchmark @classmethod def get_models_uids ( cls , benchmark_uid : str ) -> List [ str ]: \"\"\"Retrieves the list of models associated to the benchmark Args: benchmark_uid (str): UID of the benchmark. comms (Comms): Instance of the communications interface. Returns: List[str]: List of mlcube uids \"\"\" return config . comms . get_benchmark_models ( benchmark_uid ) def todict ( self ) -> dict : \"\"\"Dictionary representation of the benchmark instance Returns: dict: Dictionary containing benchmark information \"\"\" return { \"id\" : self . uid , \"name\" : self . name , \"description\" : self . description , \"docs_url\" : self . docs_url , \"created_at\" : self . created_at , \"modified_at\" : self . modified_at , \"approved_at\" : self . approved_at , \"owner\" : self . owner , \"demo_dataset_tarball_url\" : self . demo_dataset_url , \"demo_dataset_tarball_hash\" : self . demo_dataset_hash , \"demo_dataset_generated_uid\" : self . demo_dataset_generated_uid , \"data_preparation_mlcube\" : int ( self . data_preparation ), \"reference_model_mlcube\" : int ( self . reference_model ), \"models\" : self . models , # not in the server (OK) \"data_evaluator_mlcube\" : int ( self . evaluator ), \"state\" : self . state , \"is_valid\" : self . is_valid , \"is_active\" : self . is_active , \"approval_status\" : self . approval_status . value , \"metadata\" : self . metadata , \"user_metadata\" : self . user_metadata , } def write ( self ) -> str : \"\"\"Writes the benchmark into disk Args: filename (str, optional): name of the file. Defaults to config.benchmarks_filename. Returns: str: path to the created benchmark file \"\"\" data = self . todict () storage = storage_path ( config . benchmarks_storage ) bmk_path = os . path . join ( storage , str ( self . uid )) if not os . path . exists ( bmk_path ): os . makedirs ( bmk_path , exist_ok = True ) filepath = os . path . join ( bmk_path , config . benchmarks_filename ) with open ( filepath , \"w\" ) as f : yaml . dump ( data , f ) return filepath def upload ( self ): \"\"\"Uploads a benchmark to the server Args: comms (Comms): communications entity to submit through \"\"\" body = self . todict () updated_body = config . comms . upload_benchmark ( body ) updated_body [ \"models\" ] = body [ \"models\" ] return updated_body __get_local_dict ( benchmark_uid ) classmethod Retrieves a local benchmark information Parameters: Name Type Description Default benchmark_uid str uid of the local benchmark required Returns: Name Type Description dict dict information of the benchmark Source code in entities/benchmark.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @classmethod def __get_local_dict ( cls , benchmark_uid : str ) -> dict : \"\"\"Retrieves a local benchmark information Args: benchmark_uid (str): uid of the local benchmark Returns: dict: information of the benchmark \"\"\" logging . info ( f \"Retrieving benchmark { benchmark_uid } from local storage\" ) storage = storage_path ( config . benchmarks_storage ) bmk_storage = os . path . join ( storage , str ( benchmark_uid )) bmk_file = os . path . join ( bmk_storage , config . benchmarks_filename ) with open ( bmk_file , \"r\" ) as f : data = yaml . safe_load ( f ) return data __init__ ( bmk_dict ) Creates a new benchmark instance Parameters: Name Type Description Default uid str The benchmark UID required benchmark_dict dict key-value representation of the benchmark. required Source code in entities/benchmark.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def __init__ ( self , bmk_dict : dict ): \"\"\"Creates a new benchmark instance Args: uid (str): The benchmark UID benchmark_dict (dict): key-value representation of the benchmark. \"\"\" self . uid = bmk_dict [ \"id\" ] self . name = bmk_dict [ \"name\" ] self . description = bmk_dict [ \"description\" ] self . docs_url = bmk_dict [ \"docs_url\" ] self . created_at = bmk_dict [ \"created_at\" ] self . modified_at = bmk_dict [ \"modified_at\" ] self . approved_at = bmk_dict [ \"approved_at\" ] self . owner = bmk_dict [ \"owner\" ] self . demo_dataset_url = bmk_dict [ \"demo_dataset_tarball_url\" ] self . demo_dataset_hash = bmk_dict [ \"demo_dataset_tarball_hash\" ] self . demo_dataset_generated_uid = bmk_dict [ \"demo_dataset_generated_uid\" ] self . data_preparation = bmk_dict [ \"data_preparation_mlcube\" ] self . reference_model = bmk_dict [ \"reference_model_mlcube\" ] self . evaluator = bmk_dict [ \"data_evaluator_mlcube\" ] self . models = bmk_dict [ \"models\" ] self . state = bmk_dict [ \"state\" ] self . is_valid = bmk_dict [ \"is_valid\" ] self . is_active = bmk_dict [ \"is_active\" ] self . approval_status = Status ( bmk_dict [ \"approval_status\" ]) self . metadata = bmk_dict [ \"metadata\" ] self . user_metadata = bmk_dict [ \"user_metadata\" ] all () classmethod Gets and creates instances of all locally present benchmarks Returns: Type Description List [ Benchmark ] List[Benchmark]: a list of Benchmark instances. Source code in entities/benchmark.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 @classmethod def all ( cls ) -> List [ \"Benchmark\" ]: \"\"\"Gets and creates instances of all locally present benchmarks Returns: List[Benchmark]: a list of Benchmark instances. \"\"\" logging . info ( \"Retrieving all benchmarks\" ) bmks_storage = storage_path ( config . benchmarks_storage ) try : uids = next ( os . walk ( bmks_storage ))[ 1 ] except StopIteration : msg = \"Couldn't iterate over benchmarks directory\" logging . warning ( msg ) raise RuntimeError ( msg ) benchmarks = [ cls . get ( uid ) for uid in uids ] return benchmarks get ( benchmark_uid ) classmethod Retrieves and creates a Benchmark instance from the server. If benchmark already exists in the platform then retrieve that version. Parameters: Name Type Description Default benchmark_uid str UID of the benchmark. required comms Comms Instance of a communication interface. required Returns: Name Type Description Benchmark Benchmark a Benchmark instance with the retrieved data. Source code in entities/benchmark.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 @classmethod def get ( cls , benchmark_uid : str ) -> \"Benchmark\" : \"\"\"Retrieves and creates a Benchmark instance from the server. If benchmark already exists in the platform then retrieve that version. Args: benchmark_uid (str): UID of the benchmark. comms (Comms): Instance of a communication interface. Returns: Benchmark: a Benchmark instance with the retrieved data. \"\"\" comms = config . comms # Try to download first try : benchmark_dict = comms . get_benchmark ( benchmark_uid ) ref_model = benchmark_dict [ \"reference_model_mlcube\" ] add_models = cls . get_models_uids ( benchmark_uid ) benchmark_dict [ \"models\" ] = [ ref_model ] + add_models except CommunicationRetrievalError : # Get local benchmarks logging . warning ( f \"Getting benchmark { benchmark_uid } from comms failed\" ) logging . info ( f \"Looking for benchmark { benchmark_uid } locally\" ) bmk_storage = storage_path ( config . benchmarks_storage ) local_bmks = os . listdir ( bmk_storage ) if str ( benchmark_uid ) in local_bmks : benchmark_dict = cls . __get_local_dict ( benchmark_uid ) else : raise InvalidArgumentError ( \"No benchmark with the given uid could be found\" ) benchmark = cls ( benchmark_dict ) benchmark . write () return benchmark get_models_uids ( benchmark_uid ) classmethod Retrieves the list of models associated to the benchmark Parameters: Name Type Description Default benchmark_uid str UID of the benchmark. required comms Comms Instance of the communications interface. required Returns: Type Description List [ str ] List[str]: List of mlcube uids Source code in entities/benchmark.py 177 178 179 180 181 182 183 184 185 186 187 188 @classmethod def get_models_uids ( cls , benchmark_uid : str ) -> List [ str ]: \"\"\"Retrieves the list of models associated to the benchmark Args: benchmark_uid (str): UID of the benchmark. comms (Comms): Instance of the communications interface. Returns: List[str]: List of mlcube uids \"\"\" return config . comms . get_benchmark_models ( benchmark_uid ) tmp ( data_preparator , model , evaluator , demo_url = None , demo_hash = None ) classmethod Creates a temporary instance of the benchmark Parameters: Name Type Description Default data_preparator str UID of the data preparator cube to use. required model str UID of the model cube to use. required evaluator str UID of the evaluator cube to use. required demo_url str URL to obtain the demo dataset. Defaults to None. None demo_hash str Hash of the demo dataset tarball file. Defaults to None. None Returns: Name Type Description Benchmark Benchmark a benchmark instance Source code in entities/benchmark.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 @classmethod def tmp ( cls , data_preparator : str , model : str , evaluator : str , demo_url : str = None , demo_hash : str = None , ) -> \"Benchmark\" : \"\"\"Creates a temporary instance of the benchmark Args: data_preparator (str): UID of the data preparator cube to use. model (str): UID of the model cube to use. evaluator (str): UID of the evaluator cube to use. demo_url (str, optional): URL to obtain the demo dataset. Defaults to None. demo_hash (str, optional): Hash of the demo dataset tarball file. Defaults to None. Returns: Benchmark: a benchmark instance \"\"\" benchmark_uid = f \" { config . tmp_prefix }{ data_preparator } _ { model } _ { evaluator } \" benchmark_dict = { \"id\" : benchmark_uid , \"name\" : benchmark_uid , \"data_preparation_mlcube\" : data_preparator , \"reference_model_mlcube\" : model , \"data_evaluator_mlcube\" : evaluator , \"demo_dataset_tarball_url\" : demo_url , \"demo_dataset_tarball_hash\" : demo_hash , \"models\" : [ model ], # not in the server (OK) \"description\" : None , \"docs_url\" : None , \"created_at\" : None , \"modified_at\" : None , \"approved_at\" : None , \"owner\" : None , \"demo_dataset_generated_uid\" : None , \"state\" : \"DEVELOPMENT\" , \"is_valid\" : True , \"is_active\" : True , \"approval_status\" : Status . PENDING . value , \"metadata\" : {}, \"user_metadata\" : {}, } benchmark = cls ( benchmark_dict ) benchmark . write () return benchmark todict () Dictionary representation of the benchmark instance dict: Dictionary containing benchmark information Source code in entities/benchmark.py 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 def todict ( self ) -> dict : \"\"\"Dictionary representation of the benchmark instance Returns: dict: Dictionary containing benchmark information \"\"\" return { \"id\" : self . uid , \"name\" : self . name , \"description\" : self . description , \"docs_url\" : self . docs_url , \"created_at\" : self . created_at , \"modified_at\" : self . modified_at , \"approved_at\" : self . approved_at , \"owner\" : self . owner , \"demo_dataset_tarball_url\" : self . demo_dataset_url , \"demo_dataset_tarball_hash\" : self . demo_dataset_hash , \"demo_dataset_generated_uid\" : self . demo_dataset_generated_uid , \"data_preparation_mlcube\" : int ( self . data_preparation ), \"reference_model_mlcube\" : int ( self . reference_model ), \"models\" : self . models , # not in the server (OK) \"data_evaluator_mlcube\" : int ( self . evaluator ), \"state\" : self . state , \"is_valid\" : self . is_valid , \"is_active\" : self . is_active , \"approval_status\" : self . approval_status . value , \"metadata\" : self . metadata , \"user_metadata\" : self . user_metadata , } upload () Uploads a benchmark to the server Parameters: Name Type Description Default comms Comms communications entity to submit through required Source code in entities/benchmark.py 239 240 241 242 243 244 245 246 247 248 def upload ( self ): \"\"\"Uploads a benchmark to the server Args: comms (Comms): communications entity to submit through \"\"\" body = self . todict () updated_body = config . comms . upload_benchmark ( body ) updated_body [ \"models\" ] = body [ \"models\" ] return updated_body write () Writes the benchmark into disk Parameters: Name Type Description Default filename str name of the file. Defaults to config.benchmarks_filename. required Returns: Name Type Description str str path to the created benchmark file Source code in entities/benchmark.py 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 def write ( self ) -> str : \"\"\"Writes the benchmark into disk Args: filename (str, optional): name of the file. Defaults to config.benchmarks_filename. Returns: str: path to the created benchmark file \"\"\" data = self . todict () storage = storage_path ( config . benchmarks_storage ) bmk_path = os . path . join ( storage , str ( self . uid )) if not os . path . exists ( bmk_path ): os . makedirs ( bmk_path , exist_ok = True ) filepath = os . path . join ( bmk_path , config . benchmarks_filename ) with open ( filepath , \"w\" ) as f : yaml . dump ( data , f ) return filepath","title":"benchmark"},{"location":"reference/entities/benchmark/#entities.benchmark.Benchmark","text":"Bases: Entity Class representing a Benchmark a benchmark is a bundle of assets that enables quantitative measurement of the performance of AI models for a specific clinical problem. A Benchmark instance contains information regarding how to prepare datasets for execution, as well as what models to run and how to evaluate them. Source code in entities/benchmark.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 class Benchmark ( Entity ): \"\"\" Class representing a Benchmark a benchmark is a bundle of assets that enables quantitative measurement of the performance of AI models for a specific clinical problem. A Benchmark instance contains information regarding how to prepare datasets for execution, as well as what models to run and how to evaluate them. \"\"\" def __init__ ( self , bmk_dict : dict ): \"\"\"Creates a new benchmark instance Args: uid (str): The benchmark UID benchmark_dict (dict): key-value representation of the benchmark. \"\"\" self . uid = bmk_dict [ \"id\" ] self . name = bmk_dict [ \"name\" ] self . description = bmk_dict [ \"description\" ] self . docs_url = bmk_dict [ \"docs_url\" ] self . created_at = bmk_dict [ \"created_at\" ] self . modified_at = bmk_dict [ \"modified_at\" ] self . approved_at = bmk_dict [ \"approved_at\" ] self . owner = bmk_dict [ \"owner\" ] self . demo_dataset_url = bmk_dict [ \"demo_dataset_tarball_url\" ] self . demo_dataset_hash = bmk_dict [ \"demo_dataset_tarball_hash\" ] self . demo_dataset_generated_uid = bmk_dict [ \"demo_dataset_generated_uid\" ] self . data_preparation = bmk_dict [ \"data_preparation_mlcube\" ] self . reference_model = bmk_dict [ \"reference_model_mlcube\" ] self . evaluator = bmk_dict [ \"data_evaluator_mlcube\" ] self . models = bmk_dict [ \"models\" ] self . state = bmk_dict [ \"state\" ] self . is_valid = bmk_dict [ \"is_valid\" ] self . is_active = bmk_dict [ \"is_active\" ] self . approval_status = Status ( bmk_dict [ \"approval_status\" ]) self . metadata = bmk_dict [ \"metadata\" ] self . user_metadata = bmk_dict [ \"user_metadata\" ] @classmethod def all ( cls ) -> List [ \"Benchmark\" ]: \"\"\"Gets and creates instances of all locally present benchmarks Returns: List[Benchmark]: a list of Benchmark instances. \"\"\" logging . info ( \"Retrieving all benchmarks\" ) bmks_storage = storage_path ( config . benchmarks_storage ) try : uids = next ( os . walk ( bmks_storage ))[ 1 ] except StopIteration : msg = \"Couldn't iterate over benchmarks directory\" logging . warning ( msg ) raise RuntimeError ( msg ) benchmarks = [ cls . get ( uid ) for uid in uids ] return benchmarks @classmethod def get ( cls , benchmark_uid : str ) -> \"Benchmark\" : \"\"\"Retrieves and creates a Benchmark instance from the server. If benchmark already exists in the platform then retrieve that version. Args: benchmark_uid (str): UID of the benchmark. comms (Comms): Instance of a communication interface. Returns: Benchmark: a Benchmark instance with the retrieved data. \"\"\" comms = config . comms # Try to download first try : benchmark_dict = comms . get_benchmark ( benchmark_uid ) ref_model = benchmark_dict [ \"reference_model_mlcube\" ] add_models = cls . get_models_uids ( benchmark_uid ) benchmark_dict [ \"models\" ] = [ ref_model ] + add_models except CommunicationRetrievalError : # Get local benchmarks logging . warning ( f \"Getting benchmark { benchmark_uid } from comms failed\" ) logging . info ( f \"Looking for benchmark { benchmark_uid } locally\" ) bmk_storage = storage_path ( config . benchmarks_storage ) local_bmks = os . listdir ( bmk_storage ) if str ( benchmark_uid ) in local_bmks : benchmark_dict = cls . __get_local_dict ( benchmark_uid ) else : raise InvalidArgumentError ( \"No benchmark with the given uid could be found\" ) benchmark = cls ( benchmark_dict ) benchmark . write () return benchmark @classmethod def __get_local_dict ( cls , benchmark_uid : str ) -> dict : \"\"\"Retrieves a local benchmark information Args: benchmark_uid (str): uid of the local benchmark Returns: dict: information of the benchmark \"\"\" logging . info ( f \"Retrieving benchmark { benchmark_uid } from local storage\" ) storage = storage_path ( config . benchmarks_storage ) bmk_storage = os . path . join ( storage , str ( benchmark_uid )) bmk_file = os . path . join ( bmk_storage , config . benchmarks_filename ) with open ( bmk_file , \"r\" ) as f : data = yaml . safe_load ( f ) return data @classmethod def tmp ( cls , data_preparator : str , model : str , evaluator : str , demo_url : str = None , demo_hash : str = None , ) -> \"Benchmark\" : \"\"\"Creates a temporary instance of the benchmark Args: data_preparator (str): UID of the data preparator cube to use. model (str): UID of the model cube to use. evaluator (str): UID of the evaluator cube to use. demo_url (str, optional): URL to obtain the demo dataset. Defaults to None. demo_hash (str, optional): Hash of the demo dataset tarball file. Defaults to None. Returns: Benchmark: a benchmark instance \"\"\" benchmark_uid = f \" { config . tmp_prefix }{ data_preparator } _ { model } _ { evaluator } \" benchmark_dict = { \"id\" : benchmark_uid , \"name\" : benchmark_uid , \"data_preparation_mlcube\" : data_preparator , \"reference_model_mlcube\" : model , \"data_evaluator_mlcube\" : evaluator , \"demo_dataset_tarball_url\" : demo_url , \"demo_dataset_tarball_hash\" : demo_hash , \"models\" : [ model ], # not in the server (OK) \"description\" : None , \"docs_url\" : None , \"created_at\" : None , \"modified_at\" : None , \"approved_at\" : None , \"owner\" : None , \"demo_dataset_generated_uid\" : None , \"state\" : \"DEVELOPMENT\" , \"is_valid\" : True , \"is_active\" : True , \"approval_status\" : Status . PENDING . value , \"metadata\" : {}, \"user_metadata\" : {}, } benchmark = cls ( benchmark_dict ) benchmark . write () return benchmark @classmethod def get_models_uids ( cls , benchmark_uid : str ) -> List [ str ]: \"\"\"Retrieves the list of models associated to the benchmark Args: benchmark_uid (str): UID of the benchmark. comms (Comms): Instance of the communications interface. Returns: List[str]: List of mlcube uids \"\"\" return config . comms . get_benchmark_models ( benchmark_uid ) def todict ( self ) -> dict : \"\"\"Dictionary representation of the benchmark instance Returns: dict: Dictionary containing benchmark information \"\"\" return { \"id\" : self . uid , \"name\" : self . name , \"description\" : self . description , \"docs_url\" : self . docs_url , \"created_at\" : self . created_at , \"modified_at\" : self . modified_at , \"approved_at\" : self . approved_at , \"owner\" : self . owner , \"demo_dataset_tarball_url\" : self . demo_dataset_url , \"demo_dataset_tarball_hash\" : self . demo_dataset_hash , \"demo_dataset_generated_uid\" : self . demo_dataset_generated_uid , \"data_preparation_mlcube\" : int ( self . data_preparation ), \"reference_model_mlcube\" : int ( self . reference_model ), \"models\" : self . models , # not in the server (OK) \"data_evaluator_mlcube\" : int ( self . evaluator ), \"state\" : self . state , \"is_valid\" : self . is_valid , \"is_active\" : self . is_active , \"approval_status\" : self . approval_status . value , \"metadata\" : self . metadata , \"user_metadata\" : self . user_metadata , } def write ( self ) -> str : \"\"\"Writes the benchmark into disk Args: filename (str, optional): name of the file. Defaults to config.benchmarks_filename. Returns: str: path to the created benchmark file \"\"\" data = self . todict () storage = storage_path ( config . benchmarks_storage ) bmk_path = os . path . join ( storage , str ( self . uid )) if not os . path . exists ( bmk_path ): os . makedirs ( bmk_path , exist_ok = True ) filepath = os . path . join ( bmk_path , config . benchmarks_filename ) with open ( filepath , \"w\" ) as f : yaml . dump ( data , f ) return filepath def upload ( self ): \"\"\"Uploads a benchmark to the server Args: comms (Comms): communications entity to submit through \"\"\" body = self . todict () updated_body = config . comms . upload_benchmark ( body ) updated_body [ \"models\" ] = body [ \"models\" ] return updated_body","title":"Benchmark"},{"location":"reference/entities/benchmark/#entities.benchmark.Benchmark.__get_local_dict","text":"Retrieves a local benchmark information Parameters: Name Type Description Default benchmark_uid str uid of the local benchmark required Returns: Name Type Description dict dict information of the benchmark Source code in entities/benchmark.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @classmethod def __get_local_dict ( cls , benchmark_uid : str ) -> dict : \"\"\"Retrieves a local benchmark information Args: benchmark_uid (str): uid of the local benchmark Returns: dict: information of the benchmark \"\"\" logging . info ( f \"Retrieving benchmark { benchmark_uid } from local storage\" ) storage = storage_path ( config . benchmarks_storage ) bmk_storage = os . path . join ( storage , str ( benchmark_uid )) bmk_file = os . path . join ( bmk_storage , config . benchmarks_filename ) with open ( bmk_file , \"r\" ) as f : data = yaml . safe_load ( f ) return data","title":"__get_local_dict()"},{"location":"reference/entities/benchmark/#entities.benchmark.Benchmark.__init__","text":"Creates a new benchmark instance Parameters: Name Type Description Default uid str The benchmark UID required benchmark_dict dict key-value representation of the benchmark. required Source code in entities/benchmark.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def __init__ ( self , bmk_dict : dict ): \"\"\"Creates a new benchmark instance Args: uid (str): The benchmark UID benchmark_dict (dict): key-value representation of the benchmark. \"\"\" self . uid = bmk_dict [ \"id\" ] self . name = bmk_dict [ \"name\" ] self . description = bmk_dict [ \"description\" ] self . docs_url = bmk_dict [ \"docs_url\" ] self . created_at = bmk_dict [ \"created_at\" ] self . modified_at = bmk_dict [ \"modified_at\" ] self . approved_at = bmk_dict [ \"approved_at\" ] self . owner = bmk_dict [ \"owner\" ] self . demo_dataset_url = bmk_dict [ \"demo_dataset_tarball_url\" ] self . demo_dataset_hash = bmk_dict [ \"demo_dataset_tarball_hash\" ] self . demo_dataset_generated_uid = bmk_dict [ \"demo_dataset_generated_uid\" ] self . data_preparation = bmk_dict [ \"data_preparation_mlcube\" ] self . reference_model = bmk_dict [ \"reference_model_mlcube\" ] self . evaluator = bmk_dict [ \"data_evaluator_mlcube\" ] self . models = bmk_dict [ \"models\" ] self . state = bmk_dict [ \"state\" ] self . is_valid = bmk_dict [ \"is_valid\" ] self . is_active = bmk_dict [ \"is_active\" ] self . approval_status = Status ( bmk_dict [ \"approval_status\" ]) self . metadata = bmk_dict [ \"metadata\" ] self . user_metadata = bmk_dict [ \"user_metadata\" ]","title":"__init__()"},{"location":"reference/entities/benchmark/#entities.benchmark.Benchmark.all","text":"Gets and creates instances of all locally present benchmarks Returns: Type Description List [ Benchmark ] List[Benchmark]: a list of Benchmark instances. Source code in entities/benchmark.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 @classmethod def all ( cls ) -> List [ \"Benchmark\" ]: \"\"\"Gets and creates instances of all locally present benchmarks Returns: List[Benchmark]: a list of Benchmark instances. \"\"\" logging . info ( \"Retrieving all benchmarks\" ) bmks_storage = storage_path ( config . benchmarks_storage ) try : uids = next ( os . walk ( bmks_storage ))[ 1 ] except StopIteration : msg = \"Couldn't iterate over benchmarks directory\" logging . warning ( msg ) raise RuntimeError ( msg ) benchmarks = [ cls . get ( uid ) for uid in uids ] return benchmarks","title":"all()"},{"location":"reference/entities/benchmark/#entities.benchmark.Benchmark.get","text":"Retrieves and creates a Benchmark instance from the server. If benchmark already exists in the platform then retrieve that version. Parameters: Name Type Description Default benchmark_uid str UID of the benchmark. required comms Comms Instance of a communication interface. required Returns: Name Type Description Benchmark Benchmark a Benchmark instance with the retrieved data. Source code in entities/benchmark.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 @classmethod def get ( cls , benchmark_uid : str ) -> \"Benchmark\" : \"\"\"Retrieves and creates a Benchmark instance from the server. If benchmark already exists in the platform then retrieve that version. Args: benchmark_uid (str): UID of the benchmark. comms (Comms): Instance of a communication interface. Returns: Benchmark: a Benchmark instance with the retrieved data. \"\"\" comms = config . comms # Try to download first try : benchmark_dict = comms . get_benchmark ( benchmark_uid ) ref_model = benchmark_dict [ \"reference_model_mlcube\" ] add_models = cls . get_models_uids ( benchmark_uid ) benchmark_dict [ \"models\" ] = [ ref_model ] + add_models except CommunicationRetrievalError : # Get local benchmarks logging . warning ( f \"Getting benchmark { benchmark_uid } from comms failed\" ) logging . info ( f \"Looking for benchmark { benchmark_uid } locally\" ) bmk_storage = storage_path ( config . benchmarks_storage ) local_bmks = os . listdir ( bmk_storage ) if str ( benchmark_uid ) in local_bmks : benchmark_dict = cls . __get_local_dict ( benchmark_uid ) else : raise InvalidArgumentError ( \"No benchmark with the given uid could be found\" ) benchmark = cls ( benchmark_dict ) benchmark . write () return benchmark","title":"get()"},{"location":"reference/entities/benchmark/#entities.benchmark.Benchmark.get_models_uids","text":"Retrieves the list of models associated to the benchmark Parameters: Name Type Description Default benchmark_uid str UID of the benchmark. required comms Comms Instance of the communications interface. required Returns: Type Description List [ str ] List[str]: List of mlcube uids Source code in entities/benchmark.py 177 178 179 180 181 182 183 184 185 186 187 188 @classmethod def get_models_uids ( cls , benchmark_uid : str ) -> List [ str ]: \"\"\"Retrieves the list of models associated to the benchmark Args: benchmark_uid (str): UID of the benchmark. comms (Comms): Instance of the communications interface. Returns: List[str]: List of mlcube uids \"\"\" return config . comms . get_benchmark_models ( benchmark_uid )","title":"get_models_uids()"},{"location":"reference/entities/benchmark/#entities.benchmark.Benchmark.tmp","text":"Creates a temporary instance of the benchmark Parameters: Name Type Description Default data_preparator str UID of the data preparator cube to use. required model str UID of the model cube to use. required evaluator str UID of the evaluator cube to use. required demo_url str URL to obtain the demo dataset. Defaults to None. None demo_hash str Hash of the demo dataset tarball file. Defaults to None. None Returns: Name Type Description Benchmark Benchmark a benchmark instance Source code in entities/benchmark.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 @classmethod def tmp ( cls , data_preparator : str , model : str , evaluator : str , demo_url : str = None , demo_hash : str = None , ) -> \"Benchmark\" : \"\"\"Creates a temporary instance of the benchmark Args: data_preparator (str): UID of the data preparator cube to use. model (str): UID of the model cube to use. evaluator (str): UID of the evaluator cube to use. demo_url (str, optional): URL to obtain the demo dataset. Defaults to None. demo_hash (str, optional): Hash of the demo dataset tarball file. Defaults to None. Returns: Benchmark: a benchmark instance \"\"\" benchmark_uid = f \" { config . tmp_prefix }{ data_preparator } _ { model } _ { evaluator } \" benchmark_dict = { \"id\" : benchmark_uid , \"name\" : benchmark_uid , \"data_preparation_mlcube\" : data_preparator , \"reference_model_mlcube\" : model , \"data_evaluator_mlcube\" : evaluator , \"demo_dataset_tarball_url\" : demo_url , \"demo_dataset_tarball_hash\" : demo_hash , \"models\" : [ model ], # not in the server (OK) \"description\" : None , \"docs_url\" : None , \"created_at\" : None , \"modified_at\" : None , \"approved_at\" : None , \"owner\" : None , \"demo_dataset_generated_uid\" : None , \"state\" : \"DEVELOPMENT\" , \"is_valid\" : True , \"is_active\" : True , \"approval_status\" : Status . PENDING . value , \"metadata\" : {}, \"user_metadata\" : {}, } benchmark = cls ( benchmark_dict ) benchmark . write () return benchmark","title":"tmp()"},{"location":"reference/entities/benchmark/#entities.benchmark.Benchmark.todict","text":"Dictionary representation of the benchmark instance dict: Dictionary containing benchmark information Source code in entities/benchmark.py 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 def todict ( self ) -> dict : \"\"\"Dictionary representation of the benchmark instance Returns: dict: Dictionary containing benchmark information \"\"\" return { \"id\" : self . uid , \"name\" : self . name , \"description\" : self . description , \"docs_url\" : self . docs_url , \"created_at\" : self . created_at , \"modified_at\" : self . modified_at , \"approved_at\" : self . approved_at , \"owner\" : self . owner , \"demo_dataset_tarball_url\" : self . demo_dataset_url , \"demo_dataset_tarball_hash\" : self . demo_dataset_hash , \"demo_dataset_generated_uid\" : self . demo_dataset_generated_uid , \"data_preparation_mlcube\" : int ( self . data_preparation ), \"reference_model_mlcube\" : int ( self . reference_model ), \"models\" : self . models , # not in the server (OK) \"data_evaluator_mlcube\" : int ( self . evaluator ), \"state\" : self . state , \"is_valid\" : self . is_valid , \"is_active\" : self . is_active , \"approval_status\" : self . approval_status . value , \"metadata\" : self . metadata , \"user_metadata\" : self . user_metadata , }","title":"todict()"},{"location":"reference/entities/benchmark/#entities.benchmark.Benchmark.upload","text":"Uploads a benchmark to the server Parameters: Name Type Description Default comms Comms communications entity to submit through required Source code in entities/benchmark.py 239 240 241 242 243 244 245 246 247 248 def upload ( self ): \"\"\"Uploads a benchmark to the server Args: comms (Comms): communications entity to submit through \"\"\" body = self . todict () updated_body = config . comms . upload_benchmark ( body ) updated_body [ \"models\" ] = body [ \"models\" ] return updated_body","title":"upload()"},{"location":"reference/entities/benchmark/#entities.benchmark.Benchmark.write","text":"Writes the benchmark into disk Parameters: Name Type Description Default filename str name of the file. Defaults to config.benchmarks_filename. required Returns: Name Type Description str str path to the created benchmark file Source code in entities/benchmark.py 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 def write ( self ) -> str : \"\"\"Writes the benchmark into disk Args: filename (str, optional): name of the file. Defaults to config.benchmarks_filename. Returns: str: path to the created benchmark file \"\"\" data = self . todict () storage = storage_path ( config . benchmarks_storage ) bmk_path = os . path . join ( storage , str ( self . uid )) if not os . path . exists ( bmk_path ): os . makedirs ( bmk_path , exist_ok = True ) filepath = os . path . join ( bmk_path , config . benchmarks_filename ) with open ( filepath , \"w\" ) as f : yaml . dump ( data , f ) return filepath","title":"write()"},{"location":"reference/entities/cube/","text":"Cube Bases: Entity Class representing an MLCube Container Medperf platform uses the MLCube container for components such as Dataset Preparation, Evaluation, and the Registered Models. MLCube containers are software containers (e.g., Docker and Singularity) with standard metadata and a consistent file-system level interface. Source code in entities/cube.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 class Cube ( Entity ): \"\"\" Class representing an MLCube Container Medperf platform uses the MLCube container for components such as Dataset Preparation, Evaluation, and the Registered Models. MLCube containers are software containers (e.g., Docker and Singularity) with standard metadata and a consistent file-system level interface. \"\"\" def __init__ ( self , cube_dict ): \"\"\"Creates a Cube instance Args: cube_dict (dict): Dict for information regarding the cube. \"\"\" self . uid = cube_dict [ \"id\" ] self . name = cube_dict [ \"name\" ] self . git_mlcube_url = cube_dict [ \"git_mlcube_url\" ] self . git_parameters_url = cube_dict [ \"git_parameters_url\" ] self . mlcube_hash = cube_dict [ \"mlcube_hash\" ] self . parameters_hash = cube_dict [ \"parameters_hash\" ] self . image_tarball_url = cube_dict [ \"image_tarball_url\" ] self . image_tarball_hash = cube_dict [ \"image_tarball_hash\" ] if \"tarball_url\" in cube_dict : # Backwards compatibility for cubes with # tarball_url instead of additional_files_tarball_url self . additional_files_tarball_url = cube_dict [ \"tarball_url\" ] else : self . additional_files_tarball_url = cube_dict [ \"additional_files_tarball_url\" ] if \"tarball_hash\" in cube_dict : # Backwards compatibility for cubes with # tarball_hash instead of additional_files_tarball_hash self . additional_hash = cube_dict [ \"tarball_hash\" ] else : self . additional_hash = cube_dict [ \"additional_files_tarball_hash\" ] self . state = cube_dict [ \"state\" ] self . is_cube_valid = cube_dict [ \"is_valid\" ] self . owner = cube_dict [ \"owner\" ] self . metadata = cube_dict [ \"metadata\" ] self . user_metadata = cube_dict [ \"user_metadata\" ] self . created_at = cube_dict [ \"created_at\" ] self . modified_at = cube_dict [ \"modified_at\" ] cubes_storage = storage_path ( config . cubes_storage ) self . cube_path = os . path . join ( cubes_storage , str ( self . uid ), config . cube_filename ) self . params_path = None if self . git_parameters_url : self . params_path = os . path . join ( cubes_storage , str ( self . uid ), config . params_filename ) @classmethod def all ( cls ) -> List [ \"Cube\" ]: \"\"\"Class method for retrieving all cubes stored on the user's machine. Args: ui (UI): Instance of an UI implementation. Returns: List[Cube]: List containing all cubes found locally \"\"\" logging . info ( \"Retrieving all local cubes\" ) cubes_storage = storage_path ( config . cubes_storage ) try : uids = next ( os . walk ( cubes_storage ))[ 1 ] except StopIteration : msg = \"Couldn't iterate over cubes directory\" logging . warning ( msg ) raise RuntimeError ( msg ) cubes = [] for uid in uids : meta = cls . __get_local_dict ( uid ) cube = cls ( meta ) cubes . append ( cube ) return cubes @classmethod def get ( cls , cube_uid : str ) -> \"Cube\" : \"\"\"Retrieves and creates a Cube instance from the comms. If cube already exists inside the user's computer then retrieves it from there. Args: cube_uid (str): UID of the cube. Returns: Cube : a Cube instance with the retrieved data. \"\"\" logging . debug ( f \"Retrieving the cube { cube_uid } \" ) comms = config . comms # Try to download the cube first try : meta = comms . get_cube_metadata ( cube_uid ) cube = cls ( meta ) attempt = 0 while attempt < config . cube_get_max_attempts : logging . info ( f \"Downloading MLCube. Attempt { attempt + 1 } \" ) # Check first if we already have the required files if cube . is_valid (): cube . write () return cube # Try to redownload elements if invalid cube . download () attempt += 1 except CommunicationRetrievalError : logging . warning ( \"Max download attempts reached\" ) logging . warning ( f \"Getting MLCube { cube_uid } from comms failed\" ) logging . info ( f \"Retrieving MLCube { cube_uid } from local storage\" ) local_cube = list ( filter ( lambda cube : str ( cube . uid ) == str ( cube_uid ), cls . all ()) ) if len ( local_cube ) == 1 : logging . debug ( \"Found cube locally\" ) return local_cube [ 0 ] logging . error ( \"Could not find the requested MLCube\" ) cube_path = os . path . join ( storage_path ( config . cubes_storage ), str ( cube_uid )) cleanup ([ cube_path ]) raise InvalidEntityError ( \"Could not successfully get the requested MLCube\" ) def download_mlcube ( self ): url = self . git_mlcube_url path = config . comms . get_cube ( url , self . uid ) local_hash = get_file_sha1 ( path ) if not self . mlcube_hash : self . mlcube_hash = local_hash self . cube_path = path return local_hash def download_parameters ( self ): url = self . git_parameters_url if url : path = config . comms . get_cube_params ( url , self . uid ) local_hash = get_file_sha1 ( path ) if not self . parameters_hash : self . parameters_hash = local_hash self . params_path = path return local_hash return \"\" def download_additional ( self ): url = self . additional_files_tarball_url if url : path = config . comms . get_cube_additional ( url , self . uid ) local_hash = get_file_sha1 ( path ) if not self . additional_hash : self . additional_hash = local_hash untar ( path ) return local_hash return \"\" def download_image ( self ): url = self . image_tarball_url if url : path = config . comms . get_cube_image ( url , self . uid ) local_hash = get_file_sha1 ( path ) if not self . image_tarball_hash : self . image_tarball_hash = local_hash untar ( path ) return local_hash else : # Retrieve image from image registry logging . debug ( f \"Retrieving { self . uid } image\" ) cmd = f \"mlcube configure --mlcube= { self . cube_path } \" proc = pexpect . spawn ( cmd ) proc_out = combine_proc_sp_text ( proc ) logging . debug ( proc_out ) proc . close () return \"\" def download ( self ): \"\"\"Downloads the required elements for an mlcube to run locally. \"\"\" local_hashes = { \"mlcube_hash\" : self . download_mlcube (), \"parameters_hash\" : self . download_parameters (), \"additional_files_tarball_hash\" : self . download_additional (), \"image_tarball_hash\" : self . download_image (), } self . store_local_hashes ( local_hashes ) def is_valid ( self ) -> bool : \"\"\"Checks the validity of the cube and related files through hash checking. Returns: bool: Wether the cube and related files match the expeced hashes \"\"\" try : local_hashes = self . get_local_hashes () except FileNotFoundError : logging . warning ( \"Local MLCube files not found. Defaulting to invalid\" ) return False valid_cube = self . is_cube_valid valid_hashes = True server_hashes = self . todict () for key in local_hashes : if local_hashes [ key ]: if local_hashes [ key ] != server_hashes [ key ]: valid_hashes = False msg = f \" { key . replace ( '_' , ' ' ) } doesn't match\" config . ui . print_error ( msg ) return valid_cube and valid_hashes def run ( self , task : str , string_params : Dict [ str , str ] = {}, timeout : int = None , ** kwargs , ): \"\"\"Executes a given task on the cube instance Args: task (str): task to run string_params (Dict[str], optional): Extra parameters that can't be passed as normal function args. Defaults to {}. timeout (int, optional): timeout for the task in seconds. Defaults to None. kwargs (dict): additional arguments that are passed directly to the mlcube command \"\"\" kwargs . update ( string_params ) cmd = f \"mlcube run --mlcube= { self . cube_path } --task= { task } --platform= { config . platform } \" for k , v in kwargs . items (): cmd_arg = f ' { k } =\" { v } \"' cmd = \" \" . join ([ cmd , cmd_arg ]) logging . info ( f \"Running MLCube command: { cmd } \" ) proc = pexpect . spawn ( cmd , timeout = timeout ) proc_out = combine_proc_sp_text ( proc ) proc . close () logging . debug ( proc_out ) if proc . exitstatus != 0 : raise RuntimeError ( \"There was an error while executing the cube\" ) logging . debug ( list_files ( config . storage )) return proc def get_default_output ( self , task : str , out_key : str , param_key : str = None ) -> str : \"\"\"Returns the output parameter specified in the mlcube.yaml file Args: task (str): the task of interest out_key (str): key used to identify the desired output in the yaml file param_key (str): key inside the parameters file that completes the output path. Defaults to None. Returns: str: the path as specified in the mlcube.yaml file for the desired output for the desired task. Defaults to None if out_key not found \"\"\" with open ( self . cube_path , \"r\" ) as f : cube = yaml . safe_load ( f ) out_params = cube [ \"tasks\" ][ task ][ \"parameters\" ][ \"outputs\" ] if out_key not in out_params : return None out_path = cube [ \"tasks\" ][ task ][ \"parameters\" ][ \"outputs\" ][ out_key ] if type ( out_path ) == dict : # output is specified as a dict with type and default values out_path = out_path [ \"default\" ] cube_loc = str ( Path ( self . cube_path ) . parent ) out_path = os . path . join ( cube_loc , \"workspace\" , out_path ) if self . params_path is not None and param_key is not None : with open ( self . params_path , \"r\" ) as f : params = yaml . safe_load ( f ) out_path = os . path . join ( out_path , params [ param_key ]) return out_path def todict ( self ) -> Dict : return { \"name\" : self . name , \"git_mlcube_url\" : self . git_mlcube_url , \"mlcube_hash\" : self . mlcube_hash , \"git_parameters_url\" : self . git_parameters_url , \"parameters_hash\" : self . parameters_hash , \"image_tarball_url\" : self . image_tarball_url , \"image_tarball_hash\" : self . image_tarball_hash , \"additional_files_tarball_url\" : self . additional_files_tarball_url , \"additional_files_tarball_hash\" : self . additional_hash , \"state\" : self . state , \"is_valid\" : self . is_cube_valid , \"id\" : self . uid , \"owner\" : self . owner , \"metadata\" : self . metadata , \"user_metadata\" : self . user_metadata , \"created_at\" : self . created_at , \"modified_at\" : self . modified_at , } def write ( self ): cube_loc = str ( Path ( self . cube_path ) . parent ) meta_file = os . path . join ( cube_loc , config . cube_metadata_filename ) os . makedirs ( cube_loc , exist_ok = True ) with open ( meta_file , \"w\" ) as f : yaml . dump ( self . todict (), f ) def upload ( self ): cube_dict = self . todict () updated_cube_dict = config . comms . upload_mlcube ( cube_dict ) return updated_cube_dict def get_local_hashes ( self ): cubes_storage = storage_path ( config . cubes_storage ) local_hashes_file = os . path . join ( cubes_storage , str ( self . uid ), config . cube_hashes_filename ) with open ( local_hashes_file , \"r\" ) as f : local_hashes = yaml . safe_load ( f ) return local_hashes def store_local_hashes ( self , local_hashes ): cubes_storage = storage_path ( config . cubes_storage ) local_hashes_file = os . path . join ( cubes_storage , str ( self . uid ), config . cube_hashes_filename ) with open ( local_hashes_file , \"w\" ) as f : yaml . dump ( local_hashes , f ) @classmethod def __get_local_dict ( cls , uid ): cubes_storage = storage_path ( config . cubes_storage ) meta_file = os . path . join ( cubes_storage , uid , config . cube_metadata_filename ) with open ( meta_file , \"r\" ) as f : meta = yaml . safe_load ( f ) return meta __init__ ( cube_dict ) Creates a Cube instance Parameters: Name Type Description Default cube_dict dict Dict for information regarding the cube. required Source code in entities/cube.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def __init__ ( self , cube_dict ): \"\"\"Creates a Cube instance Args: cube_dict (dict): Dict for information regarding the cube. \"\"\" self . uid = cube_dict [ \"id\" ] self . name = cube_dict [ \"name\" ] self . git_mlcube_url = cube_dict [ \"git_mlcube_url\" ] self . git_parameters_url = cube_dict [ \"git_parameters_url\" ] self . mlcube_hash = cube_dict [ \"mlcube_hash\" ] self . parameters_hash = cube_dict [ \"parameters_hash\" ] self . image_tarball_url = cube_dict [ \"image_tarball_url\" ] self . image_tarball_hash = cube_dict [ \"image_tarball_hash\" ] if \"tarball_url\" in cube_dict : # Backwards compatibility for cubes with # tarball_url instead of additional_files_tarball_url self . additional_files_tarball_url = cube_dict [ \"tarball_url\" ] else : self . additional_files_tarball_url = cube_dict [ \"additional_files_tarball_url\" ] if \"tarball_hash\" in cube_dict : # Backwards compatibility for cubes with # tarball_hash instead of additional_files_tarball_hash self . additional_hash = cube_dict [ \"tarball_hash\" ] else : self . additional_hash = cube_dict [ \"additional_files_tarball_hash\" ] self . state = cube_dict [ \"state\" ] self . is_cube_valid = cube_dict [ \"is_valid\" ] self . owner = cube_dict [ \"owner\" ] self . metadata = cube_dict [ \"metadata\" ] self . user_metadata = cube_dict [ \"user_metadata\" ] self . created_at = cube_dict [ \"created_at\" ] self . modified_at = cube_dict [ \"modified_at\" ] cubes_storage = storage_path ( config . cubes_storage ) self . cube_path = os . path . join ( cubes_storage , str ( self . uid ), config . cube_filename ) self . params_path = None if self . git_parameters_url : self . params_path = os . path . join ( cubes_storage , str ( self . uid ), config . params_filename ) all () classmethod Class method for retrieving all cubes stored on the user's machine. Parameters: Name Type Description Default ui UI Instance of an UI implementation. required Returns: Type Description List [ Cube ] List[Cube]: List containing all cubes found locally Source code in entities/cube.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 @classmethod def all ( cls ) -> List [ \"Cube\" ]: \"\"\"Class method for retrieving all cubes stored on the user's machine. Args: ui (UI): Instance of an UI implementation. Returns: List[Cube]: List containing all cubes found locally \"\"\" logging . info ( \"Retrieving all local cubes\" ) cubes_storage = storage_path ( config . cubes_storage ) try : uids = next ( os . walk ( cubes_storage ))[ 1 ] except StopIteration : msg = \"Couldn't iterate over cubes directory\" logging . warning ( msg ) raise RuntimeError ( msg ) cubes = [] for uid in uids : meta = cls . __get_local_dict ( uid ) cube = cls ( meta ) cubes . append ( cube ) return cubes download () Downloads the required elements for an mlcube to run locally. Source code in entities/cube.py 201 202 203 204 205 206 207 208 209 210 211 def download ( self ): \"\"\"Downloads the required elements for an mlcube to run locally. \"\"\" local_hashes = { \"mlcube_hash\" : self . download_mlcube (), \"parameters_hash\" : self . download_parameters (), \"additional_files_tarball_hash\" : self . download_additional (), \"image_tarball_hash\" : self . download_image (), } self . store_local_hashes ( local_hashes ) get ( cube_uid ) classmethod Retrieves and creates a Cube instance from the comms. If cube already exists inside the user's computer then retrieves it from there. Parameters: Name Type Description Default cube_uid str UID of the cube. required Returns: Name Type Description Cube Cube a Cube instance with the retrieved data. Source code in entities/cube.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 @classmethod def get ( cls , cube_uid : str ) -> \"Cube\" : \"\"\"Retrieves and creates a Cube instance from the comms. If cube already exists inside the user's computer then retrieves it from there. Args: cube_uid (str): UID of the cube. Returns: Cube : a Cube instance with the retrieved data. \"\"\" logging . debug ( f \"Retrieving the cube { cube_uid } \" ) comms = config . comms # Try to download the cube first try : meta = comms . get_cube_metadata ( cube_uid ) cube = cls ( meta ) attempt = 0 while attempt < config . cube_get_max_attempts : logging . info ( f \"Downloading MLCube. Attempt { attempt + 1 } \" ) # Check first if we already have the required files if cube . is_valid (): cube . write () return cube # Try to redownload elements if invalid cube . download () attempt += 1 except CommunicationRetrievalError : logging . warning ( \"Max download attempts reached\" ) logging . warning ( f \"Getting MLCube { cube_uid } from comms failed\" ) logging . info ( f \"Retrieving MLCube { cube_uid } from local storage\" ) local_cube = list ( filter ( lambda cube : str ( cube . uid ) == str ( cube_uid ), cls . all ()) ) if len ( local_cube ) == 1 : logging . debug ( \"Found cube locally\" ) return local_cube [ 0 ] logging . error ( \"Could not find the requested MLCube\" ) cube_path = os . path . join ( storage_path ( config . cubes_storage ), str ( cube_uid )) cleanup ([ cube_path ]) raise InvalidEntityError ( \"Could not successfully get the requested MLCube\" ) get_default_output ( task , out_key , param_key = None ) Returns the output parameter specified in the mlcube.yaml file Parameters: Name Type Description Default task str the task of interest required out_key str key used to identify the desired output in the yaml file required param_key str key inside the parameters file that completes the output path. Defaults to None. None Returns: Name Type Description str str the path as specified in the mlcube.yaml file for the desired output for the desired task. Defaults to None if out_key not found Source code in entities/cube.py 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 def get_default_output ( self , task : str , out_key : str , param_key : str = None ) -> str : \"\"\"Returns the output parameter specified in the mlcube.yaml file Args: task (str): the task of interest out_key (str): key used to identify the desired output in the yaml file param_key (str): key inside the parameters file that completes the output path. Defaults to None. Returns: str: the path as specified in the mlcube.yaml file for the desired output for the desired task. Defaults to None if out_key not found \"\"\" with open ( self . cube_path , \"r\" ) as f : cube = yaml . safe_load ( f ) out_params = cube [ \"tasks\" ][ task ][ \"parameters\" ][ \"outputs\" ] if out_key not in out_params : return None out_path = cube [ \"tasks\" ][ task ][ \"parameters\" ][ \"outputs\" ][ out_key ] if type ( out_path ) == dict : # output is specified as a dict with type and default values out_path = out_path [ \"default\" ] cube_loc = str ( Path ( self . cube_path ) . parent ) out_path = os . path . join ( cube_loc , \"workspace\" , out_path ) if self . params_path is not None and param_key is not None : with open ( self . params_path , \"r\" ) as f : params = yaml . safe_load ( f ) out_path = os . path . join ( out_path , params [ param_key ]) return out_path is_valid () Checks the validity of the cube and related files through hash checking. Returns: Name Type Description bool bool Wether the cube and related files match the expeced hashes Source code in entities/cube.py 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 def is_valid ( self ) -> bool : \"\"\"Checks the validity of the cube and related files through hash checking. Returns: bool: Wether the cube and related files match the expeced hashes \"\"\" try : local_hashes = self . get_local_hashes () except FileNotFoundError : logging . warning ( \"Local MLCube files not found. Defaulting to invalid\" ) return False valid_cube = self . is_cube_valid valid_hashes = True server_hashes = self . todict () for key in local_hashes : if local_hashes [ key ]: if local_hashes [ key ] != server_hashes [ key ]: valid_hashes = False msg = f \" { key . replace ( '_' , ' ' ) } doesn't match\" config . ui . print_error ( msg ) return valid_cube and valid_hashes run ( task , string_params = {}, timeout = None , kwargs ) Executes a given task on the cube instance Parameters: Name Type Description Default task str task to run required string_params Dict [ str ] Extra parameters that can't be passed as normal function args. Defaults to {}. {} timeout int timeout for the task in seconds. Defaults to None. None kwargs dict additional arguments that are passed directly to the mlcube command {} Source code in entities/cube.py 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 def run ( self , task : str , string_params : Dict [ str , str ] = {}, timeout : int = None , ** kwargs , ): \"\"\"Executes a given task on the cube instance Args: task (str): task to run string_params (Dict[str], optional): Extra parameters that can't be passed as normal function args. Defaults to {}. timeout (int, optional): timeout for the task in seconds. Defaults to None. kwargs (dict): additional arguments that are passed directly to the mlcube command \"\"\" kwargs . update ( string_params ) cmd = f \"mlcube run --mlcube= { self . cube_path } --task= { task } --platform= { config . platform } \" for k , v in kwargs . items (): cmd_arg = f ' { k } =\" { v } \"' cmd = \" \" . join ([ cmd , cmd_arg ]) logging . info ( f \"Running MLCube command: { cmd } \" ) proc = pexpect . spawn ( cmd , timeout = timeout ) proc_out = combine_proc_sp_text ( proc ) proc . close () logging . debug ( proc_out ) if proc . exitstatus != 0 : raise RuntimeError ( \"There was an error while executing the cube\" ) logging . debug ( list_files ( config . storage )) return proc","title":"cube"},{"location":"reference/entities/cube/#entities.cube.Cube","text":"Bases: Entity Class representing an MLCube Container Medperf platform uses the MLCube container for components such as Dataset Preparation, Evaluation, and the Registered Models. MLCube containers are software containers (e.g., Docker and Singularity) with standard metadata and a consistent file-system level interface. Source code in entities/cube.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 class Cube ( Entity ): \"\"\" Class representing an MLCube Container Medperf platform uses the MLCube container for components such as Dataset Preparation, Evaluation, and the Registered Models. MLCube containers are software containers (e.g., Docker and Singularity) with standard metadata and a consistent file-system level interface. \"\"\" def __init__ ( self , cube_dict ): \"\"\"Creates a Cube instance Args: cube_dict (dict): Dict for information regarding the cube. \"\"\" self . uid = cube_dict [ \"id\" ] self . name = cube_dict [ \"name\" ] self . git_mlcube_url = cube_dict [ \"git_mlcube_url\" ] self . git_parameters_url = cube_dict [ \"git_parameters_url\" ] self . mlcube_hash = cube_dict [ \"mlcube_hash\" ] self . parameters_hash = cube_dict [ \"parameters_hash\" ] self . image_tarball_url = cube_dict [ \"image_tarball_url\" ] self . image_tarball_hash = cube_dict [ \"image_tarball_hash\" ] if \"tarball_url\" in cube_dict : # Backwards compatibility for cubes with # tarball_url instead of additional_files_tarball_url self . additional_files_tarball_url = cube_dict [ \"tarball_url\" ] else : self . additional_files_tarball_url = cube_dict [ \"additional_files_tarball_url\" ] if \"tarball_hash\" in cube_dict : # Backwards compatibility for cubes with # tarball_hash instead of additional_files_tarball_hash self . additional_hash = cube_dict [ \"tarball_hash\" ] else : self . additional_hash = cube_dict [ \"additional_files_tarball_hash\" ] self . state = cube_dict [ \"state\" ] self . is_cube_valid = cube_dict [ \"is_valid\" ] self . owner = cube_dict [ \"owner\" ] self . metadata = cube_dict [ \"metadata\" ] self . user_metadata = cube_dict [ \"user_metadata\" ] self . created_at = cube_dict [ \"created_at\" ] self . modified_at = cube_dict [ \"modified_at\" ] cubes_storage = storage_path ( config . cubes_storage ) self . cube_path = os . path . join ( cubes_storage , str ( self . uid ), config . cube_filename ) self . params_path = None if self . git_parameters_url : self . params_path = os . path . join ( cubes_storage , str ( self . uid ), config . params_filename ) @classmethod def all ( cls ) -> List [ \"Cube\" ]: \"\"\"Class method for retrieving all cubes stored on the user's machine. Args: ui (UI): Instance of an UI implementation. Returns: List[Cube]: List containing all cubes found locally \"\"\" logging . info ( \"Retrieving all local cubes\" ) cubes_storage = storage_path ( config . cubes_storage ) try : uids = next ( os . walk ( cubes_storage ))[ 1 ] except StopIteration : msg = \"Couldn't iterate over cubes directory\" logging . warning ( msg ) raise RuntimeError ( msg ) cubes = [] for uid in uids : meta = cls . __get_local_dict ( uid ) cube = cls ( meta ) cubes . append ( cube ) return cubes @classmethod def get ( cls , cube_uid : str ) -> \"Cube\" : \"\"\"Retrieves and creates a Cube instance from the comms. If cube already exists inside the user's computer then retrieves it from there. Args: cube_uid (str): UID of the cube. Returns: Cube : a Cube instance with the retrieved data. \"\"\" logging . debug ( f \"Retrieving the cube { cube_uid } \" ) comms = config . comms # Try to download the cube first try : meta = comms . get_cube_metadata ( cube_uid ) cube = cls ( meta ) attempt = 0 while attempt < config . cube_get_max_attempts : logging . info ( f \"Downloading MLCube. Attempt { attempt + 1 } \" ) # Check first if we already have the required files if cube . is_valid (): cube . write () return cube # Try to redownload elements if invalid cube . download () attempt += 1 except CommunicationRetrievalError : logging . warning ( \"Max download attempts reached\" ) logging . warning ( f \"Getting MLCube { cube_uid } from comms failed\" ) logging . info ( f \"Retrieving MLCube { cube_uid } from local storage\" ) local_cube = list ( filter ( lambda cube : str ( cube . uid ) == str ( cube_uid ), cls . all ()) ) if len ( local_cube ) == 1 : logging . debug ( \"Found cube locally\" ) return local_cube [ 0 ] logging . error ( \"Could not find the requested MLCube\" ) cube_path = os . path . join ( storage_path ( config . cubes_storage ), str ( cube_uid )) cleanup ([ cube_path ]) raise InvalidEntityError ( \"Could not successfully get the requested MLCube\" ) def download_mlcube ( self ): url = self . git_mlcube_url path = config . comms . get_cube ( url , self . uid ) local_hash = get_file_sha1 ( path ) if not self . mlcube_hash : self . mlcube_hash = local_hash self . cube_path = path return local_hash def download_parameters ( self ): url = self . git_parameters_url if url : path = config . comms . get_cube_params ( url , self . uid ) local_hash = get_file_sha1 ( path ) if not self . parameters_hash : self . parameters_hash = local_hash self . params_path = path return local_hash return \"\" def download_additional ( self ): url = self . additional_files_tarball_url if url : path = config . comms . get_cube_additional ( url , self . uid ) local_hash = get_file_sha1 ( path ) if not self . additional_hash : self . additional_hash = local_hash untar ( path ) return local_hash return \"\" def download_image ( self ): url = self . image_tarball_url if url : path = config . comms . get_cube_image ( url , self . uid ) local_hash = get_file_sha1 ( path ) if not self . image_tarball_hash : self . image_tarball_hash = local_hash untar ( path ) return local_hash else : # Retrieve image from image registry logging . debug ( f \"Retrieving { self . uid } image\" ) cmd = f \"mlcube configure --mlcube= { self . cube_path } \" proc = pexpect . spawn ( cmd ) proc_out = combine_proc_sp_text ( proc ) logging . debug ( proc_out ) proc . close () return \"\" def download ( self ): \"\"\"Downloads the required elements for an mlcube to run locally. \"\"\" local_hashes = { \"mlcube_hash\" : self . download_mlcube (), \"parameters_hash\" : self . download_parameters (), \"additional_files_tarball_hash\" : self . download_additional (), \"image_tarball_hash\" : self . download_image (), } self . store_local_hashes ( local_hashes ) def is_valid ( self ) -> bool : \"\"\"Checks the validity of the cube and related files through hash checking. Returns: bool: Wether the cube and related files match the expeced hashes \"\"\" try : local_hashes = self . get_local_hashes () except FileNotFoundError : logging . warning ( \"Local MLCube files not found. Defaulting to invalid\" ) return False valid_cube = self . is_cube_valid valid_hashes = True server_hashes = self . todict () for key in local_hashes : if local_hashes [ key ]: if local_hashes [ key ] != server_hashes [ key ]: valid_hashes = False msg = f \" { key . replace ( '_' , ' ' ) } doesn't match\" config . ui . print_error ( msg ) return valid_cube and valid_hashes def run ( self , task : str , string_params : Dict [ str , str ] = {}, timeout : int = None , ** kwargs , ): \"\"\"Executes a given task on the cube instance Args: task (str): task to run string_params (Dict[str], optional): Extra parameters that can't be passed as normal function args. Defaults to {}. timeout (int, optional): timeout for the task in seconds. Defaults to None. kwargs (dict): additional arguments that are passed directly to the mlcube command \"\"\" kwargs . update ( string_params ) cmd = f \"mlcube run --mlcube= { self . cube_path } --task= { task } --platform= { config . platform } \" for k , v in kwargs . items (): cmd_arg = f ' { k } =\" { v } \"' cmd = \" \" . join ([ cmd , cmd_arg ]) logging . info ( f \"Running MLCube command: { cmd } \" ) proc = pexpect . spawn ( cmd , timeout = timeout ) proc_out = combine_proc_sp_text ( proc ) proc . close () logging . debug ( proc_out ) if proc . exitstatus != 0 : raise RuntimeError ( \"There was an error while executing the cube\" ) logging . debug ( list_files ( config . storage )) return proc def get_default_output ( self , task : str , out_key : str , param_key : str = None ) -> str : \"\"\"Returns the output parameter specified in the mlcube.yaml file Args: task (str): the task of interest out_key (str): key used to identify the desired output in the yaml file param_key (str): key inside the parameters file that completes the output path. Defaults to None. Returns: str: the path as specified in the mlcube.yaml file for the desired output for the desired task. Defaults to None if out_key not found \"\"\" with open ( self . cube_path , \"r\" ) as f : cube = yaml . safe_load ( f ) out_params = cube [ \"tasks\" ][ task ][ \"parameters\" ][ \"outputs\" ] if out_key not in out_params : return None out_path = cube [ \"tasks\" ][ task ][ \"parameters\" ][ \"outputs\" ][ out_key ] if type ( out_path ) == dict : # output is specified as a dict with type and default values out_path = out_path [ \"default\" ] cube_loc = str ( Path ( self . cube_path ) . parent ) out_path = os . path . join ( cube_loc , \"workspace\" , out_path ) if self . params_path is not None and param_key is not None : with open ( self . params_path , \"r\" ) as f : params = yaml . safe_load ( f ) out_path = os . path . join ( out_path , params [ param_key ]) return out_path def todict ( self ) -> Dict : return { \"name\" : self . name , \"git_mlcube_url\" : self . git_mlcube_url , \"mlcube_hash\" : self . mlcube_hash , \"git_parameters_url\" : self . git_parameters_url , \"parameters_hash\" : self . parameters_hash , \"image_tarball_url\" : self . image_tarball_url , \"image_tarball_hash\" : self . image_tarball_hash , \"additional_files_tarball_url\" : self . additional_files_tarball_url , \"additional_files_tarball_hash\" : self . additional_hash , \"state\" : self . state , \"is_valid\" : self . is_cube_valid , \"id\" : self . uid , \"owner\" : self . owner , \"metadata\" : self . metadata , \"user_metadata\" : self . user_metadata , \"created_at\" : self . created_at , \"modified_at\" : self . modified_at , } def write ( self ): cube_loc = str ( Path ( self . cube_path ) . parent ) meta_file = os . path . join ( cube_loc , config . cube_metadata_filename ) os . makedirs ( cube_loc , exist_ok = True ) with open ( meta_file , \"w\" ) as f : yaml . dump ( self . todict (), f ) def upload ( self ): cube_dict = self . todict () updated_cube_dict = config . comms . upload_mlcube ( cube_dict ) return updated_cube_dict def get_local_hashes ( self ): cubes_storage = storage_path ( config . cubes_storage ) local_hashes_file = os . path . join ( cubes_storage , str ( self . uid ), config . cube_hashes_filename ) with open ( local_hashes_file , \"r\" ) as f : local_hashes = yaml . safe_load ( f ) return local_hashes def store_local_hashes ( self , local_hashes ): cubes_storage = storage_path ( config . cubes_storage ) local_hashes_file = os . path . join ( cubes_storage , str ( self . uid ), config . cube_hashes_filename ) with open ( local_hashes_file , \"w\" ) as f : yaml . dump ( local_hashes , f ) @classmethod def __get_local_dict ( cls , uid ): cubes_storage = storage_path ( config . cubes_storage ) meta_file = os . path . join ( cubes_storage , uid , config . cube_metadata_filename ) with open ( meta_file , \"r\" ) as f : meta = yaml . safe_load ( f ) return meta","title":"Cube"},{"location":"reference/entities/cube/#entities.cube.Cube.__init__","text":"Creates a Cube instance Parameters: Name Type Description Default cube_dict dict Dict for information regarding the cube. required Source code in entities/cube.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def __init__ ( self , cube_dict ): \"\"\"Creates a Cube instance Args: cube_dict (dict): Dict for information regarding the cube. \"\"\" self . uid = cube_dict [ \"id\" ] self . name = cube_dict [ \"name\" ] self . git_mlcube_url = cube_dict [ \"git_mlcube_url\" ] self . git_parameters_url = cube_dict [ \"git_parameters_url\" ] self . mlcube_hash = cube_dict [ \"mlcube_hash\" ] self . parameters_hash = cube_dict [ \"parameters_hash\" ] self . image_tarball_url = cube_dict [ \"image_tarball_url\" ] self . image_tarball_hash = cube_dict [ \"image_tarball_hash\" ] if \"tarball_url\" in cube_dict : # Backwards compatibility for cubes with # tarball_url instead of additional_files_tarball_url self . additional_files_tarball_url = cube_dict [ \"tarball_url\" ] else : self . additional_files_tarball_url = cube_dict [ \"additional_files_tarball_url\" ] if \"tarball_hash\" in cube_dict : # Backwards compatibility for cubes with # tarball_hash instead of additional_files_tarball_hash self . additional_hash = cube_dict [ \"tarball_hash\" ] else : self . additional_hash = cube_dict [ \"additional_files_tarball_hash\" ] self . state = cube_dict [ \"state\" ] self . is_cube_valid = cube_dict [ \"is_valid\" ] self . owner = cube_dict [ \"owner\" ] self . metadata = cube_dict [ \"metadata\" ] self . user_metadata = cube_dict [ \"user_metadata\" ] self . created_at = cube_dict [ \"created_at\" ] self . modified_at = cube_dict [ \"modified_at\" ] cubes_storage = storage_path ( config . cubes_storage ) self . cube_path = os . path . join ( cubes_storage , str ( self . uid ), config . cube_filename ) self . params_path = None if self . git_parameters_url : self . params_path = os . path . join ( cubes_storage , str ( self . uid ), config . params_filename )","title":"__init__()"},{"location":"reference/entities/cube/#entities.cube.Cube.all","text":"Class method for retrieving all cubes stored on the user's machine. Parameters: Name Type Description Default ui UI Instance of an UI implementation. required Returns: Type Description List [ Cube ] List[Cube]: List containing all cubes found locally Source code in entities/cube.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 @classmethod def all ( cls ) -> List [ \"Cube\" ]: \"\"\"Class method for retrieving all cubes stored on the user's machine. Args: ui (UI): Instance of an UI implementation. Returns: List[Cube]: List containing all cubes found locally \"\"\" logging . info ( \"Retrieving all local cubes\" ) cubes_storage = storage_path ( config . cubes_storage ) try : uids = next ( os . walk ( cubes_storage ))[ 1 ] except StopIteration : msg = \"Couldn't iterate over cubes directory\" logging . warning ( msg ) raise RuntimeError ( msg ) cubes = [] for uid in uids : meta = cls . __get_local_dict ( uid ) cube = cls ( meta ) cubes . append ( cube ) return cubes","title":"all()"},{"location":"reference/entities/cube/#entities.cube.Cube.download","text":"Downloads the required elements for an mlcube to run locally. Source code in entities/cube.py 201 202 203 204 205 206 207 208 209 210 211 def download ( self ): \"\"\"Downloads the required elements for an mlcube to run locally. \"\"\" local_hashes = { \"mlcube_hash\" : self . download_mlcube (), \"parameters_hash\" : self . download_parameters (), \"additional_files_tarball_hash\" : self . download_additional (), \"image_tarball_hash\" : self . download_image (), } self . store_local_hashes ( local_hashes )","title":"download()"},{"location":"reference/entities/cube/#entities.cube.Cube.get","text":"Retrieves and creates a Cube instance from the comms. If cube already exists inside the user's computer then retrieves it from there. Parameters: Name Type Description Default cube_uid str UID of the cube. required Returns: Name Type Description Cube Cube a Cube instance with the retrieved data. Source code in entities/cube.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 @classmethod def get ( cls , cube_uid : str ) -> \"Cube\" : \"\"\"Retrieves and creates a Cube instance from the comms. If cube already exists inside the user's computer then retrieves it from there. Args: cube_uid (str): UID of the cube. Returns: Cube : a Cube instance with the retrieved data. \"\"\" logging . debug ( f \"Retrieving the cube { cube_uid } \" ) comms = config . comms # Try to download the cube first try : meta = comms . get_cube_metadata ( cube_uid ) cube = cls ( meta ) attempt = 0 while attempt < config . cube_get_max_attempts : logging . info ( f \"Downloading MLCube. Attempt { attempt + 1 } \" ) # Check first if we already have the required files if cube . is_valid (): cube . write () return cube # Try to redownload elements if invalid cube . download () attempt += 1 except CommunicationRetrievalError : logging . warning ( \"Max download attempts reached\" ) logging . warning ( f \"Getting MLCube { cube_uid } from comms failed\" ) logging . info ( f \"Retrieving MLCube { cube_uid } from local storage\" ) local_cube = list ( filter ( lambda cube : str ( cube . uid ) == str ( cube_uid ), cls . all ()) ) if len ( local_cube ) == 1 : logging . debug ( \"Found cube locally\" ) return local_cube [ 0 ] logging . error ( \"Could not find the requested MLCube\" ) cube_path = os . path . join ( storage_path ( config . cubes_storage ), str ( cube_uid )) cleanup ([ cube_path ]) raise InvalidEntityError ( \"Could not successfully get the requested MLCube\" )","title":"get()"},{"location":"reference/entities/cube/#entities.cube.Cube.get_default_output","text":"Returns the output parameter specified in the mlcube.yaml file Parameters: Name Type Description Default task str the task of interest required out_key str key used to identify the desired output in the yaml file required param_key str key inside the parameters file that completes the output path. Defaults to None. None Returns: Name Type Description str str the path as specified in the mlcube.yaml file for the desired output for the desired task. Defaults to None if out_key not found Source code in entities/cube.py 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 def get_default_output ( self , task : str , out_key : str , param_key : str = None ) -> str : \"\"\"Returns the output parameter specified in the mlcube.yaml file Args: task (str): the task of interest out_key (str): key used to identify the desired output in the yaml file param_key (str): key inside the parameters file that completes the output path. Defaults to None. Returns: str: the path as specified in the mlcube.yaml file for the desired output for the desired task. Defaults to None if out_key not found \"\"\" with open ( self . cube_path , \"r\" ) as f : cube = yaml . safe_load ( f ) out_params = cube [ \"tasks\" ][ task ][ \"parameters\" ][ \"outputs\" ] if out_key not in out_params : return None out_path = cube [ \"tasks\" ][ task ][ \"parameters\" ][ \"outputs\" ][ out_key ] if type ( out_path ) == dict : # output is specified as a dict with type and default values out_path = out_path [ \"default\" ] cube_loc = str ( Path ( self . cube_path ) . parent ) out_path = os . path . join ( cube_loc , \"workspace\" , out_path ) if self . params_path is not None and param_key is not None : with open ( self . params_path , \"r\" ) as f : params = yaml . safe_load ( f ) out_path = os . path . join ( out_path , params [ param_key ]) return out_path","title":"get_default_output()"},{"location":"reference/entities/cube/#entities.cube.Cube.is_valid","text":"Checks the validity of the cube and related files through hash checking. Returns: Name Type Description bool bool Wether the cube and related files match the expeced hashes Source code in entities/cube.py 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 def is_valid ( self ) -> bool : \"\"\"Checks the validity of the cube and related files through hash checking. Returns: bool: Wether the cube and related files match the expeced hashes \"\"\" try : local_hashes = self . get_local_hashes () except FileNotFoundError : logging . warning ( \"Local MLCube files not found. Defaulting to invalid\" ) return False valid_cube = self . is_cube_valid valid_hashes = True server_hashes = self . todict () for key in local_hashes : if local_hashes [ key ]: if local_hashes [ key ] != server_hashes [ key ]: valid_hashes = False msg = f \" { key . replace ( '_' , ' ' ) } doesn't match\" config . ui . print_error ( msg ) return valid_cube and valid_hashes","title":"is_valid()"},{"location":"reference/entities/cube/#entities.cube.Cube.run","text":"Executes a given task on the cube instance Parameters: Name Type Description Default task str task to run required string_params Dict [ str ] Extra parameters that can't be passed as normal function args. Defaults to {}. {} timeout int timeout for the task in seconds. Defaults to None. None kwargs dict additional arguments that are passed directly to the mlcube command {} Source code in entities/cube.py 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 def run ( self , task : str , string_params : Dict [ str , str ] = {}, timeout : int = None , ** kwargs , ): \"\"\"Executes a given task on the cube instance Args: task (str): task to run string_params (Dict[str], optional): Extra parameters that can't be passed as normal function args. Defaults to {}. timeout (int, optional): timeout for the task in seconds. Defaults to None. kwargs (dict): additional arguments that are passed directly to the mlcube command \"\"\" kwargs . update ( string_params ) cmd = f \"mlcube run --mlcube= { self . cube_path } --task= { task } --platform= { config . platform } \" for k , v in kwargs . items (): cmd_arg = f ' { k } =\" { v } \"' cmd = \" \" . join ([ cmd , cmd_arg ]) logging . info ( f \"Running MLCube command: { cmd } \" ) proc = pexpect . spawn ( cmd , timeout = timeout ) proc_out = combine_proc_sp_text ( proc ) proc . close () logging . debug ( proc_out ) if proc . exitstatus != 0 : raise RuntimeError ( \"There was an error while executing the cube\" ) logging . debug ( list_files ( config . storage )) return proc","title":"run()"},{"location":"reference/entities/dataset/","text":"Dataset Bases: Entity Class representing a Dataset Datasets are stored locally in the Data Owner's machine. They contain information regarding the prepared dataset, such as name and description, general statistics and an UID generated by hashing the contents of the data preparation output. Source code in entities/dataset.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 class Dataset ( Entity ): \"\"\" Class representing a Dataset Datasets are stored locally in the Data Owner's machine. They contain information regarding the prepared dataset, such as name and description, general statistics and an UID generated by hashing the contents of the data preparation output. \"\"\" def __init__ ( self , dataset_dict : dict ): \"\"\"Creates a new dataset instance Args: data_uid (int): The dataset UID as found inside ~/medperf/data/ Raises: NameError: If the dataset with the given UID can't be found, this is thrown. \"\"\" self . generated_uid = dataset_dict [ \"generated_uid\" ] self . name = dataset_dict [ \"name\" ] self . description = dataset_dict [ \"description\" ] self . location = dataset_dict [ \"location\" ] self . preparation_cube_uid = dataset_dict [ \"data_preparation_mlcube\" ] self . input_data_hash = dataset_dict [ \"input_data_hash\" ] self . separate_labels = dataset_dict . get ( \"separate_labels\" , None ) # not in the server self . split_seed = dataset_dict [ \"split_seed\" ] if \"metadata\" in dataset_dict : # Make sure it is backwards-compatible self . generated_metadata = dataset_dict [ \"metadata\" ] else : self . generated_metadata = dataset_dict [ \"generated_metadata\" ] if \"status\" in dataset_dict : self . status = Status ( dataset_dict [ \"status\" ]) # not in the server else : self . status = ( Status . PENDING if dataset_dict [ \"id\" ] is None else Status . APPROVED ) self . state = dataset_dict [ \"state\" ] self . is_valid = dataset_dict [ \"is_valid\" ] self . user_metadata = dataset_dict [ \"user_metadata\" ] self . uid = dataset_dict [ \"id\" ] self . created_at = dataset_dict [ \"created_at\" ] self . modified_at = dataset_dict [ \"modified_at\" ] self . owner = dataset_dict [ \"owner\" ] self . dataset_path = os . path . join ( storage_path ( config . data_storage ), str ( self . generated_uid ) ) self . data_path = os . path . join ( self . dataset_path , \"data\" ) self . labels_path = self . data_path if self . separate_labels : self . labels_path = os . path . join ( self . dataset_path , \"labels\" ) def todict ( self ): return { \"id\" : self . uid , \"name\" : self . name , \"description\" : self . description , \"location\" : self . location , \"data_preparation_mlcube\" : self . preparation_cube_uid , \"input_data_hash\" : self . input_data_hash , \"generated_uid\" : self . generated_uid , \"split_seed\" : self . split_seed , \"generated_metadata\" : self . generated_metadata , \"status\" : self . status . value , # not in the server \"state\" : self . state , \"separate_labels\" : self . separate_labels , # not in the server \"is_valid\" : self . is_valid , \"user_metadata\" : self . user_metadata , \"created_at\" : self . created_at , \"modified_at\" : self . modified_at , \"owner\" : self . owner , } @classmethod def from_generated_uid ( cls , generated_uid : str ) -> \"Dataset\" : generated_uid = cls . __full_uid ( generated_uid ) reg = cls . __get_local_dict ( generated_uid ) return cls ( reg ) @classmethod def all ( cls ) -> List [ \"Dataset\" ]: \"\"\"Gets and creates instances of all the locally prepared datasets Returns: List[Dataset]: a list of Dataset instances. \"\"\" logging . info ( \"Retrieving all datasets\" ) data_storage = storage_path ( config . data_storage ) try : generated_uids = next ( os . walk ( data_storage ))[ 1 ] except StopIteration : logging . warning ( \"Couldn't iterate over the dataset directory\" ) raise RuntimeError ( \"Couldn't iterate over the dataset directory\" ) dsets = [] for generated_uid in generated_uids : dsets . append ( cls . from_generated_uid ( generated_uid )) return dsets @classmethod def get ( cls , dset_uid : str ) -> \"Dataset\" : \"\"\"Retrieves and creates a Dataset instance from the comms instance. If the dataset is present in the user's machine then it retrieves it from there. Args: dset_uid (str): server UID of the dataset Returns: Dataset: Specified Dataset Instance \"\"\" logging . debug ( f \"Retrieving dataset { dset_uid } \" ) comms = config . comms # Try first downloading the data try : meta = comms . get_dataset ( dset_uid ) dataset = cls ( meta ) except CommunicationRetrievalError : # Get from local cache logging . warning ( f \"Getting Dataset { dset_uid } from comms failed\" ) logging . info ( f \"Looking for dataset { dset_uid } locally\" ) local_dset = list ( filter ( lambda dset : str ( dset . uid ) == str ( dset_uid ), cls . all ()) ) if len ( local_dset ) == 1 : logging . debug ( \"Found dataset locally\" ) dataset = local_dset [ 0 ] else : raise InvalidArgumentError ( f \"Requested dataset { dset_uid } could not be retrieved\" ) dataset . write () return dataset @staticmethod def __full_uid ( uid_hint : str ) -> str : \"\"\"Returns the found UID that starts with the provided UID hint Args: uid_hint (int): a small initial portion of an existing local dataset UID Raises: NameError: If no dataset is found starting with the given hint, this is thrown. NameError: If multiple datasets are found starting with the given hint, this is thrown. Returns: str: the complete UID \"\"\" data_storage = storage_path ( config . data_storage ) dsets = get_uids ( data_storage ) match = [ uid for uid in dsets if uid . startswith ( str ( uid_hint ))] if len ( match ) == 0 : msg = f \"No dataset was found with uid hint { uid_hint } .\" elif len ( match ) > 1 : msg = f \"Multiple datasets were found with uid hint { uid_hint } .\" else : return match [ 0 ] raise InvalidArgumentError ( msg ) def write ( self ): logging . info ( f \"Updating registration information for dataset: { self . uid } \" ) logging . debug ( f \"registration information: { self . todict () } \" ) regfile = os . path . join ( self . dataset_path , config . reg_file ) os . makedirs ( self . dataset_path , exist_ok = True ) with open ( regfile , \"w\" ) as f : yaml . dump ( self . todict (), f ) def upload ( self ): \"\"\"Uploads the registration information to the comms. Args: comms (Comms): Instance of the comms interface. \"\"\" dataset_dict = self . todict () updated_dataset_dict = config . comms . upload_dataset ( dataset_dict ) updated_dataset_dict [ \"status\" ] = dataset_dict [ \"status\" ] updated_dataset_dict [ \"separate_labels\" ] = dataset_dict [ \"separate_labels\" ] return updated_dataset_dict @classmethod def __get_local_dict ( cls , generated_uid ): dataset_path = os . path . join ( storage_path ( config . data_storage ), str ( generated_uid ) ) regfile = os . path . join ( dataset_path , config . reg_file ) with open ( regfile , \"r\" ) as f : reg = yaml . safe_load ( f ) return reg __full_uid ( uid_hint ) staticmethod Returns the found UID that starts with the provided UID hint Parameters: Name Type Description Default uid_hint int a small initial portion of an existing local dataset UID required Raises: Type Description NameError If no dataset is found starting with the given hint, this is thrown. NameError If multiple datasets are found starting with the given hint, this is thrown. Returns: Name Type Description str str the complete UID Source code in entities/dataset.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 @staticmethod def __full_uid ( uid_hint : str ) -> str : \"\"\"Returns the found UID that starts with the provided UID hint Args: uid_hint (int): a small initial portion of an existing local dataset UID Raises: NameError: If no dataset is found starting with the given hint, this is thrown. NameError: If multiple datasets are found starting with the given hint, this is thrown. Returns: str: the complete UID \"\"\" data_storage = storage_path ( config . data_storage ) dsets = get_uids ( data_storage ) match = [ uid for uid in dsets if uid . startswith ( str ( uid_hint ))] if len ( match ) == 0 : msg = f \"No dataset was found with uid hint { uid_hint } .\" elif len ( match ) > 1 : msg = f \"Multiple datasets were found with uid hint { uid_hint } .\" else : return match [ 0 ] raise InvalidArgumentError ( msg ) __init__ ( dataset_dict ) Creates a new dataset instance Parameters: Name Type Description Default data_uid int The dataset UID as found inside ~/medperf/data/ required Raises: Type Description NameError If the dataset with the given UID can't be found, this is thrown. Source code in entities/dataset.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def __init__ ( self , dataset_dict : dict ): \"\"\"Creates a new dataset instance Args: data_uid (int): The dataset UID as found inside ~/medperf/data/ Raises: NameError: If the dataset with the given UID can't be found, this is thrown. \"\"\" self . generated_uid = dataset_dict [ \"generated_uid\" ] self . name = dataset_dict [ \"name\" ] self . description = dataset_dict [ \"description\" ] self . location = dataset_dict [ \"location\" ] self . preparation_cube_uid = dataset_dict [ \"data_preparation_mlcube\" ] self . input_data_hash = dataset_dict [ \"input_data_hash\" ] self . separate_labels = dataset_dict . get ( \"separate_labels\" , None ) # not in the server self . split_seed = dataset_dict [ \"split_seed\" ] if \"metadata\" in dataset_dict : # Make sure it is backwards-compatible self . generated_metadata = dataset_dict [ \"metadata\" ] else : self . generated_metadata = dataset_dict [ \"generated_metadata\" ] if \"status\" in dataset_dict : self . status = Status ( dataset_dict [ \"status\" ]) # not in the server else : self . status = ( Status . PENDING if dataset_dict [ \"id\" ] is None else Status . APPROVED ) self . state = dataset_dict [ \"state\" ] self . is_valid = dataset_dict [ \"is_valid\" ] self . user_metadata = dataset_dict [ \"user_metadata\" ] self . uid = dataset_dict [ \"id\" ] self . created_at = dataset_dict [ \"created_at\" ] self . modified_at = dataset_dict [ \"modified_at\" ] self . owner = dataset_dict [ \"owner\" ] self . dataset_path = os . path . join ( storage_path ( config . data_storage ), str ( self . generated_uid ) ) self . data_path = os . path . join ( self . dataset_path , \"data\" ) self . labels_path = self . data_path if self . separate_labels : self . labels_path = os . path . join ( self . dataset_path , \"labels\" ) all () classmethod Gets and creates instances of all the locally prepared datasets Returns: Type Description List [ Dataset ] List[Dataset]: a list of Dataset instances. Source code in entities/dataset.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 @classmethod def all ( cls ) -> List [ \"Dataset\" ]: \"\"\"Gets and creates instances of all the locally prepared datasets Returns: List[Dataset]: a list of Dataset instances. \"\"\" logging . info ( \"Retrieving all datasets\" ) data_storage = storage_path ( config . data_storage ) try : generated_uids = next ( os . walk ( data_storage ))[ 1 ] except StopIteration : logging . warning ( \"Couldn't iterate over the dataset directory\" ) raise RuntimeError ( \"Couldn't iterate over the dataset directory\" ) dsets = [] for generated_uid in generated_uids : dsets . append ( cls . from_generated_uid ( generated_uid )) return dsets get ( dset_uid ) classmethod Retrieves and creates a Dataset instance from the comms instance. If the dataset is present in the user's machine then it retrieves it from there. Parameters: Name Type Description Default dset_uid str server UID of the dataset required Returns: Name Type Description Dataset Dataset Specified Dataset Instance Source code in entities/dataset.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 @classmethod def get ( cls , dset_uid : str ) -> \"Dataset\" : \"\"\"Retrieves and creates a Dataset instance from the comms instance. If the dataset is present in the user's machine then it retrieves it from there. Args: dset_uid (str): server UID of the dataset Returns: Dataset: Specified Dataset Instance \"\"\" logging . debug ( f \"Retrieving dataset { dset_uid } \" ) comms = config . comms # Try first downloading the data try : meta = comms . get_dataset ( dset_uid ) dataset = cls ( meta ) except CommunicationRetrievalError : # Get from local cache logging . warning ( f \"Getting Dataset { dset_uid } from comms failed\" ) logging . info ( f \"Looking for dataset { dset_uid } locally\" ) local_dset = list ( filter ( lambda dset : str ( dset . uid ) == str ( dset_uid ), cls . all ()) ) if len ( local_dset ) == 1 : logging . debug ( \"Found dataset locally\" ) dataset = local_dset [ 0 ] else : raise InvalidArgumentError ( f \"Requested dataset { dset_uid } could not be retrieved\" ) dataset . write () return dataset upload () Uploads the registration information to the comms. Parameters: Name Type Description Default comms Comms Instance of the comms interface. required Source code in entities/dataset.py 189 190 191 192 193 194 195 196 197 198 199 def upload ( self ): \"\"\"Uploads the registration information to the comms. Args: comms (Comms): Instance of the comms interface. \"\"\" dataset_dict = self . todict () updated_dataset_dict = config . comms . upload_dataset ( dataset_dict ) updated_dataset_dict [ \"status\" ] = dataset_dict [ \"status\" ] updated_dataset_dict [ \"separate_labels\" ] = dataset_dict [ \"separate_labels\" ] return updated_dataset_dict","title":"dataset"},{"location":"reference/entities/dataset/#entities.dataset.Dataset","text":"Bases: Entity Class representing a Dataset Datasets are stored locally in the Data Owner's machine. They contain information regarding the prepared dataset, such as name and description, general statistics and an UID generated by hashing the contents of the data preparation output. Source code in entities/dataset.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 class Dataset ( Entity ): \"\"\" Class representing a Dataset Datasets are stored locally in the Data Owner's machine. They contain information regarding the prepared dataset, such as name and description, general statistics and an UID generated by hashing the contents of the data preparation output. \"\"\" def __init__ ( self , dataset_dict : dict ): \"\"\"Creates a new dataset instance Args: data_uid (int): The dataset UID as found inside ~/medperf/data/ Raises: NameError: If the dataset with the given UID can't be found, this is thrown. \"\"\" self . generated_uid = dataset_dict [ \"generated_uid\" ] self . name = dataset_dict [ \"name\" ] self . description = dataset_dict [ \"description\" ] self . location = dataset_dict [ \"location\" ] self . preparation_cube_uid = dataset_dict [ \"data_preparation_mlcube\" ] self . input_data_hash = dataset_dict [ \"input_data_hash\" ] self . separate_labels = dataset_dict . get ( \"separate_labels\" , None ) # not in the server self . split_seed = dataset_dict [ \"split_seed\" ] if \"metadata\" in dataset_dict : # Make sure it is backwards-compatible self . generated_metadata = dataset_dict [ \"metadata\" ] else : self . generated_metadata = dataset_dict [ \"generated_metadata\" ] if \"status\" in dataset_dict : self . status = Status ( dataset_dict [ \"status\" ]) # not in the server else : self . status = ( Status . PENDING if dataset_dict [ \"id\" ] is None else Status . APPROVED ) self . state = dataset_dict [ \"state\" ] self . is_valid = dataset_dict [ \"is_valid\" ] self . user_metadata = dataset_dict [ \"user_metadata\" ] self . uid = dataset_dict [ \"id\" ] self . created_at = dataset_dict [ \"created_at\" ] self . modified_at = dataset_dict [ \"modified_at\" ] self . owner = dataset_dict [ \"owner\" ] self . dataset_path = os . path . join ( storage_path ( config . data_storage ), str ( self . generated_uid ) ) self . data_path = os . path . join ( self . dataset_path , \"data\" ) self . labels_path = self . data_path if self . separate_labels : self . labels_path = os . path . join ( self . dataset_path , \"labels\" ) def todict ( self ): return { \"id\" : self . uid , \"name\" : self . name , \"description\" : self . description , \"location\" : self . location , \"data_preparation_mlcube\" : self . preparation_cube_uid , \"input_data_hash\" : self . input_data_hash , \"generated_uid\" : self . generated_uid , \"split_seed\" : self . split_seed , \"generated_metadata\" : self . generated_metadata , \"status\" : self . status . value , # not in the server \"state\" : self . state , \"separate_labels\" : self . separate_labels , # not in the server \"is_valid\" : self . is_valid , \"user_metadata\" : self . user_metadata , \"created_at\" : self . created_at , \"modified_at\" : self . modified_at , \"owner\" : self . owner , } @classmethod def from_generated_uid ( cls , generated_uid : str ) -> \"Dataset\" : generated_uid = cls . __full_uid ( generated_uid ) reg = cls . __get_local_dict ( generated_uid ) return cls ( reg ) @classmethod def all ( cls ) -> List [ \"Dataset\" ]: \"\"\"Gets and creates instances of all the locally prepared datasets Returns: List[Dataset]: a list of Dataset instances. \"\"\" logging . info ( \"Retrieving all datasets\" ) data_storage = storage_path ( config . data_storage ) try : generated_uids = next ( os . walk ( data_storage ))[ 1 ] except StopIteration : logging . warning ( \"Couldn't iterate over the dataset directory\" ) raise RuntimeError ( \"Couldn't iterate over the dataset directory\" ) dsets = [] for generated_uid in generated_uids : dsets . append ( cls . from_generated_uid ( generated_uid )) return dsets @classmethod def get ( cls , dset_uid : str ) -> \"Dataset\" : \"\"\"Retrieves and creates a Dataset instance from the comms instance. If the dataset is present in the user's machine then it retrieves it from there. Args: dset_uid (str): server UID of the dataset Returns: Dataset: Specified Dataset Instance \"\"\" logging . debug ( f \"Retrieving dataset { dset_uid } \" ) comms = config . comms # Try first downloading the data try : meta = comms . get_dataset ( dset_uid ) dataset = cls ( meta ) except CommunicationRetrievalError : # Get from local cache logging . warning ( f \"Getting Dataset { dset_uid } from comms failed\" ) logging . info ( f \"Looking for dataset { dset_uid } locally\" ) local_dset = list ( filter ( lambda dset : str ( dset . uid ) == str ( dset_uid ), cls . all ()) ) if len ( local_dset ) == 1 : logging . debug ( \"Found dataset locally\" ) dataset = local_dset [ 0 ] else : raise InvalidArgumentError ( f \"Requested dataset { dset_uid } could not be retrieved\" ) dataset . write () return dataset @staticmethod def __full_uid ( uid_hint : str ) -> str : \"\"\"Returns the found UID that starts with the provided UID hint Args: uid_hint (int): a small initial portion of an existing local dataset UID Raises: NameError: If no dataset is found starting with the given hint, this is thrown. NameError: If multiple datasets are found starting with the given hint, this is thrown. Returns: str: the complete UID \"\"\" data_storage = storage_path ( config . data_storage ) dsets = get_uids ( data_storage ) match = [ uid for uid in dsets if uid . startswith ( str ( uid_hint ))] if len ( match ) == 0 : msg = f \"No dataset was found with uid hint { uid_hint } .\" elif len ( match ) > 1 : msg = f \"Multiple datasets were found with uid hint { uid_hint } .\" else : return match [ 0 ] raise InvalidArgumentError ( msg ) def write ( self ): logging . info ( f \"Updating registration information for dataset: { self . uid } \" ) logging . debug ( f \"registration information: { self . todict () } \" ) regfile = os . path . join ( self . dataset_path , config . reg_file ) os . makedirs ( self . dataset_path , exist_ok = True ) with open ( regfile , \"w\" ) as f : yaml . dump ( self . todict (), f ) def upload ( self ): \"\"\"Uploads the registration information to the comms. Args: comms (Comms): Instance of the comms interface. \"\"\" dataset_dict = self . todict () updated_dataset_dict = config . comms . upload_dataset ( dataset_dict ) updated_dataset_dict [ \"status\" ] = dataset_dict [ \"status\" ] updated_dataset_dict [ \"separate_labels\" ] = dataset_dict [ \"separate_labels\" ] return updated_dataset_dict @classmethod def __get_local_dict ( cls , generated_uid ): dataset_path = os . path . join ( storage_path ( config . data_storage ), str ( generated_uid ) ) regfile = os . path . join ( dataset_path , config . reg_file ) with open ( regfile , \"r\" ) as f : reg = yaml . safe_load ( f ) return reg","title":"Dataset"},{"location":"reference/entities/dataset/#entities.dataset.Dataset.__full_uid","text":"Returns the found UID that starts with the provided UID hint Parameters: Name Type Description Default uid_hint int a small initial portion of an existing local dataset UID required Raises: Type Description NameError If no dataset is found starting with the given hint, this is thrown. NameError If multiple datasets are found starting with the given hint, this is thrown. Returns: Name Type Description str str the complete UID Source code in entities/dataset.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 @staticmethod def __full_uid ( uid_hint : str ) -> str : \"\"\"Returns the found UID that starts with the provided UID hint Args: uid_hint (int): a small initial portion of an existing local dataset UID Raises: NameError: If no dataset is found starting with the given hint, this is thrown. NameError: If multiple datasets are found starting with the given hint, this is thrown. Returns: str: the complete UID \"\"\" data_storage = storage_path ( config . data_storage ) dsets = get_uids ( data_storage ) match = [ uid for uid in dsets if uid . startswith ( str ( uid_hint ))] if len ( match ) == 0 : msg = f \"No dataset was found with uid hint { uid_hint } .\" elif len ( match ) > 1 : msg = f \"Multiple datasets were found with uid hint { uid_hint } .\" else : return match [ 0 ] raise InvalidArgumentError ( msg )","title":"__full_uid()"},{"location":"reference/entities/dataset/#entities.dataset.Dataset.__init__","text":"Creates a new dataset instance Parameters: Name Type Description Default data_uid int The dataset UID as found inside ~/medperf/data/ required Raises: Type Description NameError If the dataset with the given UID can't be found, this is thrown. Source code in entities/dataset.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def __init__ ( self , dataset_dict : dict ): \"\"\"Creates a new dataset instance Args: data_uid (int): The dataset UID as found inside ~/medperf/data/ Raises: NameError: If the dataset with the given UID can't be found, this is thrown. \"\"\" self . generated_uid = dataset_dict [ \"generated_uid\" ] self . name = dataset_dict [ \"name\" ] self . description = dataset_dict [ \"description\" ] self . location = dataset_dict [ \"location\" ] self . preparation_cube_uid = dataset_dict [ \"data_preparation_mlcube\" ] self . input_data_hash = dataset_dict [ \"input_data_hash\" ] self . separate_labels = dataset_dict . get ( \"separate_labels\" , None ) # not in the server self . split_seed = dataset_dict [ \"split_seed\" ] if \"metadata\" in dataset_dict : # Make sure it is backwards-compatible self . generated_metadata = dataset_dict [ \"metadata\" ] else : self . generated_metadata = dataset_dict [ \"generated_metadata\" ] if \"status\" in dataset_dict : self . status = Status ( dataset_dict [ \"status\" ]) # not in the server else : self . status = ( Status . PENDING if dataset_dict [ \"id\" ] is None else Status . APPROVED ) self . state = dataset_dict [ \"state\" ] self . is_valid = dataset_dict [ \"is_valid\" ] self . user_metadata = dataset_dict [ \"user_metadata\" ] self . uid = dataset_dict [ \"id\" ] self . created_at = dataset_dict [ \"created_at\" ] self . modified_at = dataset_dict [ \"modified_at\" ] self . owner = dataset_dict [ \"owner\" ] self . dataset_path = os . path . join ( storage_path ( config . data_storage ), str ( self . generated_uid ) ) self . data_path = os . path . join ( self . dataset_path , \"data\" ) self . labels_path = self . data_path if self . separate_labels : self . labels_path = os . path . join ( self . dataset_path , \"labels\" )","title":"__init__()"},{"location":"reference/entities/dataset/#entities.dataset.Dataset.all","text":"Gets and creates instances of all the locally prepared datasets Returns: Type Description List [ Dataset ] List[Dataset]: a list of Dataset instances. Source code in entities/dataset.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 @classmethod def all ( cls ) -> List [ \"Dataset\" ]: \"\"\"Gets and creates instances of all the locally prepared datasets Returns: List[Dataset]: a list of Dataset instances. \"\"\" logging . info ( \"Retrieving all datasets\" ) data_storage = storage_path ( config . data_storage ) try : generated_uids = next ( os . walk ( data_storage ))[ 1 ] except StopIteration : logging . warning ( \"Couldn't iterate over the dataset directory\" ) raise RuntimeError ( \"Couldn't iterate over the dataset directory\" ) dsets = [] for generated_uid in generated_uids : dsets . append ( cls . from_generated_uid ( generated_uid )) return dsets","title":"all()"},{"location":"reference/entities/dataset/#entities.dataset.Dataset.get","text":"Retrieves and creates a Dataset instance from the comms instance. If the dataset is present in the user's machine then it retrieves it from there. Parameters: Name Type Description Default dset_uid str server UID of the dataset required Returns: Name Type Description Dataset Dataset Specified Dataset Instance Source code in entities/dataset.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 @classmethod def get ( cls , dset_uid : str ) -> \"Dataset\" : \"\"\"Retrieves and creates a Dataset instance from the comms instance. If the dataset is present in the user's machine then it retrieves it from there. Args: dset_uid (str): server UID of the dataset Returns: Dataset: Specified Dataset Instance \"\"\" logging . debug ( f \"Retrieving dataset { dset_uid } \" ) comms = config . comms # Try first downloading the data try : meta = comms . get_dataset ( dset_uid ) dataset = cls ( meta ) except CommunicationRetrievalError : # Get from local cache logging . warning ( f \"Getting Dataset { dset_uid } from comms failed\" ) logging . info ( f \"Looking for dataset { dset_uid } locally\" ) local_dset = list ( filter ( lambda dset : str ( dset . uid ) == str ( dset_uid ), cls . all ()) ) if len ( local_dset ) == 1 : logging . debug ( \"Found dataset locally\" ) dataset = local_dset [ 0 ] else : raise InvalidArgumentError ( f \"Requested dataset { dset_uid } could not be retrieved\" ) dataset . write () return dataset","title":"get()"},{"location":"reference/entities/dataset/#entities.dataset.Dataset.upload","text":"Uploads the registration information to the comms. Parameters: Name Type Description Default comms Comms Instance of the comms interface. required Source code in entities/dataset.py 189 190 191 192 193 194 195 196 197 198 199 def upload ( self ): \"\"\"Uploads the registration information to the comms. Args: comms (Comms): Instance of the comms interface. \"\"\" dataset_dict = self . todict () updated_dataset_dict = config . comms . upload_dataset ( dataset_dict ) updated_dataset_dict [ \"status\" ] = dataset_dict [ \"status\" ] updated_dataset_dict [ \"separate_labels\" ] = dataset_dict [ \"separate_labels\" ] return updated_dataset_dict","title":"upload()"},{"location":"reference/entities/interface/","text":"Entity Bases: ABC Source code in entities/interface.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class Entity ( ABC ): @abstractmethod def all ( cls ) -> List [ \"Entity\" ]: \"\"\"Gets a list of all instances of the respective entity. Wether the list is local or remote depends on the implementation. Returns: List[Entity]: a list of entities. \"\"\" @abstractmethod def get ( cls , uid : str ) -> \"Entity\" : \"\"\"Gets an instance of the respective entity. Wether this requires only local read or remote calls depends on the implementation. Args: uid (str): Unique Identifier to retrieve the entity Returns: Entity: Entity Instance associated to the UID \"\"\" @abstractmethod def todict ( self ) -> Dict : \"\"\"Dictionary representation of the entity Returns: Dict: Dictionary containing information about the entity \"\"\" @abstractmethod def upload ( self ): \"\"\"Upload the entity-related information to the communication's interface \"\"\" all () abstractmethod Gets a list of all instances of the respective entity. Wether the list is local or remote depends on the implementation. Returns: Type Description List [ Entity ] List[Entity]: a list of entities. Source code in entities/interface.py 6 7 8 9 10 11 12 13 @abstractmethod def all ( cls ) -> List [ \"Entity\" ]: \"\"\"Gets a list of all instances of the respective entity. Wether the list is local or remote depends on the implementation. Returns: List[Entity]: a list of entities. \"\"\" get ( uid ) abstractmethod Gets an instance of the respective entity. Wether this requires only local read or remote calls depends on the implementation. Parameters: Name Type Description Default uid str Unique Identifier to retrieve the entity required Returns: Name Type Description Entity Entity Entity Instance associated to the UID Source code in entities/interface.py 15 16 17 18 19 20 21 22 23 24 25 26 @abstractmethod def get ( cls , uid : str ) -> \"Entity\" : \"\"\"Gets an instance of the respective entity. Wether this requires only local read or remote calls depends on the implementation. Args: uid (str): Unique Identifier to retrieve the entity Returns: Entity: Entity Instance associated to the UID \"\"\" todict () abstractmethod Dictionary representation of the entity Returns: Name Type Description Dict Dict Dictionary containing information about the entity Source code in entities/interface.py 28 29 30 31 32 33 34 @abstractmethod def todict ( self ) -> Dict : \"\"\"Dictionary representation of the entity Returns: Dict: Dictionary containing information about the entity \"\"\" upload () abstractmethod Upload the entity-related information to the communication's interface Source code in entities/interface.py 36 37 38 39 @abstractmethod def upload ( self ): \"\"\"Upload the entity-related information to the communication's interface \"\"\"","title":"interface"},{"location":"reference/entities/interface/#entities.interface.Entity","text":"Bases: ABC Source code in entities/interface.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class Entity ( ABC ): @abstractmethod def all ( cls ) -> List [ \"Entity\" ]: \"\"\"Gets a list of all instances of the respective entity. Wether the list is local or remote depends on the implementation. Returns: List[Entity]: a list of entities. \"\"\" @abstractmethod def get ( cls , uid : str ) -> \"Entity\" : \"\"\"Gets an instance of the respective entity. Wether this requires only local read or remote calls depends on the implementation. Args: uid (str): Unique Identifier to retrieve the entity Returns: Entity: Entity Instance associated to the UID \"\"\" @abstractmethod def todict ( self ) -> Dict : \"\"\"Dictionary representation of the entity Returns: Dict: Dictionary containing information about the entity \"\"\" @abstractmethod def upload ( self ): \"\"\"Upload the entity-related information to the communication's interface \"\"\"","title":"Entity"},{"location":"reference/entities/interface/#entities.interface.Entity.all","text":"Gets a list of all instances of the respective entity. Wether the list is local or remote depends on the implementation. Returns: Type Description List [ Entity ] List[Entity]: a list of entities. Source code in entities/interface.py 6 7 8 9 10 11 12 13 @abstractmethod def all ( cls ) -> List [ \"Entity\" ]: \"\"\"Gets a list of all instances of the respective entity. Wether the list is local or remote depends on the implementation. Returns: List[Entity]: a list of entities. \"\"\"","title":"all()"},{"location":"reference/entities/interface/#entities.interface.Entity.get","text":"Gets an instance of the respective entity. Wether this requires only local read or remote calls depends on the implementation. Parameters: Name Type Description Default uid str Unique Identifier to retrieve the entity required Returns: Name Type Description Entity Entity Entity Instance associated to the UID Source code in entities/interface.py 15 16 17 18 19 20 21 22 23 24 25 26 @abstractmethod def get ( cls , uid : str ) -> \"Entity\" : \"\"\"Gets an instance of the respective entity. Wether this requires only local read or remote calls depends on the implementation. Args: uid (str): Unique Identifier to retrieve the entity Returns: Entity: Entity Instance associated to the UID \"\"\"","title":"get()"},{"location":"reference/entities/interface/#entities.interface.Entity.todict","text":"Dictionary representation of the entity Returns: Name Type Description Dict Dict Dictionary containing information about the entity Source code in entities/interface.py 28 29 30 31 32 33 34 @abstractmethod def todict ( self ) -> Dict : \"\"\"Dictionary representation of the entity Returns: Dict: Dictionary containing information about the entity \"\"\"","title":"todict()"},{"location":"reference/entities/interface/#entities.interface.Entity.upload","text":"Upload the entity-related information to the communication's interface Source code in entities/interface.py 36 37 38 39 @abstractmethod def upload ( self ): \"\"\"Upload the entity-related information to the communication's interface \"\"\"","title":"upload()"},{"location":"reference/entities/result/","text":"Result Bases: Entity Class representing a Result entry Results are obtained after successfully running a benchmark execution flow. They contain information regarding the components involved in obtaining metrics results, as well as the results themselves. This class provides methods for working with benchmark results and how to upload them to the backend. Source code in entities/result.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 class Result ( Entity ): \"\"\" Class representing a Result entry Results are obtained after successfully running a benchmark execution flow. They contain information regarding the components involved in obtaining metrics results, as well as the results themselves. This class provides methods for working with benchmark results and how to upload them to the backend. \"\"\" def __init__ ( self , results_info : dict ): \"\"\"Creates a new result instance Args: benchmark_uid (str): UID of the executed benchmark. dataset_uid (str): UID of the dataset used. model_uid (str): UID of the model used. results() \"\"\" self . uid = results_info [ \"id\" ] self . name = results_info [ \"name\" ] self . owner = results_info [ \"owner\" ] self . benchmark_uid = results_info [ \"benchmark\" ] self . model_uid = results_info [ \"model\" ] self . dataset_uid = results_info [ \"dataset\" ] self . results = results_info [ \"results\" ] self . status = Status ( results_info [ \"approval_status\" ]) self . metadata = results_info [ \"metadata\" ] self . approved_at = results_info [ \"approved_at\" ] self . created_at = results_info [ \"created_at\" ] self . modified_at = results_info [ \"modified_at\" ] self . path = results_path ( self . benchmark_uid , self . model_uid , self . dataset_uid ) self . path = os . path . join ( self . path , config . results_info_file ) @classmethod def from_entities_uids ( cls , benchmark_uid : str , model_uid : str , dataset_uid : str ) -> \"Result\" : results_info = cls . __get_local_dict ( benchmark_uid , model_uid , dataset_uid ) return cls ( results_info ) @classmethod def all ( cls ) -> List [ \"Result\" ]: \"\"\"Gets and creates instances of all the user's results\"\"\" logging . info ( \"Retrieving all results\" ) results_ids_tuple = results_ids () results = [] for result_ids in results_ids_tuple : b_id , m_id , d_id = result_ids results . append ( cls . from_entities_uids ( b_id , m_id , d_id )) return results @classmethod def get ( cls , result_uid : str ) -> \"Result\" : \"\"\"Retrieves and creates a Result instance obtained from the platform. If the result instance already exists in the user's machine, it loads the local instance Args: result_uid (str): UID of the Result instance Returns: Result: Specified Result instance \"\"\" logging . debug ( f \"Retrieving result { result_uid } \" ) comms = config . comms # Try to download first try : meta = comms . get_result ( result_uid ) result = cls ( meta ) except CommunicationRetrievalError : # Get local results logging . warning ( f \"Getting result { result_uid } from comms failed\" ) logging . info ( f \"Looking for result { result_uid } locally\" ) local_result = list ( filter ( lambda res : str ( res . uid ) == str ( result_uid ), cls . all ()) ) if len ( local_result ) == 1 : logging . debug ( \"Found result locally\" ) result = local_result [ 0 ] else : raise InvalidArgumentError ( f \"The requested result { result_uid } could not be retrieved\" ) result . write () return result def todict ( self ): return { \"id\" : self . uid , \"name\" : self . name , \"owner\" : self . owner , \"benchmark\" : self . benchmark_uid , \"model\" : self . model_uid , \"dataset\" : self . dataset_uid , \"results\" : self . results , \"metadata\" : self . metadata , \"approval_status\" : self . status . value , \"approved_at\" : self . approved_at , \"created_at\" : self . created_at , \"modified_at\" : self . modified_at , } def upload ( self ): \"\"\"Uploads the results to the comms Args: comms (Comms): Instance of the communications interface. \"\"\" results_info = self . todict () updated_results_info = config . comms . upload_results ( results_info ) return updated_results_info def write ( self ): if os . path . exists ( self . path ): write_access = os . access ( self . path , os . W_OK ) logging . debug ( f \"file has write access? { write_access } \" ) if not write_access : logging . debug ( \"removing outdated and inaccessible results\" ) os . remove ( self . path ) os . makedirs ( Path ( self . path ) . parent , exist_ok = True ) with open ( self . path , \"w\" ) as f : yaml . dump ( self . todict (), f ) @classmethod def __get_local_dict ( cls , benchmark_uid , model_uid , dataset_uid ): path = results_path ( benchmark_uid , model_uid , dataset_uid ) path = os . path . join ( path , config . results_info_file ) with open ( path , \"r\" ) as f : results_info = yaml . safe_load ( f ) return results_info __init__ ( results_info ) Creates a new result instance Parameters: Name Type Description Default benchmark_uid str UID of the executed benchmark. required dataset_uid str UID of the dataset used. required model_uid str UID of the model used. required Source code in entities/result.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def __init__ ( self , results_info : dict ): \"\"\"Creates a new result instance Args: benchmark_uid (str): UID of the executed benchmark. dataset_uid (str): UID of the dataset used. model_uid (str): UID of the model used. results() \"\"\" self . uid = results_info [ \"id\" ] self . name = results_info [ \"name\" ] self . owner = results_info [ \"owner\" ] self . benchmark_uid = results_info [ \"benchmark\" ] self . model_uid = results_info [ \"model\" ] self . dataset_uid = results_info [ \"dataset\" ] self . results = results_info [ \"results\" ] self . status = Status ( results_info [ \"approval_status\" ]) self . metadata = results_info [ \"metadata\" ] self . approved_at = results_info [ \"approved_at\" ] self . created_at = results_info [ \"created_at\" ] self . modified_at = results_info [ \"modified_at\" ] self . path = results_path ( self . benchmark_uid , self . model_uid , self . dataset_uid ) self . path = os . path . join ( self . path , config . results_info_file ) all () classmethod Gets and creates instances of all the user's results Source code in entities/result.py 60 61 62 63 64 65 66 67 68 69 70 @classmethod def all ( cls ) -> List [ \"Result\" ]: \"\"\"Gets and creates instances of all the user's results\"\"\" logging . info ( \"Retrieving all results\" ) results_ids_tuple = results_ids () results = [] for result_ids in results_ids_tuple : b_id , m_id , d_id = result_ids results . append ( cls . from_entities_uids ( b_id , m_id , d_id )) return results get ( result_uid ) classmethod Retrieves and creates a Result instance obtained from the platform. If the result instance already exists in the user's machine, it loads the local instance Parameters: Name Type Description Default result_uid str UID of the Result instance required Returns: Name Type Description Result Result Specified Result instance Source code in entities/result.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 @classmethod def get ( cls , result_uid : str ) -> \"Result\" : \"\"\"Retrieves and creates a Result instance obtained from the platform. If the result instance already exists in the user's machine, it loads the local instance Args: result_uid (str): UID of the Result instance Returns: Result: Specified Result instance \"\"\" logging . debug ( f \"Retrieving result { result_uid } \" ) comms = config . comms # Try to download first try : meta = comms . get_result ( result_uid ) result = cls ( meta ) except CommunicationRetrievalError : # Get local results logging . warning ( f \"Getting result { result_uid } from comms failed\" ) logging . info ( f \"Looking for result { result_uid } locally\" ) local_result = list ( filter ( lambda res : str ( res . uid ) == str ( result_uid ), cls . all ()) ) if len ( local_result ) == 1 : logging . debug ( \"Found result locally\" ) result = local_result [ 0 ] else : raise InvalidArgumentError ( f \"The requested result { result_uid } could not be retrieved\" ) result . write () return result upload () Uploads the results to the comms Parameters: Name Type Description Default comms Comms Instance of the communications interface. required Source code in entities/result.py 124 125 126 127 128 129 130 131 132 def upload ( self ): \"\"\"Uploads the results to the comms Args: comms (Comms): Instance of the communications interface. \"\"\" results_info = self . todict () updated_results_info = config . comms . upload_results ( results_info ) return updated_results_info","title":"result"},{"location":"reference/entities/result/#entities.result.Result","text":"Bases: Entity Class representing a Result entry Results are obtained after successfully running a benchmark execution flow. They contain information regarding the components involved in obtaining metrics results, as well as the results themselves. This class provides methods for working with benchmark results and how to upload them to the backend. Source code in entities/result.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 class Result ( Entity ): \"\"\" Class representing a Result entry Results are obtained after successfully running a benchmark execution flow. They contain information regarding the components involved in obtaining metrics results, as well as the results themselves. This class provides methods for working with benchmark results and how to upload them to the backend. \"\"\" def __init__ ( self , results_info : dict ): \"\"\"Creates a new result instance Args: benchmark_uid (str): UID of the executed benchmark. dataset_uid (str): UID of the dataset used. model_uid (str): UID of the model used. results() \"\"\" self . uid = results_info [ \"id\" ] self . name = results_info [ \"name\" ] self . owner = results_info [ \"owner\" ] self . benchmark_uid = results_info [ \"benchmark\" ] self . model_uid = results_info [ \"model\" ] self . dataset_uid = results_info [ \"dataset\" ] self . results = results_info [ \"results\" ] self . status = Status ( results_info [ \"approval_status\" ]) self . metadata = results_info [ \"metadata\" ] self . approved_at = results_info [ \"approved_at\" ] self . created_at = results_info [ \"created_at\" ] self . modified_at = results_info [ \"modified_at\" ] self . path = results_path ( self . benchmark_uid , self . model_uid , self . dataset_uid ) self . path = os . path . join ( self . path , config . results_info_file ) @classmethod def from_entities_uids ( cls , benchmark_uid : str , model_uid : str , dataset_uid : str ) -> \"Result\" : results_info = cls . __get_local_dict ( benchmark_uid , model_uid , dataset_uid ) return cls ( results_info ) @classmethod def all ( cls ) -> List [ \"Result\" ]: \"\"\"Gets and creates instances of all the user's results\"\"\" logging . info ( \"Retrieving all results\" ) results_ids_tuple = results_ids () results = [] for result_ids in results_ids_tuple : b_id , m_id , d_id = result_ids results . append ( cls . from_entities_uids ( b_id , m_id , d_id )) return results @classmethod def get ( cls , result_uid : str ) -> \"Result\" : \"\"\"Retrieves and creates a Result instance obtained from the platform. If the result instance already exists in the user's machine, it loads the local instance Args: result_uid (str): UID of the Result instance Returns: Result: Specified Result instance \"\"\" logging . debug ( f \"Retrieving result { result_uid } \" ) comms = config . comms # Try to download first try : meta = comms . get_result ( result_uid ) result = cls ( meta ) except CommunicationRetrievalError : # Get local results logging . warning ( f \"Getting result { result_uid } from comms failed\" ) logging . info ( f \"Looking for result { result_uid } locally\" ) local_result = list ( filter ( lambda res : str ( res . uid ) == str ( result_uid ), cls . all ()) ) if len ( local_result ) == 1 : logging . debug ( \"Found result locally\" ) result = local_result [ 0 ] else : raise InvalidArgumentError ( f \"The requested result { result_uid } could not be retrieved\" ) result . write () return result def todict ( self ): return { \"id\" : self . uid , \"name\" : self . name , \"owner\" : self . owner , \"benchmark\" : self . benchmark_uid , \"model\" : self . model_uid , \"dataset\" : self . dataset_uid , \"results\" : self . results , \"metadata\" : self . metadata , \"approval_status\" : self . status . value , \"approved_at\" : self . approved_at , \"created_at\" : self . created_at , \"modified_at\" : self . modified_at , } def upload ( self ): \"\"\"Uploads the results to the comms Args: comms (Comms): Instance of the communications interface. \"\"\" results_info = self . todict () updated_results_info = config . comms . upload_results ( results_info ) return updated_results_info def write ( self ): if os . path . exists ( self . path ): write_access = os . access ( self . path , os . W_OK ) logging . debug ( f \"file has write access? { write_access } \" ) if not write_access : logging . debug ( \"removing outdated and inaccessible results\" ) os . remove ( self . path ) os . makedirs ( Path ( self . path ) . parent , exist_ok = True ) with open ( self . path , \"w\" ) as f : yaml . dump ( self . todict (), f ) @classmethod def __get_local_dict ( cls , benchmark_uid , model_uid , dataset_uid ): path = results_path ( benchmark_uid , model_uid , dataset_uid ) path = os . path . join ( path , config . results_info_file ) with open ( path , \"r\" ) as f : results_info = yaml . safe_load ( f ) return results_info","title":"Result"},{"location":"reference/entities/result/#entities.result.Result.__init__","text":"Creates a new result instance Parameters: Name Type Description Default benchmark_uid str UID of the executed benchmark. required dataset_uid str UID of the dataset used. required model_uid str UID of the model used. required Source code in entities/result.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def __init__ ( self , results_info : dict ): \"\"\"Creates a new result instance Args: benchmark_uid (str): UID of the executed benchmark. dataset_uid (str): UID of the dataset used. model_uid (str): UID of the model used. results() \"\"\" self . uid = results_info [ \"id\" ] self . name = results_info [ \"name\" ] self . owner = results_info [ \"owner\" ] self . benchmark_uid = results_info [ \"benchmark\" ] self . model_uid = results_info [ \"model\" ] self . dataset_uid = results_info [ \"dataset\" ] self . results = results_info [ \"results\" ] self . status = Status ( results_info [ \"approval_status\" ]) self . metadata = results_info [ \"metadata\" ] self . approved_at = results_info [ \"approved_at\" ] self . created_at = results_info [ \"created_at\" ] self . modified_at = results_info [ \"modified_at\" ] self . path = results_path ( self . benchmark_uid , self . model_uid , self . dataset_uid ) self . path = os . path . join ( self . path , config . results_info_file )","title":"__init__()"},{"location":"reference/entities/result/#entities.result.Result.all","text":"Gets and creates instances of all the user's results Source code in entities/result.py 60 61 62 63 64 65 66 67 68 69 70 @classmethod def all ( cls ) -> List [ \"Result\" ]: \"\"\"Gets and creates instances of all the user's results\"\"\" logging . info ( \"Retrieving all results\" ) results_ids_tuple = results_ids () results = [] for result_ids in results_ids_tuple : b_id , m_id , d_id = result_ids results . append ( cls . from_entities_uids ( b_id , m_id , d_id )) return results","title":"all()"},{"location":"reference/entities/result/#entities.result.Result.get","text":"Retrieves and creates a Result instance obtained from the platform. If the result instance already exists in the user's machine, it loads the local instance Parameters: Name Type Description Default result_uid str UID of the Result instance required Returns: Name Type Description Result Result Specified Result instance Source code in entities/result.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 @classmethod def get ( cls , result_uid : str ) -> \"Result\" : \"\"\"Retrieves and creates a Result instance obtained from the platform. If the result instance already exists in the user's machine, it loads the local instance Args: result_uid (str): UID of the Result instance Returns: Result: Specified Result instance \"\"\" logging . debug ( f \"Retrieving result { result_uid } \" ) comms = config . comms # Try to download first try : meta = comms . get_result ( result_uid ) result = cls ( meta ) except CommunicationRetrievalError : # Get local results logging . warning ( f \"Getting result { result_uid } from comms failed\" ) logging . info ( f \"Looking for result { result_uid } locally\" ) local_result = list ( filter ( lambda res : str ( res . uid ) == str ( result_uid ), cls . all ()) ) if len ( local_result ) == 1 : logging . debug ( \"Found result locally\" ) result = local_result [ 0 ] else : raise InvalidArgumentError ( f \"The requested result { result_uid } could not be retrieved\" ) result . write () return result","title":"get()"},{"location":"reference/entities/result/#entities.result.Result.upload","text":"Uploads the results to the comms Parameters: Name Type Description Default comms Comms Instance of the communications interface. required Source code in entities/result.py 124 125 126 127 128 129 130 131 132 def upload ( self ): \"\"\"Uploads the results to the comms Args: comms (Comms): Instance of the communications interface. \"\"\" results_info = self . todict () updated_results_info = config . comms . upload_results ( results_info ) return updated_results_info","title":"upload()"},{"location":"reference/ui/cli/","text":"CLI Bases: UI Source code in ui/cli.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 class CLI ( UI ): def __init__ ( self ): self . spinner = yaspin ( color = \"green\" ) self . is_interactive = False def print ( self , msg : str = \"\" ): \"\"\"Display a message on the command line Args: msg (str): message to print \"\"\" self . __print ( msg ) def print_error ( self , msg : str ): \"\"\"Display an error message on the command line Args: msg (str): error message to display \"\"\" msg = f \"\u274c { msg } \" msg = typer . style ( msg , fg = typer . colors . RED , bold = True ) self . __print ( msg ) def __print ( self , msg : str = \"\" ): if self . is_interactive : self . spinner . write ( msg ) else : typer . echo ( msg ) def start_interactive ( self ): \"\"\"Start an interactive session where messages can be overwritten and animations can be displayed\"\"\" self . is_interactive = True self . spinner . start () def stop_interactive ( self ): \"\"\"Stop an interactive session \"\"\" self . is_interactive = False self . spinner . stop () @contextmanager def interactive ( self ): \"\"\"Context managed interactive session. Yields: CLI: Yields the current CLI instance with an interactive session initialized \"\"\" self . start_interactive () try : yield self finally : self . stop_interactive () @property def text ( self ): return self . spinner . text @text . setter def text ( self , msg : str = \"\" ): \"\"\"Displays a message that overwrites previous messages if they were created during an interactive ui session. If not on interactive session already, then it calls the ui print function Args: msg (str): message to display \"\"\" if not self . is_interactive : self . print ( msg ) self . spinner . text = msg def prompt ( self , msg : str ) -> str : \"\"\"Displays a prompt to the user and waits for an answer Args: msg (str): message to use for the prompt Returns: str: user input \"\"\" return input ( msg ) def hidden_prompt ( self , msg : str ) -> str : \"\"\"Displays a prompt to the user and waits for an aswer. User input is not displayed Args: msg (str): message to use for the prompt Returns: str: user input \"\"\" return getpass ( msg ) hidden_prompt ( msg ) Displays a prompt to the user and waits for an aswer. User input is not displayed Parameters: Name Type Description Default msg str message to use for the prompt required Returns: Name Type Description str str user input Source code in ui/cli.py 93 94 95 96 97 98 99 100 101 102 def hidden_prompt ( self , msg : str ) -> str : \"\"\"Displays a prompt to the user and waits for an aswer. User input is not displayed Args: msg (str): message to use for the prompt Returns: str: user input \"\"\" return getpass ( msg ) interactive () Context managed interactive session. Yields: Name Type Description CLI Yields the current CLI instance with an interactive session initialized Source code in ui/cli.py 50 51 52 53 54 55 56 57 58 59 60 61 @contextmanager def interactive ( self ): \"\"\"Context managed interactive session. Yields: CLI: Yields the current CLI instance with an interactive session initialized \"\"\" self . start_interactive () try : yield self finally : self . stop_interactive () print ( msg = '' ) Display a message on the command line Parameters: Name Type Description Default msg str message to print '' Source code in ui/cli.py 14 15 16 17 18 19 20 def print ( self , msg : str = \"\" ): \"\"\"Display a message on the command line Args: msg (str): message to print \"\"\" self . __print ( msg ) print_error ( msg ) Display an error message on the command line Parameters: Name Type Description Default msg str error message to display required Source code in ui/cli.py 22 23 24 25 26 27 28 29 30 def print_error ( self , msg : str ): \"\"\"Display an error message on the command line Args: msg (str): error message to display \"\"\" msg = f \"\u274c { msg } \" msg = typer . style ( msg , fg = typer . colors . RED , bold = True ) self . __print ( msg ) prompt ( msg ) Displays a prompt to the user and waits for an answer Parameters: Name Type Description Default msg str message to use for the prompt required Returns: Name Type Description str str user input Source code in ui/cli.py 82 83 84 85 86 87 88 89 90 91 def prompt ( self , msg : str ) -> str : \"\"\"Displays a prompt to the user and waits for an answer Args: msg (str): message to use for the prompt Returns: str: user input \"\"\" return input ( msg ) start_interactive () Start an interactive session where messages can be overwritten and animations can be displayed Source code in ui/cli.py 38 39 40 41 42 def start_interactive ( self ): \"\"\"Start an interactive session where messages can be overwritten and animations can be displayed\"\"\" self . is_interactive = True self . spinner . start () stop_interactive () Stop an interactive session Source code in ui/cli.py 44 45 46 47 48 def stop_interactive ( self ): \"\"\"Stop an interactive session \"\"\" self . is_interactive = False self . spinner . stop ()","title":"cli"},{"location":"reference/ui/cli/#ui.cli.CLI","text":"Bases: UI Source code in ui/cli.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 class CLI ( UI ): def __init__ ( self ): self . spinner = yaspin ( color = \"green\" ) self . is_interactive = False def print ( self , msg : str = \"\" ): \"\"\"Display a message on the command line Args: msg (str): message to print \"\"\" self . __print ( msg ) def print_error ( self , msg : str ): \"\"\"Display an error message on the command line Args: msg (str): error message to display \"\"\" msg = f \"\u274c { msg } \" msg = typer . style ( msg , fg = typer . colors . RED , bold = True ) self . __print ( msg ) def __print ( self , msg : str = \"\" ): if self . is_interactive : self . spinner . write ( msg ) else : typer . echo ( msg ) def start_interactive ( self ): \"\"\"Start an interactive session where messages can be overwritten and animations can be displayed\"\"\" self . is_interactive = True self . spinner . start () def stop_interactive ( self ): \"\"\"Stop an interactive session \"\"\" self . is_interactive = False self . spinner . stop () @contextmanager def interactive ( self ): \"\"\"Context managed interactive session. Yields: CLI: Yields the current CLI instance with an interactive session initialized \"\"\" self . start_interactive () try : yield self finally : self . stop_interactive () @property def text ( self ): return self . spinner . text @text . setter def text ( self , msg : str = \"\" ): \"\"\"Displays a message that overwrites previous messages if they were created during an interactive ui session. If not on interactive session already, then it calls the ui print function Args: msg (str): message to display \"\"\" if not self . is_interactive : self . print ( msg ) self . spinner . text = msg def prompt ( self , msg : str ) -> str : \"\"\"Displays a prompt to the user and waits for an answer Args: msg (str): message to use for the prompt Returns: str: user input \"\"\" return input ( msg ) def hidden_prompt ( self , msg : str ) -> str : \"\"\"Displays a prompt to the user and waits for an aswer. User input is not displayed Args: msg (str): message to use for the prompt Returns: str: user input \"\"\" return getpass ( msg )","title":"CLI"},{"location":"reference/ui/cli/#ui.cli.CLI.hidden_prompt","text":"Displays a prompt to the user and waits for an aswer. User input is not displayed Parameters: Name Type Description Default msg str message to use for the prompt required Returns: Name Type Description str str user input Source code in ui/cli.py 93 94 95 96 97 98 99 100 101 102 def hidden_prompt ( self , msg : str ) -> str : \"\"\"Displays a prompt to the user and waits for an aswer. User input is not displayed Args: msg (str): message to use for the prompt Returns: str: user input \"\"\" return getpass ( msg )","title":"hidden_prompt()"},{"location":"reference/ui/cli/#ui.cli.CLI.interactive","text":"Context managed interactive session. Yields: Name Type Description CLI Yields the current CLI instance with an interactive session initialized Source code in ui/cli.py 50 51 52 53 54 55 56 57 58 59 60 61 @contextmanager def interactive ( self ): \"\"\"Context managed interactive session. Yields: CLI: Yields the current CLI instance with an interactive session initialized \"\"\" self . start_interactive () try : yield self finally : self . stop_interactive ()","title":"interactive()"},{"location":"reference/ui/cli/#ui.cli.CLI.print","text":"Display a message on the command line Parameters: Name Type Description Default msg str message to print '' Source code in ui/cli.py 14 15 16 17 18 19 20 def print ( self , msg : str = \"\" ): \"\"\"Display a message on the command line Args: msg (str): message to print \"\"\" self . __print ( msg )","title":"print()"},{"location":"reference/ui/cli/#ui.cli.CLI.print_error","text":"Display an error message on the command line Parameters: Name Type Description Default msg str error message to display required Source code in ui/cli.py 22 23 24 25 26 27 28 29 30 def print_error ( self , msg : str ): \"\"\"Display an error message on the command line Args: msg (str): error message to display \"\"\" msg = f \"\u274c { msg } \" msg = typer . style ( msg , fg = typer . colors . RED , bold = True ) self . __print ( msg )","title":"print_error()"},{"location":"reference/ui/cli/#ui.cli.CLI.prompt","text":"Displays a prompt to the user and waits for an answer Parameters: Name Type Description Default msg str message to use for the prompt required Returns: Name Type Description str str user input Source code in ui/cli.py 82 83 84 85 86 87 88 89 90 91 def prompt ( self , msg : str ) -> str : \"\"\"Displays a prompt to the user and waits for an answer Args: msg (str): message to use for the prompt Returns: str: user input \"\"\" return input ( msg )","title":"prompt()"},{"location":"reference/ui/cli/#ui.cli.CLI.start_interactive","text":"Start an interactive session where messages can be overwritten and animations can be displayed Source code in ui/cli.py 38 39 40 41 42 def start_interactive ( self ): \"\"\"Start an interactive session where messages can be overwritten and animations can be displayed\"\"\" self . is_interactive = True self . spinner . start ()","title":"start_interactive()"},{"location":"reference/ui/cli/#ui.cli.CLI.stop_interactive","text":"Stop an interactive session Source code in ui/cli.py 44 45 46 47 48 def stop_interactive ( self ): \"\"\"Stop an interactive session \"\"\" self . is_interactive = False self . spinner . stop ()","title":"stop_interactive()"},{"location":"reference/ui/factory/","text":"","title":"factory"},{"location":"reference/ui/interface/","text":"UI Bases: ABC Source code in ui/interface.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 class UI ( ABC ): @abstractmethod def print ( self , msg : str = \"\" ): \"\"\"Display a message to the interface. If on interactive session overrides previous message \"\"\" @abstractmethod def print_error ( self , msg : str ): \"\"\"Display an error message to the interface\"\"\" @abstractmethod def start_interactive ( self ): \"\"\"Initialize an interactive session for animations or overriding messages. If the UI doesn't support this, the function can be left empty. \"\"\" @abstractmethod def stop_interactive ( self ): \"\"\"Terminate an interactive session. If the UI doesn't support this, the function can be left empty. \"\"\" @abstractmethod @contextmanager def interactive ( self ): \"\"\"Context managed interactive session. Expected to yield the same instance \"\"\" @abstractmethod def text ( self , msg : str ): \"\"\"Displays a messages that overwrites previous messages if they were created during an interactive session. If not supported or not on an interactive session, it is expected to fallback to the UI print function. Args: msg (str): message to display \"\"\" @abstractmethod def prompt ( msg : str ) -> str : \"\"\"Displays a prompt to the user and waits for an answer\"\"\" @abstractmethod def hidden_prompt ( self , msg : str ) -> str : \"\"\"Displays a prompt to the user and waits for an aswer. User input is not displayed Args: msg (str): message to use for the prompt Returns: str: user input \"\"\" hidden_prompt ( msg ) abstractmethod Displays a prompt to the user and waits for an aswer. User input is not displayed Parameters: Name Type Description Default msg str message to use for the prompt required Returns: Name Type Description str str user input Source code in ui/interface.py 49 50 51 52 53 54 55 56 57 58 @abstractmethod def hidden_prompt ( self , msg : str ) -> str : \"\"\"Displays a prompt to the user and waits for an aswer. User input is not displayed Args: msg (str): message to use for the prompt Returns: str: user input \"\"\" interactive () abstractmethod Context managed interactive session. Expected to yield the same instance Source code in ui/interface.py 28 29 30 31 32 @abstractmethod @contextmanager def interactive ( self ): \"\"\"Context managed interactive session. Expected to yield the same instance \"\"\" print ( msg = '' ) abstractmethod Display a message to the interface. If on interactive session overrides previous message Source code in ui/interface.py 6 7 8 9 10 @abstractmethod def print ( self , msg : str = \"\" ): \"\"\"Display a message to the interface. If on interactive session overrides previous message \"\"\" print_error ( msg ) abstractmethod Display an error message to the interface Source code in ui/interface.py 12 13 14 @abstractmethod def print_error ( self , msg : str ): \"\"\"Display an error message to the interface\"\"\" prompt ( msg ) abstractmethod Displays a prompt to the user and waits for an answer Source code in ui/interface.py 45 46 47 @abstractmethod def prompt ( msg : str ) -> str : \"\"\"Displays a prompt to the user and waits for an answer\"\"\" start_interactive () abstractmethod Initialize an interactive session for animations or overriding messages. If the UI doesn't support this, the function can be left empty. Source code in ui/interface.py 16 17 18 19 20 @abstractmethod def start_interactive ( self ): \"\"\"Initialize an interactive session for animations or overriding messages. If the UI doesn't support this, the function can be left empty. \"\"\" stop_interactive () abstractmethod Terminate an interactive session. If the UI doesn't support this, the function can be left empty. Source code in ui/interface.py 22 23 24 25 26 @abstractmethod def stop_interactive ( self ): \"\"\"Terminate an interactive session. If the UI doesn't support this, the function can be left empty. \"\"\" text ( msg ) abstractmethod Displays a messages that overwrites previous messages if they were created during an interactive session. If not supported or not on an interactive session, it is expected to fallback to the UI print function. Parameters: Name Type Description Default msg str message to display required Source code in ui/interface.py 34 35 36 37 38 39 40 41 42 43 @abstractmethod def text ( self , msg : str ): \"\"\"Displays a messages that overwrites previous messages if they were created during an interactive session. If not supported or not on an interactive session, it is expected to fallback to the UI print function. Args: msg (str): message to display \"\"\"","title":"interface"},{"location":"reference/ui/interface/#ui.interface.UI","text":"Bases: ABC Source code in ui/interface.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 class UI ( ABC ): @abstractmethod def print ( self , msg : str = \"\" ): \"\"\"Display a message to the interface. If on interactive session overrides previous message \"\"\" @abstractmethod def print_error ( self , msg : str ): \"\"\"Display an error message to the interface\"\"\" @abstractmethod def start_interactive ( self ): \"\"\"Initialize an interactive session for animations or overriding messages. If the UI doesn't support this, the function can be left empty. \"\"\" @abstractmethod def stop_interactive ( self ): \"\"\"Terminate an interactive session. If the UI doesn't support this, the function can be left empty. \"\"\" @abstractmethod @contextmanager def interactive ( self ): \"\"\"Context managed interactive session. Expected to yield the same instance \"\"\" @abstractmethod def text ( self , msg : str ): \"\"\"Displays a messages that overwrites previous messages if they were created during an interactive session. If not supported or not on an interactive session, it is expected to fallback to the UI print function. Args: msg (str): message to display \"\"\" @abstractmethod def prompt ( msg : str ) -> str : \"\"\"Displays a prompt to the user and waits for an answer\"\"\" @abstractmethod def hidden_prompt ( self , msg : str ) -> str : \"\"\"Displays a prompt to the user and waits for an aswer. User input is not displayed Args: msg (str): message to use for the prompt Returns: str: user input \"\"\"","title":"UI"},{"location":"reference/ui/interface/#ui.interface.UI.hidden_prompt","text":"Displays a prompt to the user and waits for an aswer. User input is not displayed Parameters: Name Type Description Default msg str message to use for the prompt required Returns: Name Type Description str str user input Source code in ui/interface.py 49 50 51 52 53 54 55 56 57 58 @abstractmethod def hidden_prompt ( self , msg : str ) -> str : \"\"\"Displays a prompt to the user and waits for an aswer. User input is not displayed Args: msg (str): message to use for the prompt Returns: str: user input \"\"\"","title":"hidden_prompt()"},{"location":"reference/ui/interface/#ui.interface.UI.interactive","text":"Context managed interactive session. Expected to yield the same instance Source code in ui/interface.py 28 29 30 31 32 @abstractmethod @contextmanager def interactive ( self ): \"\"\"Context managed interactive session. Expected to yield the same instance \"\"\"","title":"interactive()"},{"location":"reference/ui/interface/#ui.interface.UI.print","text":"Display a message to the interface. If on interactive session overrides previous message Source code in ui/interface.py 6 7 8 9 10 @abstractmethod def print ( self , msg : str = \"\" ): \"\"\"Display a message to the interface. If on interactive session overrides previous message \"\"\"","title":"print()"},{"location":"reference/ui/interface/#ui.interface.UI.print_error","text":"Display an error message to the interface Source code in ui/interface.py 12 13 14 @abstractmethod def print_error ( self , msg : str ): \"\"\"Display an error message to the interface\"\"\"","title":"print_error()"},{"location":"reference/ui/interface/#ui.interface.UI.prompt","text":"Displays a prompt to the user and waits for an answer Source code in ui/interface.py 45 46 47 @abstractmethod def prompt ( msg : str ) -> str : \"\"\"Displays a prompt to the user and waits for an answer\"\"\"","title":"prompt()"},{"location":"reference/ui/interface/#ui.interface.UI.start_interactive","text":"Initialize an interactive session for animations or overriding messages. If the UI doesn't support this, the function can be left empty. Source code in ui/interface.py 16 17 18 19 20 @abstractmethod def start_interactive ( self ): \"\"\"Initialize an interactive session for animations or overriding messages. If the UI doesn't support this, the function can be left empty. \"\"\"","title":"start_interactive()"},{"location":"reference/ui/interface/#ui.interface.UI.stop_interactive","text":"Terminate an interactive session. If the UI doesn't support this, the function can be left empty. Source code in ui/interface.py 22 23 24 25 26 @abstractmethod def stop_interactive ( self ): \"\"\"Terminate an interactive session. If the UI doesn't support this, the function can be left empty. \"\"\"","title":"stop_interactive()"},{"location":"reference/ui/interface/#ui.interface.UI.text","text":"Displays a messages that overwrites previous messages if they were created during an interactive session. If not supported or not on an interactive session, it is expected to fallback to the UI print function. Parameters: Name Type Description Default msg str message to display required Source code in ui/interface.py 34 35 36 37 38 39 40 41 42 43 @abstractmethod def text ( self , msg : str ): \"\"\"Displays a messages that overwrites previous messages if they were created during an interactive session. If not supported or not on an interactive session, it is expected to fallback to the UI print function. Args: msg (str): message to display \"\"\"","title":"text()"},{"location":"reference/ui/stdin/","text":"StdIn Bases: UI Class for using sys.stdin/sys.stdout exclusively. Used mainly for automating execution with class-like objects. Using only basic IO methods ensures that piping from the command-line. Should not be used in normal execution, as hidden prompts and interactive prints will not work as expected. Source code in ui/stdin.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class StdIn ( UI ): \"\"\" Class for using sys.stdin/sys.stdout exclusively. Used mainly for automating execution with class-like objects. Using only basic IO methods ensures that piping from the command-line. Should not be used in normal execution, as hidden prompts and interactive prints will not work as expected. \"\"\" def print ( self , msg : str = \"\" ): return print ( msg ) def print_error ( self , msg : str ): return self . print ( msg ) def start_interactive ( self ): pass def stop_interactive ( self ): pass @contextmanager def interactive ( self ): yield self @property def text ( self ): return \"\" @text . setter def text ( self , msg : str = \"\" ): return def prompt ( self , msg : str ) -> str : return input ( msg ) def hidden_prompt ( self , msg : str ) -> str : return self . prompt ( msg )","title":"stdin"},{"location":"reference/ui/stdin/#ui.stdin.StdIn","text":"Bases: UI Class for using sys.stdin/sys.stdout exclusively. Used mainly for automating execution with class-like objects. Using only basic IO methods ensures that piping from the command-line. Should not be used in normal execution, as hidden prompts and interactive prints will not work as expected. Source code in ui/stdin.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class StdIn ( UI ): \"\"\" Class for using sys.stdin/sys.stdout exclusively. Used mainly for automating execution with class-like objects. Using only basic IO methods ensures that piping from the command-line. Should not be used in normal execution, as hidden prompts and interactive prints will not work as expected. \"\"\" def print ( self , msg : str = \"\" ): return print ( msg ) def print_error ( self , msg : str ): return self . print ( msg ) def start_interactive ( self ): pass def stop_interactive ( self ): pass @contextmanager def interactive ( self ): yield self @property def text ( self ): return \"\" @text . setter def text ( self , msg : str = \"\" ): return def prompt ( self , msg : str ) -> str : return input ( msg ) def hidden_prompt ( self , msg : str ) -> str : return self . prompt ( msg )","title":"StdIn"},{"location":"server/","text":"MedPerf API Server Code is tested on latest python 3.9 Install all dependencies pip install -r requirements.txt Create .env file with your environment settings Sample .env.example is added to root. Rename .env.example to .env and modify with your env vars. cp .env.example .env Create tables and existing models python manage.py migrate Start the server python manage.py runserver API Server is running at http://127.0.0.1:8000/ by default. You can view and experiment Medperf API at http://127.0.0.1:8000/swagger Test MedPerf API You can run server script to verify a sample work. See script pip install -r test-requirements.txt python seed.py","title":"MedPerf API Server"},{"location":"server/#medperf-api-server","text":"Code is tested on latest python 3.9","title":"MedPerf API Server"},{"location":"server/#install-all-dependencies","text":"pip install -r requirements.txt","title":"Install all dependencies"},{"location":"server/#create-env-file-with-your-environment-settings","text":"Sample .env.example is added to root. Rename .env.example to .env and modify with your env vars. cp .env.example .env","title":"Create .env file with your environment settings"},{"location":"server/#create-tables-and-existing-models","text":"python manage.py migrate","title":"Create tables and existing models"},{"location":"server/#start-the-server","text":"python manage.py runserver API Server is running at http://127.0.0.1:8000/ by default. You can view and experiment Medperf API at http://127.0.0.1:8000/swagger","title":"Start the server"},{"location":"server/#test-medperf-api","text":"You can run server script to verify a sample work. See script pip install -r test-requirements.txt python seed.py","title":"Test MedPerf API"}]}